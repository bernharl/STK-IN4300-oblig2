{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 1;\n",
       "                var nbb_formatted_code = \"import pandas as pd  # For dataframe\\nimport numpy as np  # For matrix operations\\nimport sklearn.preprocessing as sklpre  # For preprocessing (scaling)\\nimport sklearn.linear_model as skllm  # For OLS\\nimport sklearn.model_selection as sklms  # For train_test_split\\nfrom scipy import stats  # To calc p-value\\nimport matplotlib.pyplot as plt  # For plotting\\n\\n# For automatic formatting of code, sparing you from my usually horrible looking code\\n%load_ext nb_black\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd  # For dataframe\n",
    "import numpy as np  # For matrix operations\n",
    "import sklearn.preprocessing as sklpre  # For preprocessing (scaling)\n",
    "import sklearn.linear_model as skllm  # For OLS\n",
    "import sklearn.model_selection as sklms  # For train_test_split\n",
    "from scipy import stats  # To calc p-value\n",
    "import matplotlib.pyplot as plt  # For plotting\n",
    "\n",
    "# For automatic formatting of code, sparing you from my usually horrible looking code\n",
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1.1\n",
    "I have chosen to one-hot encode the SEX-category, as neither male nor female should be considered adifferent numbers. \n",
    "The rest of the categorical values are just true/false, so those aren't encoded. \n",
    "Then I scale all the scalar features, not touching the categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_formatted_code = \"# Reading the data into dataframe\\ndf = pd.read_csv(\\\"data_task_1.txt\\\", header=0, sep=\\\" \\\")\\n# Onehot-encoding gender\\nonehot_gender = pd.get_dummies(df[\\\"SEX\\\"]).set_axis(\\n    [\\\"Male\\\", \\\"Female\\\"], axis=1, inplace=False\\n)\\n# Replacing old gender column\\ndf = df.join(onehot_gender)\\ndf.drop(\\\"SEX\\\", axis=1, inplace=True)\\n# List of boolean categories\\ncategorical = [\\n    \\\"ADHEU\\\",\\n    \\\"HOCHOZON\\\",\\n    \\\"AMATOP\\\",\\n    \\\"AVATOP\\\",\\n    \\\"ADEKZ\\\",\\n    \\\"ARAUCH\\\",\\n    \\\"FSNIGHT\\\",\\n    \\\"FSPT\\\",\\n    \\\"FSATEM\\\",\\n    \\\"FSAUGE\\\",\\n    \\\"FSPFEI\\\",\\n    \\\"FSHLAUF\\\",\\n    \\\"Male\\\",\\n    \\\"Female\\\",\\n]\\n\\n# A loop that splits the data and tries again until there is no split where only one modality is in one split\\nfirst = True\\nwhile (\\n    first\\n    or np.any(\\n        np.logical_or(X_train.sum(axis=0) == 0, X_train.sum(axis=0) == X_train.shape[0])\\n    )\\n    or np.any(\\n        np.logical_or(X_test.sum(axis=0) == 0, X_test.sum(axis=0) == X_test.shape[0])\\n    )\\n):\\n    first = False\\n    # Splitting over and over until splits are good, stratifying the most biased feature.\\n    X_train, X_test, y_train, y_test = sklms.train_test_split(\\n        df.loc[:, df.columns != \\\"FFVC\\\"],\\n        df[\\\"FFVC\\\"],\\n        test_size=0.5,\\n        stratify=df[\\\"FSATEM\\\"],\\n    )\\n# Scaling scalar features based on train set\\nscaler = sklpre.StandardScaler()\\nX_train_continous = scaler.fit_transform(\\n    X_train.loc[:, np.logical_not(np.isin(X_train.columns, categorical))].values\\n)\\nX_test_continous = scaler.transform(\\n    X_test.loc[:, np.logical_not(np.isin(X_test.columns, categorical))].values\\n)\\n# Putting all scalar and categorical features together\\nX_train.loc[\\n    :, np.logical_not(np.isin(X_train.columns, categorical))\\n] = X_train_continous\\nX_test.loc[:, np.logical_not(np.isin(X_test.columns, categorical))] = X_test_continous\\n# All preprocessing done!\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reading the data into dataframe\n",
    "df = pd.read_csv(\"data_task_1.txt\", header=0, sep=\" \")\n",
    "# Onehot-encoding gender\n",
    "onehot_gender = pd.get_dummies(df[\"SEX\"]).set_axis(\n",
    "    [\"Male\", \"Female\"], axis=1, inplace=False\n",
    ")\n",
    "# Replacing old gender column\n",
    "df = df.join(onehot_gender)\n",
    "df.drop(\"SEX\", axis=1, inplace=True)\n",
    "# List of boolean categories\n",
    "categorical = [\n",
    "    \"ADHEU\",\n",
    "    \"HOCHOZON\",\n",
    "    \"AMATOP\",\n",
    "    \"AVATOP\",\n",
    "    \"ADEKZ\",\n",
    "    \"ARAUCH\",\n",
    "    \"FSNIGHT\",\n",
    "    \"FSPT\",\n",
    "    \"FSATEM\",\n",
    "    \"FSAUGE\",\n",
    "    \"FSPFEI\",\n",
    "    \"FSHLAUF\",\n",
    "    \"Male\",\n",
    "    \"Female\",\n",
    "]\n",
    "\n",
    "# A loop that splits the data and tries again until there is no split where only one modality is in one split\n",
    "first = True\n",
    "while (\n",
    "    first\n",
    "    or np.any(\n",
    "        np.logical_or(X_train.sum(axis=0) == 0, X_train.sum(axis=0) == X_train.shape[0])\n",
    "    )\n",
    "    or np.any(\n",
    "        np.logical_or(X_test.sum(axis=0) == 0, X_test.sum(axis=0) == X_test.shape[0])\n",
    "    )\n",
    "):\n",
    "    first = False\n",
    "    # Splitting over and over until splits are good, stratifying the most biased feature.\n",
    "    X_train, X_test, y_train, y_test = sklms.train_test_split(\n",
    "        df.loc[:, df.columns != \"FFVC\"],\n",
    "        df[\"FFVC\"],\n",
    "        test_size=0.5,\n",
    "        stratify=df[\"FSATEM\"],\n",
    "    )\n",
    "# Scaling scalar features based on train set\n",
    "scaler = sklpre.StandardScaler()\n",
    "X_train_continous = scaler.fit_transform(\n",
    "    X_train.loc[:, np.logical_not(np.isin(X_train.columns, categorical))].values\n",
    ")\n",
    "X_test_continous = scaler.transform(\n",
    "    X_test.loc[:, np.logical_not(np.isin(X_test.columns, categorical))].values\n",
    ")\n",
    "# Putting all scalar and categorical features together\n",
    "X_train.loc[\n",
    "    :, np.logical_not(np.isin(X_train.columns, categorical))\n",
    "] = X_train_continous\n",
    "X_test.loc[:, np.logical_not(np.isin(X_test.columns, categorical))] = X_test_continous\n",
    "# All preprocessing done!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1.2\n",
    "Running OLS, calculating uncertainties and p-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_formatted_code = \"def get_summary_linear_model(model, X_train, y_train):\\n    \\\"\\\"\\\"\\n    Scikit-learn has no built in support for confidence intervals and p-values, so I \\n    made this to calculate it for me after fitting the model. Put into a function for reuse.\\n    \\\"\\\"\\\"\\n    # Combining intercept and coefficients in same array\\n    coefficients = np.append(model.intercept_, model.coef_)\\n\\n    # Predicting y\\n    y_hat = model.predict(X_train)\\n    # Calculating RSS to get variance for use when calculating stddev of coeffs\\n    residuals = y_train.values - y_hat\\n    rss = residuals.reshape(-1, 1).T @ residuals.reshape(-1, 1)\\n    var = rss[0, 0] / (len(X_train) - len(X_train.columns) - 1)\\n\\n    # Adding intercept to X_train, as sklearn usually does not need the column of 1's\\n    X_with_intercept = np.append(\\n        np.ones(X_train.shape[0]).reshape(-1, 1), X_train, axis=1\\n    )\\n    # Stddev of coefficients\\n    stddev = np.sqrt(\\n        (np.diag(var * np.linalg.pinv(X_with_intercept.T @ X_with_intercept)))\\n    )\\n    labels = [\\\"Intercept\\\"] + X_train.columns.tolist()\\n\\n    coef_over_std = coefficients / stddev\\n    p_values = [\\n        2 * (1 - stats.t.cdf(np.abs(i), (len(X_with_intercept) - 1)))\\n        for i in coef_over_std\\n    ]\\n\\n    # Putting results into table\\n    coeffs_table = pd.DataFrame(zip(labels, coefficients, stddev, p_values))\\n    # Giving nice names with TeX formatting\\n    coeffs_table.rename(\\n        columns={0: \\\"Feature\\\", 1: r\\\"$\\\\beta_i$\\\", 2: r\\\"$\\\\pm$\\\", 3: \\\"p-values\\\"},\\n        inplace=True,\\n    )\\n    return coeffs_table\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_summary_linear_model(model, X_train, y_train):\n",
    "    \"\"\"\n",
    "    Scikit-learn has no built in support for confidence intervals and p-values, so I \n",
    "    made this to calculate it for me after fitting the model. Put into a function for reuse.\n",
    "    \"\"\"\n",
    "    # Combining intercept and coefficients in same array\n",
    "    coefficients = np.append(model.intercept_, model.coef_)\n",
    "\n",
    "    # Predicting y\n",
    "    y_hat = model.predict(X_train)\n",
    "    # Calculating RSS to get variance for use when calculating stddev of coeffs\n",
    "    residuals = y_train.values - y_hat\n",
    "    rss = residuals.reshape(-1, 1).T @ residuals.reshape(-1, 1)\n",
    "    var = rss[0, 0] / (len(X_train) - len(X_train.columns) - 1)\n",
    "\n",
    "    # Adding intercept to X_train, as sklearn usually does not need the column of 1's\n",
    "    X_with_intercept = np.append(\n",
    "        np.ones(X_train.shape[0]).reshape(-1, 1), X_train, axis=1\n",
    "    )\n",
    "    # Stddev of coefficients\n",
    "    stddev = np.sqrt(\n",
    "        (np.diag(var * np.linalg.pinv(X_with_intercept.T @ X_with_intercept)))\n",
    "    )\n",
    "    labels = [\"Intercept\"] + X_train.columns.tolist()\n",
    "\n",
    "    coef_over_std = coefficients / stddev\n",
    "    p_values = [\n",
    "        2 * (1 - stats.t.cdf(np.abs(i), (len(X_with_intercept) - 1)))\n",
    "        for i in coef_over_std\n",
    "    ]\n",
    "\n",
    "    # Putting results into table\n",
    "    coeffs_table = pd.DataFrame(zip(labels, coefficients, stddev, p_values))\n",
    "    # Giving nice names with TeX formatting\n",
    "    coeffs_table.rename(\n",
    "        columns={0: \"Feature\", 1: r\"$\\beta_i$\", 2: r\"$\\pm$\", 3: \"p-values\"},\n",
    "        inplace=True,\n",
    "    )\n",
    "    return coeffs_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got an R^2 score of 0.57 for the test set.\n",
      "The most important feature (lowest p-value) is FLGROSS.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>$\\beta_i$</th>\n",
       "      <th>$\\pm$</th>\n",
       "      <th>p-values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Intercept</td>\n",
       "      <td>2.381548</td>\n",
       "      <td>0.025444</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ALTER</td>\n",
       "      <td>0.015350</td>\n",
       "      <td>0.016262</td>\n",
       "      <td>3.461102e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ADHEU</td>\n",
       "      <td>0.020827</td>\n",
       "      <td>0.065994</td>\n",
       "      <td>7.525777e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HOCHOZON</td>\n",
       "      <td>-0.104616</td>\n",
       "      <td>0.040513</td>\n",
       "      <td>1.039210e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AMATOP</td>\n",
       "      <td>-0.024356</td>\n",
       "      <td>0.033657</td>\n",
       "      <td>4.699621e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AVATOP</td>\n",
       "      <td>-0.018221</td>\n",
       "      <td>0.034199</td>\n",
       "      <td>5.946568e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ADEKZ</td>\n",
       "      <td>-0.002312</td>\n",
       "      <td>0.037170</td>\n",
       "      <td>9.504434e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ARAUCH</td>\n",
       "      <td>-0.011440</td>\n",
       "      <td>0.032000</td>\n",
       "      <td>7.210342e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AGEBGEW</td>\n",
       "      <td>0.014338</td>\n",
       "      <td>0.014085</td>\n",
       "      <td>3.096774e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>FSNIGHT</td>\n",
       "      <td>-0.069022</td>\n",
       "      <td>0.049439</td>\n",
       "      <td>1.639352e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>FLGROSS</td>\n",
       "      <td>0.167469</td>\n",
       "      <td>0.023176</td>\n",
       "      <td>6.173284e-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>FMILB</td>\n",
       "      <td>-0.012161</td>\n",
       "      <td>0.018565</td>\n",
       "      <td>5.130585e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>FNOH24</td>\n",
       "      <td>-0.051352</td>\n",
       "      <td>0.018488</td>\n",
       "      <td>5.896828e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>FTIER</td>\n",
       "      <td>0.004552</td>\n",
       "      <td>0.016549</td>\n",
       "      <td>7.834923e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>FPOLL</td>\n",
       "      <td>-0.003102</td>\n",
       "      <td>0.028241</td>\n",
       "      <td>9.126101e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>FLTOTMED</td>\n",
       "      <td>-0.023009</td>\n",
       "      <td>0.014686</td>\n",
       "      <td>1.184741e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>FO3H24</td>\n",
       "      <td>0.065951</td>\n",
       "      <td>0.031831</td>\n",
       "      <td>3.931592e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>FSPT</td>\n",
       "      <td>-0.045687</td>\n",
       "      <td>0.071990</td>\n",
       "      <td>5.262595e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>FTEH24</td>\n",
       "      <td>-0.042416</td>\n",
       "      <td>0.029092</td>\n",
       "      <td>1.461110e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>FSATEM</td>\n",
       "      <td>0.261347</td>\n",
       "      <td>0.082907</td>\n",
       "      <td>1.820113e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>FSAUGE</td>\n",
       "      <td>-0.029349</td>\n",
       "      <td>0.048436</td>\n",
       "      <td>5.451069e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>FLGEW</td>\n",
       "      <td>0.100324</td>\n",
       "      <td>0.021777</td>\n",
       "      <td>6.553868e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>FSPFEI</td>\n",
       "      <td>-0.015773</td>\n",
       "      <td>0.095976</td>\n",
       "      <td>8.695953e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>FSHLAUF</td>\n",
       "      <td>-0.042896</td>\n",
       "      <td>0.071160</td>\n",
       "      <td>5.471846e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Male</td>\n",
       "      <td>0.093053</td>\n",
       "      <td>0.018485</td>\n",
       "      <td>9.263729e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Female</td>\n",
       "      <td>-0.093053</td>\n",
       "      <td>0.019623</td>\n",
       "      <td>3.577259e-06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Feature  $\\beta_i$     $\\pm$      p-values\n",
       "0   Intercept   2.381548  0.025444  0.000000e+00\n",
       "1       ALTER   0.015350  0.016262  3.461102e-01\n",
       "2       ADHEU   0.020827  0.065994  7.525777e-01\n",
       "3    HOCHOZON  -0.104616  0.040513  1.039210e-02\n",
       "4      AMATOP  -0.024356  0.033657  4.699621e-01\n",
       "5      AVATOP  -0.018221  0.034199  5.946568e-01\n",
       "6       ADEKZ  -0.002312  0.037170  9.504434e-01\n",
       "7      ARAUCH  -0.011440  0.032000  7.210342e-01\n",
       "8     AGEBGEW   0.014338  0.014085  3.096774e-01\n",
       "9     FSNIGHT  -0.069022  0.049439  1.639352e-01\n",
       "10    FLGROSS   0.167469  0.023176  6.173284e-12\n",
       "11      FMILB  -0.012161  0.018565  5.130585e-01\n",
       "12     FNOH24  -0.051352  0.018488  5.896828e-03\n",
       "13      FTIER   0.004552  0.016549  7.834923e-01\n",
       "14      FPOLL  -0.003102  0.028241  9.126101e-01\n",
       "15   FLTOTMED  -0.023009  0.014686  1.184741e-01\n",
       "16     FO3H24   0.065951  0.031831  3.931592e-02\n",
       "17       FSPT  -0.045687  0.071990  5.262595e-01\n",
       "18     FTEH24  -0.042416  0.029092  1.461110e-01\n",
       "19     FSATEM   0.261347  0.082907  1.820113e-03\n",
       "20     FSAUGE  -0.029349  0.048436  5.451069e-01\n",
       "21      FLGEW   0.100324  0.021777  6.553868e-06\n",
       "22     FSPFEI  -0.015773  0.095976  8.695953e-01\n",
       "23    FSHLAUF  -0.042896  0.071160  5.471846e-01\n",
       "24       Male   0.093053  0.018485  9.263729e-07\n",
       "25     Female  -0.093053  0.019623  3.577259e-06"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_formatted_code = \"# OLS on train data\\nols_reg = skllm.LinearRegression().fit(X_train, y_train)\\n# R2 test score\\nr2 = ols_reg.score(X_test, y_test)\\n# Using the function in the above cell to get table\\ncoeffs_table = get_summary_linear_model(ols_reg, X_train, y_train)\\n# Using Numpy indexing stuff to get feature with lowest p-value\\nmost_important = coeffs_table[\\\"Feature\\\"].values[1:][\\n    np.argmin(coeffs_table[\\\"p-values\\\"].values[1:])\\n]\\n# Printing results\\nprint(f\\\"Got an R^2 score of {r2:.2f} for the test set.\\\")\\nprint(f\\\"The most important feature (lowest p-value) is {most_important}.\\\")\\ncoeffs_table\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# OLS on train data\n",
    "ols_reg = skllm.LinearRegression().fit(X_train, y_train)\n",
    "# R2 test score\n",
    "r2 = ols_reg.score(X_test, y_test)\n",
    "# Using the function in the above cell to get table\n",
    "coeffs_table = get_summary_linear_model(ols_reg, X_train, y_train)\n",
    "# Using Numpy indexing stuff to get feature with lowest p-value\n",
    "most_important = coeffs_table[\"Feature\"].values[1:][\n",
    "    np.argmin(coeffs_table[\"p-values\"].values[1:])\n",
    "]\n",
    "# Printing results\n",
    "print(f\"Got an R^2 score of {r2:.2f} for the test set.\")\n",
    "print(f\"The most important feature (lowest p-value) is {most_important}.\")\n",
    "coeffs_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important (lower p-value) feature seems to be FLGROSS. Some other important features seem to be gender. Male and female seem to completely cancel each other, implying that men are of higher risk?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1.3\n",
    "Scikit-learn for some reason doesn't have built in forward and backward selection, so I will create my own functions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 5;\n",
       "                var nbb_formatted_code = \"def backward_elimination(regressor, X_train, y_train, max_p_limit):\\n    \\\"\\\"\\\"\\n    Takes a regressor, training set and a max p-value, runs backward\\n    elimination and returns the regresson fitted on the reduced\\n    features, the reduced feature matrix, a table of betas, \\n    standard deviations and p-values and the removed features\\n    \\\"\\\"\\\"\\n    # Fitting regressor on full model\\n    regressor.fit(X_train, y_train)\\n    # Getting table of p-values to find what to eliminate\\n    result_table = get_summary_linear_model(regressor, X_train, y_train)\\n    p_values = result_table[\\\"p-values\\\"].values\\n    p_val_max, p_val_max_pos = p_values[1:].max(), p_values[1:].argmax() + 1\\n    # Getting name of feature with highest p-val to make list of removed features\\n    feature_max_p_val = result_table[\\\"Feature\\\"][p_val_max_pos]\\n    removed_features = [feature_max_p_val]\\n    # Dropping feature with highest p-val\\n    X_reduce = X_train.drop(columns=feature_max_p_val, inplace=False)\\n    # Running backwards elimination until all p-values are below limit\\n    while p_val_max > max_p_limit:\\n        # Fitting on reduced model\\n        regressor.fit(X_reduce, y_train)\\n        result_table = get_summary_linear_model(regressor, X_reduce, y_train)\\n        p_values = result_table[\\\"p-values\\\"].values\\n        p_val_max, p_val_max_pos = p_values[1:].max(), p_values[1:].argmax() + 1\\n        feature_max_p_val = result_table[\\\"Feature\\\"][p_val_max_pos]\\n        # If one or more features have to high p-value, remove\\n        if p_val_max > max_p_limit:\\n            # Append name to list that keeps track of removed features\\n            removed_features.append(feature_max_p_val)\\n            # Dropping feature\\n            X_reduce.drop(columns=feature_max_p_val, inplace=True)\\n            # Sorting features\\n            X_reduce.sort_index(axis=1, inplace=True)\\n            # Fitting reduced model\\n            regressor.fit(X_reduce, y_train)\\n\\n    return regressor, X_reduce, result_table, removed_features\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def backward_elimination(regressor, X_train, y_train, max_p_limit):\n",
    "    \"\"\"\n",
    "    Takes a regressor, training set and a max p-value, runs backward\n",
    "    elimination and returns the regresson fitted on the reduced\n",
    "    features, the reduced feature matrix, a table of betas, \n",
    "    standard deviations and p-values and the removed features\n",
    "    \"\"\"\n",
    "    # Fitting regressor on full model\n",
    "    regressor.fit(X_train, y_train)\n",
    "    # Getting table of p-values to find what to eliminate\n",
    "    result_table = get_summary_linear_model(regressor, X_train, y_train)\n",
    "    p_values = result_table[\"p-values\"].values\n",
    "    p_val_max, p_val_max_pos = p_values[1:].max(), p_values[1:].argmax() + 1\n",
    "    # Getting name of feature with highest p-val to make list of removed features\n",
    "    feature_max_p_val = result_table[\"Feature\"][p_val_max_pos]\n",
    "    removed_features = [feature_max_p_val]\n",
    "    # Dropping feature with highest p-val\n",
    "    X_reduce = X_train.drop(columns=feature_max_p_val, inplace=False)\n",
    "    # Running backwards elimination until all p-values are below limit\n",
    "    while p_val_max > max_p_limit:\n",
    "        # Fitting on reduced model\n",
    "        regressor.fit(X_reduce, y_train)\n",
    "        result_table = get_summary_linear_model(regressor, X_reduce, y_train)\n",
    "        p_values = result_table[\"p-values\"].values\n",
    "        p_val_max, p_val_max_pos = p_values[1:].max(), p_values[1:].argmax() + 1\n",
    "        feature_max_p_val = result_table[\"Feature\"][p_val_max_pos]\n",
    "        # If one or more features have to high p-value, remove\n",
    "        if p_val_max > max_p_limit:\n",
    "            # Append name to list that keeps track of removed features\n",
    "            removed_features.append(feature_max_p_val)\n",
    "            # Dropping feature\n",
    "            X_reduce.drop(columns=feature_max_p_val, inplace=True)\n",
    "            # Sorting features\n",
    "            X_reduce.sort_index(axis=1, inplace=True)\n",
    "            # Fitting reduced model\n",
    "            regressor.fit(X_reduce, y_train)\n",
    "\n",
    "    return regressor, X_reduce, result_table, removed_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 score of full model: 0.57 Backward Model: 0.61\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>$\\beta_i$</th>\n",
       "      <th>$\\pm$</th>\n",
       "      <th>p-values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Intercept</td>\n",
       "      <td>2.290714</td>\n",
       "      <td>0.008879</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FLGEW</td>\n",
       "      <td>0.110877</td>\n",
       "      <td>0.019735</td>\n",
       "      <td>5.184692e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FLGROSS</td>\n",
       "      <td>0.172880</td>\n",
       "      <td>0.019860</td>\n",
       "      <td>4.440892e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Female</td>\n",
       "      <td>-0.096168</td>\n",
       "      <td>0.014058</td>\n",
       "      <td>6.155254e-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Male</td>\n",
       "      <td>0.096168</td>\n",
       "      <td>0.014264</td>\n",
       "      <td>1.096052e-10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Feature  $\\beta_i$     $\\pm$      p-values\n",
       "0  Intercept   2.290714  0.008879  0.000000e+00\n",
       "1      FLGEW   0.110877  0.019735  5.184692e-08\n",
       "2    FLGROSS   0.172880  0.019860  4.440892e-16\n",
       "3     Female  -0.096168  0.014058  6.155254e-11\n",
       "4       Male   0.096168  0.014264  1.096052e-10"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 6;\n",
       "                var nbb_formatted_code = \"# Model to send in to function. It is an OLS regressor.\\nbase_regressor = skllm.LinearRegression()\\n# Running backwards elimination\\n(\\n    regressor_reduced,\\n    X_reduce_train,\\n    result_table_reduced,\\n    removed_features,\\n) = backward_elimination(base_regressor, X_train, y_train, max_p_limit=1e-2)\\n# Using list of reduced features to also create test feature matrix with same features\\nX_reduce_test = X_test.drop(columns=removed_features).sort_index(axis=1)\\n# Printing results\\nprint(\\n    f\\\"R^2 score of full model: {ols_reg.score(X_test, y_test):.2f} Backward Model: {regressor_reduced.score(X_reduce_test, y_test):.2f}\\\"\\n)\\nresult_table_reduced\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Model to send in to function. It is an OLS regressor.\n",
    "base_regressor = skllm.LinearRegression()\n",
    "# Running backwards elimination\n",
    "(\n",
    "    regressor_reduced,\n",
    "    X_reduce_train,\n",
    "    result_table_reduced,\n",
    "    removed_features,\n",
    ") = backward_elimination(base_regressor, X_train, y_train, max_p_limit=1e-2)\n",
    "# Using list of reduced features to also create test feature matrix with same features\n",
    "X_reduce_test = X_test.drop(columns=removed_features).sort_index(axis=1)\n",
    "# Printing results\n",
    "print(\n",
    "    f\"R^2 score of full model: {ols_reg.score(X_test, y_test):.2f} Backward Model: {regressor_reduced.score(X_reduce_test, y_test):.2f}\"\n",
    ")\n",
    "result_table_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 7;\n",
       "                var nbb_formatted_code = \"def forward_selection(regressor, X_train, y_train, max_p_limit):\\n    \\\"\\\"\\\"\\n    Takes a regressor, training set and a max p-value, runs forward\\n    selection and returns the regresson fitted on the reduced\\n    features, the reduced feature matrix, a table of betas, \\n    standard deviations and p-values and the removed features\\n    \\\"\\\"\\\"\\n    X_null = pd.DataFrame({\\\"null\\\": np.zeros_like(y_train)})\\n    regressor.fit(X_null, y_train)\\n    # The p-value for the 0-column is invalid, but also not used, so I ignore the warnings\\n    with np.errstate(invalid=\\\"ignore\\\"):\\n        # Getting results for null-model\\n        result_table = get_summary_linear_model(regressor, X_null, y_train)\\n    # p-value for intercept\\n    p_val_max = result_table[\\\"p-values\\\"][0]\\n    # Dataframe used for incresing\\n    X_increased = pd.DataFrame()\\n    # List of features\\n    features = X_train.columns.values\\n    # while max p-val is below threshold, repeat\\n    while p_val_max < max_p_limit:\\n        # Set best p to infinity so that all values are less\\n        best_p = np.inf\\n        # Looping over features\\n        for feature in features:\\n            # Creating new column with feature in loop\\n            new_col = pd.DataFrame({feature: X_train[feature].values})\\n            # If null model we need to append to the dataframe differently than usual\\n            if len(X_increased.values) == 0:\\n                # Adding new feature to null model\\n                X_candidate = X_increased.append(new_col)\\n            else:\\n                # Adding new feature to model\\n                new_col_names = np.append(\\n                    X_increased.columns.values, new_col.columns.values\\n                )\\n                X_candidate = pd.DataFrame(\\n                    np.append(X_increased.values, new_col.values, axis=1),\\n                    columns=new_col_names,\\n                )\\n            # Fitting increased model to find p-value\\n            regressor.fit(X_candidate, y_train)\\n            result_table = get_summary_linear_model(regressor, X_candidate, y_train)\\n            p_i = result_table[\\\"p-values\\\"].values[-1]\\n            # This if-statement is used to find the minimum p-value of the potential features to add\\n            if p_i < best_p:\\n                best_p = p_i\\n                best_new_feature = feature\\n        # Now that we have the best feature to add, we add it properly\\n        new_col = pd.DataFrame({best_new_feature: X_train[best_new_feature].values})\\n        if len(X_increased.values) == 0:\\n            X_candidate = X_increased.append(new_col)\\n        else:\\n            new_col_names = np.append(\\n                X_increased.columns.values, new_col.columns.values\\n            )\\n            X_candidate = pd.DataFrame(\\n                np.append(X_increased.values, new_col.values, axis=1),\\n                columns=new_col_names,\\n            )\\n        # Get results for new model\\n        result_table = get_summary_linear_model(regressor, X_candidate, y_train)\\n        p_val_max = result_table[\\\"p-values\\\"].values.max()\\n\\n        # Sorting features\\n        X_increased = X_candidate.sort_index(axis=1)\\n        # Removing added feature from list of potential featues so that we can't add it again next iteration\\n        features = features[features != best_new_feature]\\n\\n    # List of omitted features\\n    omitted_features = features\\n    # Fitting increased model\\n    regressor.fit(X_increased, y_train)\\n    # Table of results for best model\\n    result_table_best = get_summary_linear_model(regressor, X_increased, y_train)\\n    return regressor, X_increased, result_table_best, omitted_features\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def forward_selection(regressor, X_train, y_train, max_p_limit):\n",
    "    \"\"\"\n",
    "    Takes a regressor, training set and a max p-value, runs forward\n",
    "    selection and returns the regresson fitted on the reduced\n",
    "    features, the reduced feature matrix, a table of betas, \n",
    "    standard deviations and p-values and the removed features\n",
    "    \"\"\"\n",
    "    X_null = pd.DataFrame({\"null\": np.zeros_like(y_train)})\n",
    "    regressor.fit(X_null, y_train)\n",
    "    # The p-value for the 0-column is invalid, but also not used, so I ignore the warnings\n",
    "    with np.errstate(invalid=\"ignore\"):\n",
    "        # Getting results for null-model\n",
    "        result_table = get_summary_linear_model(regressor, X_null, y_train)\n",
    "    # p-value for intercept\n",
    "    p_val_max = result_table[\"p-values\"][0]\n",
    "    # Dataframe used for incresing\n",
    "    X_increased = pd.DataFrame()\n",
    "    # List of features\n",
    "    features = X_train.columns.values\n",
    "    # while max p-val is below threshold, repeat\n",
    "    while p_val_max < max_p_limit:\n",
    "        # Set best p to infinity so that all values are less\n",
    "        best_p = np.inf\n",
    "        # Looping over features\n",
    "        for feature in features:\n",
    "            # Creating new column with feature in loop\n",
    "            new_col = pd.DataFrame({feature: X_train[feature].values})\n",
    "            # If null model we need to append to the dataframe differently than usual\n",
    "            if len(X_increased.values) == 0:\n",
    "                # Adding new feature to null model\n",
    "                X_candidate = X_increased.append(new_col)\n",
    "            else:\n",
    "                # Adding new feature to model\n",
    "                new_col_names = np.append(\n",
    "                    X_increased.columns.values, new_col.columns.values\n",
    "                )\n",
    "                X_candidate = pd.DataFrame(\n",
    "                    np.append(X_increased.values, new_col.values, axis=1),\n",
    "                    columns=new_col_names,\n",
    "                )\n",
    "            # Fitting increased model to find p-value\n",
    "            regressor.fit(X_candidate, y_train)\n",
    "            result_table = get_summary_linear_model(regressor, X_candidate, y_train)\n",
    "            p_i = result_table[\"p-values\"].values[-1]\n",
    "            # This if-statement is used to find the minimum p-value of the potential features to add\n",
    "            if p_i < best_p:\n",
    "                best_p = p_i\n",
    "                best_new_feature = feature\n",
    "        # Now that we have the best feature to add, we add it properly\n",
    "        new_col = pd.DataFrame({best_new_feature: X_train[best_new_feature].values})\n",
    "        if len(X_increased.values) == 0:\n",
    "            X_candidate = X_increased.append(new_col)\n",
    "        else:\n",
    "            new_col_names = np.append(\n",
    "                X_increased.columns.values, new_col.columns.values\n",
    "            )\n",
    "            X_candidate = pd.DataFrame(\n",
    "                np.append(X_increased.values, new_col.values, axis=1),\n",
    "                columns=new_col_names,\n",
    "            )\n",
    "        # Get results for new model\n",
    "        result_table = get_summary_linear_model(regressor, X_candidate, y_train)\n",
    "        p_val_max = result_table[\"p-values\"].values.max()\n",
    "\n",
    "        # Sorting features\n",
    "        X_increased = X_candidate.sort_index(axis=1)\n",
    "        # Removing added feature from list of potential featues so that we can't add it again next iteration\n",
    "        features = features[features != best_new_feature]\n",
    "\n",
    "    # List of omitted features\n",
    "    omitted_features = features\n",
    "    # Fitting increased model\n",
    "    regressor.fit(X_increased, y_train)\n",
    "    # Table of results for best model\n",
    "    result_table_best = get_summary_linear_model(regressor, X_increased, y_train)\n",
    "    return regressor, X_increased, result_table_best, omitted_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 score of forward model: 0.61\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>$\\beta_i$</th>\n",
       "      <th>$\\pm$</th>\n",
       "      <th>p-values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Intercept</td>\n",
       "      <td>2.290714</td>\n",
       "      <td>0.008879</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FLGEW</td>\n",
       "      <td>0.110877</td>\n",
       "      <td>0.019735</td>\n",
       "      <td>5.184692e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FLGROSS</td>\n",
       "      <td>0.172880</td>\n",
       "      <td>0.019860</td>\n",
       "      <td>4.440892e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Female</td>\n",
       "      <td>-0.096168</td>\n",
       "      <td>0.014058</td>\n",
       "      <td>6.155254e-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Male</td>\n",
       "      <td>0.096168</td>\n",
       "      <td>0.014264</td>\n",
       "      <td>1.096052e-10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Feature  $\\beta_i$     $\\pm$      p-values\n",
       "0  Intercept   2.290714  0.008879  0.000000e+00\n",
       "1      FLGEW   0.110877  0.019735  5.184692e-08\n",
       "2    FLGROSS   0.172880  0.019860  4.440892e-16\n",
       "3     Female  -0.096168  0.014058  6.155254e-11\n",
       "4       Male   0.096168  0.014264  1.096052e-10"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 8;\n",
       "                var nbb_formatted_code = \"# Model to send in to function. It is an OLS regressor.\\nbase_regressor = skllm.LinearRegression()\\n# Running forward selection\\n(\\n    regressor_increased,\\n    X_increased_train,\\n    result_table_increased,\\n    omitted_features_increased,\\n) = forward_selection(base_regressor, X_train, y_train, max_p_limit=1e-2)\\n# Using list of reduced features to also create test feature matrix with same features\\nX_increased_test = X_test.drop(columns=omitted_features_increased).sort_index(axis=1)\\n# Printing results\\nprint(\\n    f\\\"R^2 score of forward model: {regressor_increased.score(X_increased_test, y_test):.2f}\\\"\\n)\\nresult_table_increased\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Model to send in to function. It is an OLS regressor.\n",
    "base_regressor = skllm.LinearRegression()\n",
    "# Running forward selection\n",
    "(\n",
    "    regressor_increased,\n",
    "    X_increased_train,\n",
    "    result_table_increased,\n",
    "    omitted_features_increased,\n",
    ") = forward_selection(base_regressor, X_train, y_train, max_p_limit=1e-2)\n",
    "# Using list of reduced features to also create test feature matrix with same features\n",
    "X_increased_test = X_test.drop(columns=omitted_features_increased).sort_index(axis=1)\n",
    "# Printing results\n",
    "print(\n",
    "    f\"R^2 score of forward model: {regressor_increased.score(X_increased_test, y_test):.2f}\"\n",
    ")\n",
    "result_table_increased"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the reduced models with $p_\\text{max}=0.01$, both forward and backward selection give the exact same model, and therefore the same features. They also get a slightly better $R^2$-score, possibly because they have less features, and are therefore less likely to overfit on the training data. I chose to look at $R^2$ instead of MSE as I feel it is a more intuitive value. However, higher $R^2$ also implies lower MSE, so the models are better.\n",
    "\n",
    "Next I will test with a less strict $p_\\text{max}=0.1$ and see how the models perform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 score of full model: 0.57 Backward Model: 0.60\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>$\\beta_i$</th>\n",
       "      <th>$\\pm$</th>\n",
       "      <th>p-values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Intercept</td>\n",
       "      <td>2.354737</td>\n",
       "      <td>0.018622</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FLGEW</td>\n",
       "      <td>0.105261</td>\n",
       "      <td>0.019636</td>\n",
       "      <td>1.903341e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FLGROSS</td>\n",
       "      <td>0.171637</td>\n",
       "      <td>0.019614</td>\n",
       "      <td>2.220446e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FLTOTMED</td>\n",
       "      <td>-0.028146</td>\n",
       "      <td>0.013471</td>\n",
       "      <td>3.769980e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FNOH24</td>\n",
       "      <td>-0.051155</td>\n",
       "      <td>0.017655</td>\n",
       "      <td>4.099075e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>FO3H24</td>\n",
       "      <td>0.030305</td>\n",
       "      <td>0.015344</td>\n",
       "      <td>4.937143e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>FSATEM</td>\n",
       "      <td>0.193437</td>\n",
       "      <td>0.061164</td>\n",
       "      <td>1.759272e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>FSPT</td>\n",
       "      <td>-0.058479</td>\n",
       "      <td>0.029742</td>\n",
       "      <td>5.039546e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Female</td>\n",
       "      <td>-0.097184</td>\n",
       "      <td>0.015939</td>\n",
       "      <td>4.128840e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>HOCHOZON</td>\n",
       "      <td>-0.091458</td>\n",
       "      <td>0.036730</td>\n",
       "      <td>1.343162e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Male</td>\n",
       "      <td>0.097184</td>\n",
       "      <td>0.016291</td>\n",
       "      <td>8.404746e-09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Feature  $\\beta_i$     $\\pm$      p-values\n",
       "0   Intercept   2.354737  0.018622  0.000000e+00\n",
       "1       FLGEW   0.105261  0.019636  1.903341e-07\n",
       "2     FLGROSS   0.171637  0.019614  2.220446e-16\n",
       "3    FLTOTMED  -0.028146  0.013471  3.769980e-02\n",
       "4      FNOH24  -0.051155  0.017655  4.099075e-03\n",
       "5      FO3H24   0.030305  0.015344  4.937143e-02\n",
       "6      FSATEM   0.193437  0.061164  1.759272e-03\n",
       "7        FSPT  -0.058479  0.029742  5.039546e-02\n",
       "8      Female  -0.097184  0.015939  4.128840e-09\n",
       "9    HOCHOZON  -0.091458  0.036730  1.343162e-02\n",
       "10       Male   0.097184  0.016291  8.404746e-09"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 9;\n",
       "                var nbb_formatted_code = \"# Model to send in to function. It is an OLS regressor.\\nbase_regressor = skllm.LinearRegression()\\n# Running backwards elimination\\n(\\n    regressor_reduced_less_strict,\\n    X_reduce_train_less_strict,\\n    result_table_reduced_less_strict,\\n    removed_features_less_strict,\\n) = backward_elimination(base_regressor, X_train, y_train, max_p_limit=1e-1)\\n# Using list of reduced features to also create test feature matrix with same features\\nX_reduce_test_less_strict = X_test.drop(\\n    columns=removed_features_less_strict\\n).sort_index(axis=1)\\n# Printing results\\nprint(\\n    f\\\"R^2 score of full model: {ols_reg.score(X_test, y_test):.2f}\\\"\\n    + f\\\" Backward Model: {regressor_reduced_less_strict.score(X_reduce_test_less_strict, y_test):.2f}\\\"\\n)\\nresult_table_reduced_less_strict\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Model to send in to function. It is an OLS regressor.\n",
    "base_regressor = skllm.LinearRegression()\n",
    "# Running backwards elimination\n",
    "(\n",
    "    regressor_reduced_less_strict,\n",
    "    X_reduce_train_less_strict,\n",
    "    result_table_reduced_less_strict,\n",
    "    removed_features_less_strict,\n",
    ") = backward_elimination(base_regressor, X_train, y_train, max_p_limit=1e-1)\n",
    "# Using list of reduced features to also create test feature matrix with same features\n",
    "X_reduce_test_less_strict = X_test.drop(\n",
    "    columns=removed_features_less_strict\n",
    ").sort_index(axis=1)\n",
    "# Printing results\n",
    "print(\n",
    "    f\"R^2 score of full model: {ols_reg.score(X_test, y_test):.2f}\"\n",
    "    + f\" Backward Model: {regressor_reduced_less_strict.score(X_reduce_test_less_strict, y_test):.2f}\"\n",
    ")\n",
    "result_table_reduced_less_strict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 score of forward model: 0.61\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>$\\beta_i$</th>\n",
       "      <th>$\\pm$</th>\n",
       "      <th>p-values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Intercept</td>\n",
       "      <td>2.290714</td>\n",
       "      <td>0.008879</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FLGEW</td>\n",
       "      <td>0.110877</td>\n",
       "      <td>0.019735</td>\n",
       "      <td>5.184692e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FLGROSS</td>\n",
       "      <td>0.172880</td>\n",
       "      <td>0.019860</td>\n",
       "      <td>4.440892e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Female</td>\n",
       "      <td>-0.096168</td>\n",
       "      <td>0.014058</td>\n",
       "      <td>6.155254e-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Male</td>\n",
       "      <td>0.096168</td>\n",
       "      <td>0.014264</td>\n",
       "      <td>1.096052e-10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Feature  $\\beta_i$     $\\pm$      p-values\n",
       "0  Intercept   2.290714  0.008879  0.000000e+00\n",
       "1      FLGEW   0.110877  0.019735  5.184692e-08\n",
       "2    FLGROSS   0.172880  0.019860  4.440892e-16\n",
       "3     Female  -0.096168  0.014058  6.155254e-11\n",
       "4       Male   0.096168  0.014264  1.096052e-10"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 10;\n",
       "                var nbb_formatted_code = \"# Model to send in to function. It is an OLS regressor.\\nbase_regressor = skllm.LinearRegression()\\n# Running forward selection\\n(\\n    regressor_increased_less_strict,\\n    X_increased_train_less_strict,\\n    result_table_increased_less_strict,\\n    omitted_features_increased_less_strict,\\n) = forward_selection(base_regressor, X_train, y_train, max_p_limit=1e-1)\\n# Using list of reduced features to also create test feature matrix with same features\\nX_increased_test_less_strict = X_test.drop(\\n    columns=omitted_features_increased_less_strict\\n).sort_index(axis=1)\\n# Printing results\\nprint(\\n    f\\\"R^2 score of forward model: {regressor_increased.score(X_increased_test, y_test):.2f}\\\"\\n)\\nresult_table_increased_less_strict\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Model to send in to function. It is an OLS regressor.\n",
    "base_regressor = skllm.LinearRegression()\n",
    "# Running forward selection\n",
    "(\n",
    "    regressor_increased_less_strict,\n",
    "    X_increased_train_less_strict,\n",
    "    result_table_increased_less_strict,\n",
    "    omitted_features_increased_less_strict,\n",
    ") = forward_selection(base_regressor, X_train, y_train, max_p_limit=1e-1)\n",
    "# Using list of reduced features to also create test feature matrix with same features\n",
    "X_increased_test_less_strict = X_test.drop(\n",
    "    columns=omitted_features_increased_less_strict\n",
    ").sort_index(axis=1)\n",
    "# Printing results\n",
    "print(\n",
    "    f\"R^2 score of forward model: {regressor_increased.score(X_increased_test, y_test):.2f}\"\n",
    ")\n",
    "result_table_increased_less_strict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the models are not the same anymore. This is to be expected, as the p-values estimated are not the same for each feature independent of the other features. The backward elimination model seems to give a better $R^2$-score this time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1.4\n",
    "CV is easily implemented in Scikit-Learn. Bootstap on the other hand... I need to create my own class (Maybe there is a better way of doing this than what I'm doing...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEeCAYAAACQfIJ4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3deZhcZZn38e+v1+x7CBASEiAsAZSlCZuiI7sLcRxQQEUclMF30NEZHfGdcXmZcWFGHXXQUVAURDZxi4KiwiAiW5oQQkLISvatk+4k3Ul6rfv945yGoqhOOqGrq6rz+1xXXX2W5zznrjrd567neU6fo4jAzMwsV0WxAzAzs9LkBGFmZnk5QZiZWV5OEGZmlpcThJmZ5eUEYWZmeTlB7IcknSlpiaQWSe/cQ9kvSLp9N+tXSDqn76MsHZJ+JOnf0+k3SlrUm7L7uK8WSYft6/a7qXfAH6dckh6W9KFix1HOnCAGgPQPoTU9ubTs7gSWuh64MSKGRcQv+yPGgSIi/hwRR/VFXflOYOkxWd4X9dvLJF0p6dFix1FunCAGjmvTk8uwXpzADgUW9EdQvSGpqtgxWHH42Jc2J4j9jKRlwGHAr9PWRq2kgyXNktQoaamkD+9m+/dLWilpi6R/2cO+Bkv6Wlp+m6RH02VTJIWkqyStAh5Ky18kaYGkrem362Oy6vq0pLWSmiUtknR2unyGpHpJ2yVtlPT1HmJZKOntWfNVkhoknZTO/1TShjTORyQd20M9b5a0Jmv+RElz0rjuBgZlrRst6TfpfprS6UPSdV8E3gjcmB6HG9PlIemIdHqkpNvS7VdK+ldJFem6K9PP86tp3S9KunB3xyMrrhmSHk8/5/WSbpRUk66TpP+StCn9TJ+TdFy67q2Snk/f61pJn8yq88Pp705j+rt0cA/77unYnybpsTSmZyW9OWubKyUtT/f7oqT3pstf0f2ZVXdVzj6PAb4LnJ5+1lv39H4sFRF+lfkLeBhoADYDfwHevIfyK4BzsuYfAb5DcnI7Ia3rLem6LwC3p9PTgRbgLKAW+DrQmV1Xzn6+ncY2EagEzki3mwIEcBswFBgMHAnsAM4FqoF/BpYCNcBRwGrg4LTeKcDh6fTjwPvT6WHAaT3E8jngJ1nzbwMWZs3/LTA8je8bwNysdT8C/j2dfjOwJp2uAVYCn0hjvhjoyCo7FvgbYEha90+BX+Yctw/lxBnAEen0bcCv0m2nAIuBq9J1V6b7+nD62X4EWAdoT8ccOBk4DahK610IfDxddz7wNDAKEHAMcFC6bj3wxnR6NHBSOv0Wkt+9k9LP77+BR3qII9+xnwhsAd5K8qX13HR+fFpmO3BUuv1BwLG5v5s5dVflfr7p5/VoTix5349fL7/cghgYPk3SKpgI3ETSOji8NxtKmgScCXw6IlojYi7wfeCKPMUvBn4TEY9ERBvwWSDTQ70VJCfdf4iItRHRFRGPpdt1+0JE7IiIXcB7gPsi4g8R0QF8leTkcQbQRXLimS6pOiJWRMSytI4O4AhJ4yKiJSKe6OGt3gFcJGlIOn85cGf3yoi4JSKa0/i+ALxe0siePzkgOclWA9+IiI6IuBeYnVXnloj4WUTsjIhm4IvAm/ZQJwCSKoFLgc+kca0Avga8P6vYyoi4OSK6gFtJTp4T9lR3RDwdEU9ERGda7/ey4uogSUhHkySbhRGxPmvddEkjIqIpIuaky98L3BIRc9LP7zMk39an7CaM7GP/PuD+iLg/IjIR8QegniRhQPI7dpykwRGxPiL6qnu0p/djKSeIASAinuw+uUXErSStiLcCpF023YPXb8yz+cFAY3oC67aSJNnkK7s6a787SL7p5TOOpEWyrIf1ZNeV1r0yq+5Mun5iRCwFPk5y4t4k6a6sLoyrSFofL0iand2NlC2tYyHwjjRJXESSNJBUKekrkpZJ2k7ybbv7PezOwcDaSL+Cpl56D5KGSPpe2j20naSlNio9+e/JOJLkszJrWe5x2ZD1/namk8P2VLGkI9Purg1pXF9K90dEPATcSNL62yTpJkkj0k3/huT3aqWkP0k6PV2ee+xaSH4v8v0Odcs+9ocCl6TdS1vTLqA3kLRcdpB8ebgGWC/pPklH7+k99lJP78dSThADU5B0DxARx8bLg9d/zlN2HTBG0vCsZZOBtXnKrgcmdc+kJ9qxPcSwGWgFdteSyT6xriM5UXTXrXRfa9P3cUdEvCEtE8AN6fIlEXEZcEC67F5JQ3vY353AZcBM4Pk0aUDSmpgJnAOMJOmqgPQz3I31wMQ01m6Ts6b/iaR77NSIGEHSNZdd7+5upbyZ5BvuoVnLejoue+t/gBeAaWlc/zcrJiLiWxFxMkmX4pHAp9LlsyNiJsln/UvgnnST3GM3lOT3YnexZr/31cCPI2JU1mtoRHwl3e8DEXEuSQvpBeDmdLsdJN133Q7s5f6632dP78dSThBlTtIoSedLGqRk4PW9JCei3/Vm+4hYDTwGfDmt43Uk38rz/e/DvcDbJb0hHdS8nh5+h9IWwC3A15UMgldKOl1SbQ+h3AO8TdLZkqpJTq5twGOSjpL0lnTbVmAXadeWpPdJGp/ub2taV95uL+Au4DyS/vo7spYPT/e1heSE86Uets/1OMkYzMckVUt6FzAjp95dwFZJY4DP52y/kaRr8FXSbqN7gC9KGi7pUOAfyX9c9tZwkn79lvTb+Ee6V0g6RdKp6THYQfJ5ZyTVSHqvpJFpF+B2Xv6c7wQ+KOmE9Bh9CXgy7b7qjdtJWnbnp78ng5RcDHCIpAmSZqZJp41kDKx7v3OBsyRNTrsDP7ObfWwEDtHLg/G7ez+WcoIof9XAv/PyIPVHgXdGxOK9qOMykm/N64BfAJ+PiD/mFkr7fv+e5OS6HmgC1uSWy/JJ4DmSfvlGkm/4PSWURSR90f+dvo93AO+IiHaS8YevpMs3kHzj6z4ZXAAskNQCfBO4NO3XzreP9SQn9TOAu7NW3UbSRbIWeB7oaRwjt7524F0kA6CNJF0hP88q8g2ScZTNaZ25SfubwMVKrkL6Vp5dfJTkJL0ceJTkc7+lN7HtwSdJWk3NJN/Gsz+LEemyJpLPZAvwn+m69wMr0m6pa0jGHkh/Vz4L/Izk9+JwkvGTXkm/pMwkack0kLQoPkXyu1JBkhjXkXzGbyJNaOlYxd3APJKB9d/sZjcPkVzavUHS5t29H3uZXtl9amZmlnALwszM8nKCMDOzvJwgzMwsLycIMzPLywnCzMzyGlB3Uhw3blxMmTKl2GGYmZWVp59+enNEjM9dPqASxJQpU6ivry92GGZmZUXSynzL3cVkZmZ5OUGYmVleThBmZpaXE4SZmeXlBGFmZnk5QZiZWV5OEGZmZay1o4tfP7uOrTvb+7xuJwgzszI2Z1UTH73zGZ5ZtXXPhfeSE4SZWRl76sVGJDh5yug+r9sJwsysjD31YiPTDxrBiEHVfV63E4SZWZlq78wwZ1UTp0wZU5D6nSDMzMrU/HXbaO3IcOpUJwgzM8vy1IuNAJziBGFmZtmeerGRw8cPZdyw2oLU7wRhZlaGujLB7BWNzJg6tmD7cIIwMytDizY009zaWbDxB3CCMDMrS0+9uAUo3PgDOEGYmZWlp1Y0MnHUYCaOGlywfRQ0QUi6QNIiSUslXZdn/VmS5kjqlHRx1vITJD0uaYGkeZLeU8g4zczKSUTw1IuNBe1eggImCEmVwLeBC4HpwGWSpucUWwVcCdyRs3wncEVEHAtcAHxD0qhCxWpmVk6Wb97B5pZ2ZhQ4QVQVsO4ZwNKIWA4g6S5gJvB8d4GIWJGuy2RvGBGLs6bXSdoEjAf6/m5UZmZl5ukVTQDUFeD+S9kK2cU0EVidNb8mXbZXJM0AaoBlPay/WlK9pPqGhoZ9CtTMrJw8s3orIwZVcdi4YQXdT0kPUks6CPgx8MGIyOQrExE3RURdRNSNHz++fwM0MyuCZ1Y18fpJo6ioUEH3U8gEsRaYlDV/SLqsVySNAO4D/iUinujj2MzMytKOtk4Wb2zmxMmF7V6CwiaI2cA0SVMl1QCXArN6s2Fa/hfAbRFxbwFjNDMrK/PWbCMTcOKkwl+3U7AEERGdwLXAA8BC4J6IWCDpekkXAUg6RdIa4BLge5IWpJu/GzgLuFLS3PR1QqFiNTMrF3NXJ9fqnNAPCaKQVzEREfcD9+cs+1zW9GySrqfc7W4Hbi9kbGZm5eiZVU1MGTuE0UNrCr6vkh6kNjOzl0UEz6ze2i/jD+AEYWZWNtZta6Whua1fupfACcLMrGzMXdV/4w/gBGFmVjaeWdVETVUFxxw0ol/25wRhZlYm5q7eynEHj6Cmqn9O3U4QZmZloKMrw3Nrt/XbADU4QZiZlYUX1jfT1pnpt/EHcIIwMysL89b27wA1OEGYmZWFxRuaGVpTySGjC/cEuVxOEGZmZWDxxhaOmDAcqbB3cM3mBGFmVgaWbGrmqAmFff5DLicIM7MSt6Wljc0t7Rw5YXi/7tcJwsysxC3e2ALANCcIMzPLtmRTMwBHuovJzMyyLd7YzPBBVRw4YlC/7regCULSBZIWSVoq6bo868+SNEdSp6SLc9b9TtJWSb8pZIxmZqVu8YYWjuznK5iggAlCUiXwbeBCYDpwmaTpOcVWAVcCd+Sp4j+B9xcqPjOzchARLN7U3O/dS1DYFsQMYGlELI+IduAuYGZ2gYhYERHzgEzuxhHxINBcwPjMzEpeQ0sbW3d29PsVTFDYBDERWJ01vyZdZmZmvbQkvYJpoCWIfiHpakn1kuobGhqKHY6ZWZ9atCHpSJk2wLqY1gKTsuYPSZf1qYi4KSLqIqJu/PjxfV29mVlRLdnUzOgh1YwfVtvv+y5kgpgNTJM0VVINcCkwq4D7MzMbcBZvbGFaEa5gggImiIjoBK4FHgAWAvdExAJJ10u6CEDSKZLWAJcA35O0oHt7SX8GfgqcLWmNpPMLFauZWSmKCBZvLM4VTABVhaw8Iu4H7s9Z9rms6dkkXU/5tn1jIWMzMyt1G7a30tzaWZQBahgAg9RmZgPV4iJewQROEGZmJWvxhu57MDlBmJlZlgXrtnHQyEGMGVpTlP07QZiZlaj567Zz7MEji7Z/JwgzsxK0s72TZQ0tHDdxRNFicIIwMytBC9dvJwKOcwvCzMyyzV+7HYDjJjpBmJlZlvlrtzF2aA0TRvT/LTa6OUGYmZWg+eu2c+zEkUW5xUY3JwgzsxLT2tHFko3NHHdw8QaowQnCzKzkLN7YTGcmijr+AE4QZmYlZ8G6dIC6iFcwgROEmVnJmb92G8MHVTFpzOCixuEEYWZWYuav285xBxd3gBqcIMzMSkpHV4aF67cX9T+ouzlBmJmVkGUNLbR3Zoo+QA0FThCSLpC0SNJSSdflWX+WpDmSOiVdnLPuA5KWpK8PFDJOM7NS0f0f1MW8SV+3giUISZXAt4ELgenAZZKm5xRbBVwJ3JGz7Rjg88CpwAzg85JGFypWM7NSMW/NVobUVDJ13NBih1LQFsQMYGlELI+IduAuYGZ2gYhYERHzgEzOtucDf4iIxohoAv4AXFDAWM3MSsKcVU2cMGkUlRXFHaCGwiaIicDqrPk16bJCb2tmVpZ2tneycH0zJ00ujQ6Tsh+klnS1pHpJ9Q0NDcUOx8xsnz27ehtdmeCkQ0cVOxSgsAliLTApa/6QdFmfbhsRN0VEXUTUjR8/fp8CNTMrBXNWNQFw4qSB34KYDUyTNFVSDXApMKuX2z4AnCdpdDo4fV66zMxswJqzsonDxg9ldJGeQZ2rYAkiIjqBa0lO7AuBeyJigaTrJV0EIOkUSWuAS4DvSVqQbtsI/BtJkpkNXJ8uMzMbkCKCOauaOLlExh8AqgpZeUTcD9yfs+xzWdOzSbqP8m17C3BLIeMzMysVL27eQdPODk46tHQSRNkPUpuZDQRzVm0FKJkrmMAJwsysJMxZ1cTw2iqmHTCs2KG8xAnCzKwEzFnZxAmTR1FRAv8g180JwsysyJpbO1i0sZmTS2j8AZwgzMyKbu7qrUSU1vgDOEGYmRXdnJVbkeCEyaXxH9TdnCDMzIrsyRe3cNSE4YwYVF3sUF7BCcLMrIhaO7qoX9nEmUeMK3Yor+IEYWZWRHNWNdHemeGMw8cWO5RXcYIwMyuix5dtobJCzJg6ptihvIoThJlZET22bAvHTxzJ8BIbfwAnCDOzomlp6+TZ1VtLsnsJnCDMzIpm9opGOjPBGYeX3gA1OEGYmRXN48u2UFNZUXL/Qd3NCcLMrEgeW7aZEyePYnBNZbFDycsJwsysCLbubGfBuu0l270EBU4Qki6QtEjSUknX5VlfK+nudP2Tkqaky2sk/VDSc5KelfTmQsZpZtbfnljeSASccURpDlBDAROEpErg28CFwHTgMknTc4pdBTRFxBHAfwE3pMs/DBARxwPnAl+T5NaOmQ0Yjy/bzODqSl5/SGndfylbIU+6M4ClEbE8ItqBu4CZOWVmArem0/cCZ0sSSUJ5CCAiNgFbgboCxmpm1m8igv9d1MBph42hpqp0v/sWMrKJwOqs+TXpsrxlIqIT2AaMBZ4FLpJUJWkqcDIwKd9OJF0tqV5SfUNDQx+/BTOzvresYQerGnfylmMmFDuU3SrV1HULSUKpB74BPAZ05SsYETdFRF1E1I0fP74fQzQz2zcPvbARgLccfUCRI9m9qj0VkPRDIHpR1y8jYlbW/Fpe+a3/kHQZecqskVQFjAS2REQAn8iK4TFgcS9iMDMreQ8u3MTRBw5n4qjBxQ5lt/aYIIAf9bKuFTnzs4FpaRfRWuBS4PKcMrOADwCPAxcDD0VESBoCKCJ2SDoX6IyI53sZh5lZydq2q4P6lU1c86bDih3KHu0xQUTEn/al4ojolHQt8ABQCdwSEQskXQ/Up62NHwA/lrQUaCRJIgAHAA9IypAkl/fvSwxmZqXmkcUNdGWi5LuXoHddTPdExLvT6Rsi4tNZ634fEef1tG1E3A/cn7Psc1nTrcAlebZbARzVmzdgZlZOHnphE6OHVHPCpNK8vUa23gxST8uaPjdnnUeFzcx6qSsTPLxoE3911AFUVqjY4exRbxLE7gaoezN4bWZmwNzVTTTt7OAtx5R+9xL0bpB6iKQTSZLJ4HRa6au0h+DNzErIgws3UVUh3jitPDpfepMgNgBfzzPdPW9mZr3wx4UbqZsympGDS+/pcfn05iqmN/dDHGZmA9qyhhYWb2zhC+/IvSVd6drjGISkUyQdmDV/haRfSfqWpNJ7yraZWQn63fykw+W8Yw/cQ8nS0ZtB6u8B7QCSzgK+AtxGct+kmwoXmpnZwPHAgg28ftIoDi7x/57O1psEURkRjen0e4CbIuJnEfFZ4IjChWZmNjCs3bqLeWu2ceFx5dN6gF4miPQ+SQBnk96GO9WbQW4zs/1ad/fS+WXUvQS9O8HfCfxJ0mZgF/BnAElHkHQzmZnZbjwwfwNHHzicqeOGFjuUvdKbq5i+KOlB4CDg9+mdViFpfXy0kMGZmZW7huY2Zq9s5GNvmbbnwiWmN/diGkNyq+3FQK2k2nTV5vRlZmY9+MPzG4mAC8ps/AF618W0meThPZ3pfPYNRAIo/XvWmpkVyW/nr+fQsUM4+sDhxQ5lr/VmkPpbQBPwO5JnNxwWEVPTl5ODmVkPGprb+MvSzbz1+IOQSv/mfLn2mCAi4uPACcBPSZ7L8Iyk/0gfBGRmZj349bPryAS868SJxQ5ln/TqmdSR+F/gn4HvAh8EzilkYGZm5e6Xc9dy3MQRTJtQft1L0LtbbQyVdLmkX5E8/GcYcHJE3NybHUi6QNIiSUslXZdnfa2ku9P1T0qaki6vlnSrpOckLZT0mb16Z2ZmRbR0Uwvz1mzjnSeUZ+sBejdIvQlYAtyV/gygTlIdQET8vKcNJVUC3yZ50NAaYLakWTnPl74KaIqIIyRdCtxA8h/blwC1EXF8+ozq5yXdmT5tzsyspP1q7loqBBe9/uBih7LPepMgfkqSFI7i1Y8BDaDHBAHMAJZGxHIASXcBM4HsBDET+EI6fS9wo5LRnACGpv/FPZjkflDbexGvmVlRZTLBL55Zy5lHjOOAEYOKHc4+602C+NzuVkqanE5ujYjcE/hEYHXW/Brg1J7KRESnpG3AWJJkMRNYDwwBPpF1T6js/V8NXA0wefLk3NVmZv3u6VVNrGnaxT+dd2SxQ3lNepMgbmX3jxbt/rb/I5K7vPaVGUAXcDAwGvizpD92t0a6RcRNpHeVraur8yNQzazofvHMWgZXV3Le9PL757hsvbnVxl+9hvrXApOy5g9Jl+UrsybtThoJbAEuB34XER3AJkl/AeqA5ZiZlahd7V385tl1nH/sBIbWlvf9THt1metrMBuYJmmqpBrgUmBWTplZJP+AB3Ax8FB6v6dVwFsguZIKOA14ocDxmpm9JrOeXcv21k4uP/XQYofymhU0QUREJ3At8ACwELgnIhZIul7SRWmxHwBjJS0F/hHovhT228AwSQtIEs0PI2JeIeM1M3stIoLbHl/JUROGc8qU0cUO5zUrePsnIu4n+f+J7GWfy5puJbmkNXe7lnzLzcxK1dzVW1mwbjv/9s7jyvLWGrkK3cVkZrbf+PETKxlaU8lfl+mtNXI5QZiZ9YHGHe38Zt563nXSIQwr88Hpbk4QZmZ94Kf1q2nvzPC+08p/cLqbE4SZ2WvUlQluf3IlM6aO4agyfO5DT5wgzMxeo9/OX8/qxl188IwpxQ6lTzlBmJm9BhHB/zy8jMPGDeW8Y8v7P6dzOUGYmb0Gjy7dzIJ127n6rMOorCj/S1uzOUGYmb0G3/3TMg4YXstfnzQwLm3N5gRhZraP5q3Zyl+WbuGqN0yltqqy2OH0OScIM7N99N0/LWP4oCouP3VgPmrACcLMbB8s3dTMb+dv4P2nHcrwQdXFDqcgnCDMzPbBtx5cyuDqSq56w9Rih1IwThBmZntpycZmfj1vHVecPoWxw2qLHU7BOEGYme2lbz2UtB6uPuuwYodSUE4QZmZ7YcnGZn4zbx0fOGMKY4bWFDucgnKCMDPbC998cAlDqiv58BsHdusBCpwgJF0gaZGkpZKuy7O+VtLd6fonJU1Jl79X0tysV0bSCYWM1cxsTxas28Z9z63niv2g9QAFTBCSKkkeG3ohMB24TNL0nGJXAU0RcQTwX8ANABHxk4g4ISJOAN4PvBgRcwsVq5nZnkQE/+/XzzNqcDXXnHV4scPpF4VsQcwAlkbE8ohoB+4CZuaUmQncmk7fC5ytVz+n77J0WzOzovnt/A089WIj/3TeUYwcMjD/7yFXIRPERGB11vyadFneMhHRCWwDxuaUeQ9wZ087kXS1pHpJ9Q0NDa85aDOzXK0dXXzp/oUcfeBwLj1lUrHD6TclPUgt6VRgZ0TM76lMRNwUEXURUTd+/Ph+jM7M9hff//Ny1jTt4nNvn05VZUmfNvtUId/pWiA71R6SLstbRlIVMBLYkrX+UnbTejAzK7T123bxnYeXcf6xEzjjiHHFDqdfFTJBzAamSZoqqYbkZD8rp8ws4APp9MXAQxERAJIqgHfj8QczK5KI4DM/f44I+Ne35V5jM/BVFariiOiUdC3wAFAJ3BIRCyRdD9RHxCzgB8CPJS0FGkmSSLezgNURsbxQMZqZ7c7P5qzl4UUNfP4d05k0Zkixw+l3Sr+wDwh1dXVRX19f7DDMbADYuL2Vc7/+J446cDh3X306FQPsaXHZJD0dEXW5y/ef0RYzs16KCP7lF/Np68zwHxe/fkAnh91xgjAzy3FP/Wr+uHAjnzr/KKaOG1rscIrGCcLMLMvz67bzuV8t4A1HjOODZw7cZz30hhOEmVmqubWDv79jDiMHV/ONS0+gcj/tWupWsKuYzMzKSURw3c+eY1XjTu740KmMG8APAuottyDMzICbHlnOfc+t55PnHcWph+Xe8Wf/5ARhZvu9++at58u/fYG3HX8QfzfAnxK3N5wgzGy/Vr+ikU/cM5e6Q0fztXfvv5e05uMEYWb7rWUNLXz4tnomjhrMzVfUMai6stghlRQnCDPbLy1vaOHym5+gQuKHV57C6P3gCXF7ywnCzPY7yxtauOzmJ+jsCu748GlM2Y//GW53nCDMbL+yLCc5HHXg8GKHVLL8fxBmtt+oX9HIh26rp1JycugFtyDMbL9w37z1XP79Jxk9pIaf/58znBx6wS0IMxvQMpngOw8v5au/X0zdoaO5+Yo6D0j3UkFbEJIukLRI0lJJ1+VZXyvp7nT9k5KmZK17naTHJS2Q9JykQYWM1cwGnqYd7Vx162y++vvFzDzhYG7/0KlODnuhYC0ISZXAt4FzgTXAbEmzIuL5rGJXAU0RcYSkS4EbgPekz6e+HXh/RDwraSzQUahYzWzgeXplIx+7cy4NzW382zuP432nTkbyP8HtjUJ2Mc0AlnY/MlTSXcBMIDtBzAS+kE7fC9yo5AieB8yLiGcBImJLAeM0swGktaOLr/1+Ed9/9EUmjhrMvR85ndcdMqrYYZWlQiaIicDqrPk1wKk9lUmfYb0NGAscCYSkB4DxwF0R8R8FjNXMBoCnXmzkup/NY/nmHbz31Mlcd+HRDB9UXeywylapDlJXAW8ATgF2Ag+mz0x9MLegpKuBqwEmT57cr0GaWWlYt3UXX/7tC/z62XVMHDWYn3zoVM48Ylyxwyp7hUwQa4FJWfOHpMvylVmTjjuMBLaQtDYeiYjNAJLuB04CXpUgIuIm4CaAurq66OP3YGYlbNuuDn7w6Ivc9MgyIuBjZ0/jmjcdxpCaUv3uW14K+SnOBqZJmkqSCC4FLs8pMwv4APA4cDHwUER0dy39s6QhQDvwJuC/ChirmZWR7a0d/PDRFXz/0eU0t3bytuMP4roLj2bSmCHFDm1AKViCSMcUrgUeACqBWyJigaTrgfqImAX8APixpKVAI0kSISKaJH2dJMkEcH9E3FeoWM2sPKxu3Mmtj63g7tmraW7r5NzpE/j4OdM49uCRxQ5tQFLEwOmVqauri/r6+mKHYWZ9qCsTPLp0M3c+uYrfP7+BCom3Hn8QV591GMdNdGLoC+kYb13ucnfUmVlJighu/vNybn1sJWu37mLM0BquPutwPnDGoTd6cyUAAA5ZSURBVBw0cnCxw9svOEGYWUn6xh+X8M0Hl3DG4WP5v289hnOmH0BtlR/o05+cIMys5NwzezXffHAJl5x8CP9x8ev8H9BF4ru5mllJ+dPiBj7zi+d447RxfOldxzs5FJEThJmVjIXrt/P3P5nDkROG8533nkR1pU9RxeRP38xKwqbtrVz1o9kMq63ih1ee4ltklACPQZhZ0e1s7+SqW+vZuquDn15zOgeO9N39S4FbEGZWVF2Z4ON3zWXBum3892Un+p/eSogThJkVTVcm+NRPn+X3z2/ks2+fztnHTCh2SJbFCcLMiqI7Ofz8mbX807lH8sEzpxY7JMvhBGFm/a4rE3zq3iQ5/OO5R/LRs6cVOyTLw4PUZtavdrR18g93PcMfF27iE+ccycecHEqWE4SZ9ZuN21v52x/NZuH67Vw/81iuOH1KsUOy3XCCMLN+MW/NVv7ux0+zfVcHP/jAKfzV0QcUOyTbAycIMyuoiOCWv6zgK79dyAHDB3HPNaf7UtYy4QRhZgXTuKOdT/9sHn94fiPnHDOBr17yOkYNqSl2WNZLBb2KSdIFkhZJWirpujzrayXdna5/UtKUdPkUSbskzU1f3y1knGbWtyKCX81dyzlf/xMPL9rEZ98+nZuvONnJocwUrAUhqRL4NnAusAaYLWlWRDyfVewqoCkijpB0KXAD8J503bKIOKFQ8ZlZYazaspPPz5rP/y5q4IRJo7jhb17HUQcOL3ZYtg8K2cU0A1gaEcsBJN0FzASyE8RM4Avp9L3AjfK9fc3K0paWNv77oaX85MmVVFVU8Nm3T+fKM6ZQWeE/6XJVyAQxEVidNb8GOLWnMhHRKWkbMDZdN1XSM8B24F8j4s8FjNXM9tGm5lZ+9JcV3PrYClo7M7y7bhIfP2caE0b4hnvlrlQHqdcDkyNii6STgV9KOjYitucWlHQ1cDXA5MmT+zlMs/3Xko3N/ODRF/n5nLV0ZjJcePxB/OO5R3L4+GHFDs36SCETxFpgUtb8IemyfGXWSKoCRgJbIiKANoCIeFrSMuBIoD53JxFxE3ATQF1dXfT1mzCzl7V1dvG7+Rv4yROreGpFI7VVFbz7lEP40BsOY8q4ocUOz/pYIRPEbGCapKkkieBS4PKcMrOADwCPAxcDD0VESBoPNEZEl6TDgGnA8gLGamY9yGSCp1Y08qu5a7lv3nq2t3Zy6NghfObCo7mkbhJjhvrKpIGqYAkiHVO4FngAqARuiYgFkq4H6iNiFvAD4MeSlgKNJEkE4CzgekkdQAa4JiIaCxWrmb1SW2cXjy/bwu+f38gfn9/IpuY2htRUcv6xB/KukyZy5uHjqPDg84CnpDdnYKirq4v6+lf1Qu3Rko3NbNzeRm11BbVVFdRWVSY/q7Omqyqo8vNxbYDq7MrwwoZmHlu2mUeXbmH2i43s6uhiSE0lbzpyPBccdyDnTp/AkJpSHba010LS0xFRl7vcRxu49fEV3P7Eqj2Wq6wQtVUVDKqufNXPQdXpz6qs6epKaqsr0mXJ8u6Ek73t4JqXyw/uftUk633Vr/W1TCZY1biT59dvZ8G6bTyzaitzV29lZ3sXAEccMIz3nDKJNx05ntMPH8ug6soiR2zF4gQB/N1Zh3PR6yfS1tlFW0eG1s4u2jsztHdmaO3oor0rQ2tHOt2ZrG/tyNCWrm/tSLZr2tGelOvsYld7F7s6umhL69kXEq9IGENqKhlcU8WQ6kqG1r48PaS2kqE1VS//rKlkWG0VQ9PX8EFVDEt/Dq2pctfAfqC1o4uG5jbWNO1i7dZdrG7cyYubd7B8cwvLG3a8lAwqK8QxBw3nkpMP4aRDR3Pq1LF+HrS9xAkCmDRmCJPGDClY/ZlM0NaZoS1NLK1p4uhOLq3p9K72ZH5X96v95USzq72Lne1d7OzoYmdbJ+u2drCro4sdbZ3sau9iR3snmV70FkowrLaKEYOqGT4oSRovT1czYnDyMzepDBtUxfDaaobWVjK0tsqtm34UEexs72J7awdNOzrYurOdxp3tbGlpZ0tLG5t3tNPY0s6WHW1s2dFOQ3Mbza2dr6hDgoNHDubwA4ZRd+gYjjloONMPGsm0CcPcQrAeOUH0g4oKMbgmaQUUSkTQ2pFhZ3snO9u7aGnrZEdbJy1tnTS3dv/soLm1M+uVzG/Y3sqSTcn89tZOunqRaaoqlLRQaioZkrZUhtVmtVzSFs2Q6qRFM6imkkF5uudqs7va0lf3WFB/JqCOrswrPpOtOzvY3NJGQ3MbTTvbkaBSoqJCVEhUCCQhgUh+VggqJCorklf3tIAAIiATQVcm6OjK0NEVSau1M8PO9Dhtb+2kpa3jFcdod8ekQjB6SA1jhiavow8czlnTxjN+eC3jh9cycdRgJo4azEGjBlFb5URge8cJYoCQXk5CY/dcvEfd31ZbspLLjjzTLW2d7GzrZEd7FzvbO2lpS1ozm5t3Juvak3WvpXuttqqC6soKqipEZUUF1ZXJSbeqUlRXJutqKkVNVQU16cUF1ZWiqqKCqrRsJoJMQFcmQ3tn0JlJWms72pJWV0t6It7V0dVjLFVpl1xnb5po+6CmsoJB1RUvtdyGD6piwohBHHFAMj1ycHXayqtm9JBqRg2pYfTQasYNq2X0kBrfysIKxgnCXkHSS2MXE/qgvs6uDK2dScumtT3z0jfm7vGcl7ra0u63Xe3pdNq11pkJOruSE3tXJujMJN/AO7uC9q5M1lhRhm27OujsSr6dd2aCiJe/1VdUiJrKJMnUVlUyblgNh9YOeakbrfvk3N3dNmJw9UvfwofXVr3UmslkguDllgAkLYOgOxEFmUy8tL4r6ypBISoqoKqi4qULHmoqKzwmZCXLCcIKqqqygmGVFQyrHRi/at0n80qEu+5toPOF/WZmlpcThJmZ5eUEYWZmeTlBmJlZXk4QZmaWlxOEmZnl5QRhZmZ5OUGYmVleA+p5EJIagJXp7Ehg22uscl/q6O02eyq3u/U9rcu3PN+yccDmXsTY1/rimOxrPb3ZZn88JuC/lZ6Wlfsx2Zt6Do2I8a9aGhED8gXcVIw6ervNnsrtbn1P6/It72FZfbkek0Iel/3xmPTVcRmIfyvlfkz6op6B3MX06yLV0dtt9lRud+t7WpdveV98Dn2lr2Ip1HHZH48J+G+lt/vpT8X8W3nJgOpist6RVB95Hi9oxeNjUnp8TDxIvb+6qdgB2Kv4mJSe/f6YuAVhZmZ5uQVhZmZ5OUGYmVleThBmZpaXE4S9RNIxkr4r6V5JHyl2PJaQ9E5JN0u6W9J5xY7HQNJhkn4g6d5ix1JIThADhKRbJG2SND9n+QWSFklaKum63dUREQsj4hrg3cCZhYx3f9FHx+WXEfFh4BrgPYWMd3/QR8dkeURcVdhIi89XMQ0Qks4CWoDbIuK4dFklsBg4F1gDzAYuAyqBL+dU8bcRsUnSRcBHgB9HxB39Ff9A1VfHJd3ua8BPImJOP4U/IPXxMbk3Ii7ur9j728B4krwREY9ImpKzeAawNCKWA0i6C5gZEV8G3t5DPbOAWZLuA5wgXqO+OC6SBHwF+K2Tw2vXV38r+wN3MQ1sE4HVWfNr0mV5SXqzpG9J+h5wf6GD24/t1XEBPgqcA1ws6ZpCBrYf29u/lbGSvgucKOkzhQ6uWNyCsJdExMPAw0UOw3JExLeAbxU7DntZRGwhGRMa0NyCGNjWApOy5g9Jl1lx+biUHh+TPJwgBrbZwDRJUyXVAJcCs4ock/m4lCIfkzycIAYISXcCjwNHSVoj6aqI6ASuBR4AFgL3RMSCYsa5v/FxKT0+Jr3ny1zNzCwvtyDMzCwvJwgzM8vLCcLMzPJygjAzs7ycIMzMLC8nCDMzy8sJwszM8nKCMDOzvJwgrCxJasmZv1LSjcWKp5gkjZL0f/qoruMlrfQTBQ2cIMz2SInX9LfSF3XsxihgrxJET/FExHMk9yG6oo9iszLmBGEDiqTrJX08a/6Lkv5B0hRJL0j6iaSF6XO3h6Rl3ifpKUlzJX1PUmVafpGk24D5wKQ91PFLSU9LWiDp6nRZvjp6KveCpB9JWpzWf46kv0haImlG1vt5VawkDxM6PF32n3vznnr4GDcBx/btkbGyFBF++VV2L6ALmJv1WgXcCEwB5qRlKoBlwNh0eQBnputuAT4JHAP8GqhOl3+H5NvzFCADnJa1z7x1pNNj0p+DSU6+Y3uoo6dyncDxacxPp3ULmAn8Mt1md7HOz9pHr99TD5/tT4E24NBiH2e/ivvyA4OsXO2KiBO6ZyRdCdRFxApJWySdCEwAnomILZKGA6sj4i/pJrcDHwNagZOB2cmTPRlM8g36EWBlRDyRs998dXwV+Jikv06XTwKmARvy1NFTuRcj6d5B0gLgwYgISc+RnNgBzt5NrNl2Vy7fe3qJpAuBocB9JK2IlT2VtYHPCcIGou8DVwIHknwT75Z76+Ig+ZZ+a0S84rGR6TOLd+Sp+1V1SHozySNBT4+InZIeBgal61+qYw/l2rLqzGTNZ3j573R3sb5i0V6+p+71g4AbgIuADwLH4UfP7tc8BmED0S+AC4BTSO7v322ypNPT6cuBR4EHSZ71fACApDGSDt1N3fnqGAk0pSf9o4HTeti2t+V60lOszcDwXpTbk38FbouIFcBzJAnC9mNOEDbgREQ78L8kD33pylq1CPh7SQuB0cD/RMTzJCfG30uaB/wBOGg31b+qDuB3QFW67CtAT104vS3X0/vKG2skz0f+i6T5kv5zH94Tko4CzgW+kS5ygjA/MMgGnvTyzTnAJRGxJF02BfhNROzzSa8v6jArJ25B2IAiaTqwlGSQd0mx4zErZ25BmJlZXm5BmJlZXk4QZmaWlxOEmZnl5QRhZmZ5OUGYmVleThBmZpaXE4SZmeXlBGFmZnn9f83hgqjJ3QL+AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameter is 3.77e-03, giving a test R^2 score of 0.61\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 11;\n",
       "                var nbb_formatted_code = \"# 5-fold cross validation\\nlasso_cv = skllm.LassoCV(n_jobs=-1, cv=5).fit(X_train, y_train)\\nlasso_cv.score(X_test, y_test)\\nlambdas_lasso_cv = lasso_cv.alphas_\\nmses_lasso_cv = lasso_cv.mse_path_.mean(axis=1)\\nplt.semilogx(lambdas_lasso_cv, mses_lasso_cv)\\nplt.xlabel(r\\\"Hyperparameter $\\\\lambda$\\\")\\nplt.ylabel(r\\\"|MSE|\\\")\\nplt.title(\\\"5-fold cross validation lasso results\\\")\\nplt.show()\\nprint(\\n    f\\\"Best hyperparameter is {lasso_cv.alpha_:.2e}, giving a test R^2 score of {lasso_cv.score(X_test, y_test):.2f}\\\"\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 5-fold cross validation\n",
    "lasso_cv = skllm.LassoCV(n_jobs=-1, cv=5).fit(X_train, y_train)\n",
    "lasso_cv.score(X_test, y_test)\n",
    "lambdas_lasso_cv = lasso_cv.alphas_\n",
    "mses_lasso_cv = lasso_cv.mse_path_.mean(axis=1)\n",
    "plt.semilogx(lambdas_lasso_cv, mses_lasso_cv)\n",
    "plt.xlabel(r\"Hyperparameter $\\lambda$\")\n",
    "plt.ylabel(r\"|MSE|\")\n",
    "plt.title(\"5-fold cross validation lasso results\")\n",
    "plt.show()\n",
    "print(\n",
    "    f\"Best hyperparameter is {lasso_cv.alpha_:.2e}, giving a test R^2 score of {lasso_cv.score(X_test, y_test):.2f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems to be worse than forward and backward selection. Now I need to make a new class for bootstrap manually implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected cv as an integer, cross-validation object (from sklearn.model_selection) or an iterable. Got <__main__.Bootstrap object at 0x7f2058b2e4d0>.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-6227fe12ee79>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mbootstrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBootstrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mskllm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLassoCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbootstrap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/share/virtualenvs/STK-IN4300-oblig2-dUJPFG6m/lib/python3.7/site-packages/sklearn/linear_model/coordinate_descent.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;31m# init cross-validation generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m         \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_cv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m         \u001b[0;31m# Compute path for all folds and compute MSE to get the best alpha\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/STK-IN4300-oblig2-dUJPFG6m/lib/python3.7/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mcheck_cv\u001b[0;34m(cv, y, classifier)\u001b[0m\n\u001b[1;32m   1990\u001b[0m             raise ValueError(\"Expected cv as an integer, cross-validation \"\n\u001b[1;32m   1991\u001b[0m                              \u001b[0;34m\"object (from sklearn.model_selection) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1992\u001b[0;31m                              \"or an iterable. Got %s.\" % cv)\n\u001b[0m\u001b[1;32m   1993\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_CVIterableWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1994\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected cv as an integer, cross-validation object (from sklearn.model_selection) or an iterable. Got <__main__.Bootstrap object at 0x7f2058b2e4d0>."
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 27;\n",
       "                var nbb_formatted_code = \"class Bootstrap:\\n    def __init__(self, X, y):\\n        self.X = X\\n        self.y = y\\n\\n    def __getitem__(self, i):\\n        indices = np.arange(len(self.y))\\n        indices_train = np.random.choice(indices, replace=True, size=len(self.y))\\n        indices_validate = np.random.choice(indices, replace=True, size=len(self.y))\\n        return [indices_train, indices_validate]\\n\\n\\nbootstrap = Bootstrap(X_train, y_train)\\ntest = skllm.LassoCV(n_jobs=-1, cv=bootstrap).fit(X_train, y_train)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Bootstrap:\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        indices = np.arange(len(self.y))\n",
    "        indices_train = np.random.choice(indices, replace=True, size=len(self.y))\n",
    "        indices_validate = np.random.choice(indices, replace=True, size=len(self.y))\n",
    "        return [indices_train, indices_validate]\n",
    "\n",
    "\n",
    "bootstrap = Bootstrap(X_train, y_train)\n",
    "test = skllm.LassoCV(n_jobs=-1, cv=bootstrap).fit(X_train, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
