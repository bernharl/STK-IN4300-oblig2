{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 1;\n",
       "                var nbb_formatted_code = \"import pandas as pd  # For dataframe\\nimport numpy as np  # For matrix operations\\nimport sklearn.preprocessing as sklpre  # For preprocessing (scaling)\\nimport sklearn.linear_model as skllm  # For OLS\\nimport sklearn.model_selection as sklms  # For train_test_split\\nfrom scipy import stats  # To calc p-value\\nimport matplotlib.pyplot as plt  # For plotting\\n\\n# For automatic formatting of code, sparing you from my usually horrible looking code\\n%load_ext nb_black\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd  # For dataframe\n",
    "import numpy as np  # For matrix operations\n",
    "import sklearn.preprocessing as sklpre  # For preprocessing (scaling)\n",
    "import sklearn.linear_model as skllm  # For OLS\n",
    "import sklearn.model_selection as sklms  # For train_test_split\n",
    "from scipy import stats  # To calc p-value\n",
    "import matplotlib.pyplot as plt  # For plotting\n",
    "\n",
    "# For automatic formatting of code, sparing you from my usually horrible looking code\n",
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1.1\n",
    "I have chosen to one-hot encode the SEX-category, as neither male nor female should be considered adifferent numbers. \n",
    "The rest of the categorical values are just true/false, so those aren't encoded. \n",
    "Then I scale all the scalar features, not touching the categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_formatted_code = \"# Reading the data into dataframe\\ndf = pd.read_csv(\\\"data_task_1.txt\\\", header=0, sep=\\\" \\\")\\n# Onehot-encoding gender\\nonehot_gender = pd.get_dummies(df[\\\"SEX\\\"]).set_axis(\\n    [\\\"Male\\\", \\\"Female\\\"], axis=1, inplace=False\\n)\\n# Replacing old gender column\\ndf = df.join(onehot_gender)\\ndf.drop(\\\"SEX\\\", axis=1, inplace=True)\\n# List of boolean categories\\ncategorical = [\\n    \\\"ADHEU\\\",\\n    \\\"HOCHOZON\\\",\\n    \\\"AMATOP\\\",\\n    \\\"AVATOP\\\",\\n    \\\"ADEKZ\\\",\\n    \\\"ARAUCH\\\",\\n    \\\"FSNIGHT\\\",\\n    \\\"FSPT\\\",\\n    \\\"FSATEM\\\",\\n    \\\"FSAUGE\\\",\\n    \\\"FSPFEI\\\",\\n    \\\"FSHLAUF\\\",\\n    \\\"Male\\\",\\n    \\\"Female\\\",\\n]\\n\\n# A loop that splits the data and tries again until there is no split where only one modality is in one split\\nfirst = True\\nwhile (\\n    first\\n    or np.any(\\n        np.logical_or(X_train.sum(axis=0) == 0, X_train.sum(axis=0) == X_train.shape[0])\\n    )\\n    or np.any(\\n        np.logical_or(X_test.sum(axis=0) == 0, X_test.sum(axis=0) == X_test.shape[0])\\n    )\\n):\\n    first = False\\n    # Splitting over and over until splits are good, stratifying the most biased feature.\\n    X_train, X_test, y_train, y_test = sklms.train_test_split(\\n        df.loc[:, df.columns != \\\"FFVC\\\"],\\n        df[\\\"FFVC\\\"],\\n        test_size=0.5,\\n        stratify=df[\\\"FSATEM\\\"],\\n    )\\n# Scaling scalar features based on train set\\nscaler = sklpre.StandardScaler()\\nX_train_continous = scaler.fit_transform(\\n    X_train.loc[:, np.logical_not(np.isin(X_train.columns, categorical))].values\\n)\\nX_test_continous = scaler.transform(\\n    X_test.loc[:, np.logical_not(np.isin(X_test.columns, categorical))].values\\n)\\n# Putting all scalar and categorical features together\\nX_train.loc[\\n    :, np.logical_not(np.isin(X_train.columns, categorical))\\n] = X_train_continous\\nX_test.loc[:, np.logical_not(np.isin(X_test.columns, categorical))] = X_test_continous\\n# All preprocessing done!\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reading the data into dataframe\n",
    "df = pd.read_csv(\"data_task_1.txt\", header=0, sep=\" \")\n",
    "# Onehot-encoding gender\n",
    "onehot_gender = pd.get_dummies(df[\"SEX\"]).set_axis(\n",
    "    [\"Male\", \"Female\"], axis=1, inplace=False\n",
    ")\n",
    "# Replacing old gender column\n",
    "df = df.join(onehot_gender)\n",
    "df.drop(\"SEX\", axis=1, inplace=True)\n",
    "# List of boolean categories\n",
    "categorical = [\n",
    "    \"ADHEU\",\n",
    "    \"HOCHOZON\",\n",
    "    \"AMATOP\",\n",
    "    \"AVATOP\",\n",
    "    \"ADEKZ\",\n",
    "    \"ARAUCH\",\n",
    "    \"FSNIGHT\",\n",
    "    \"FSPT\",\n",
    "    \"FSATEM\",\n",
    "    \"FSAUGE\",\n",
    "    \"FSPFEI\",\n",
    "    \"FSHLAUF\",\n",
    "    \"Male\",\n",
    "    \"Female\",\n",
    "]\n",
    "\n",
    "# A loop that splits the data and tries again until there is no split where only one modality is in one split\n",
    "first = True\n",
    "while (\n",
    "    first\n",
    "    or np.any(\n",
    "        np.logical_or(X_train.sum(axis=0) == 0, X_train.sum(axis=0) == X_train.shape[0])\n",
    "    )\n",
    "    or np.any(\n",
    "        np.logical_or(X_test.sum(axis=0) == 0, X_test.sum(axis=0) == X_test.shape[0])\n",
    "    )\n",
    "):\n",
    "    first = False\n",
    "    # Splitting over and over until splits are good, stratifying the most biased feature.\n",
    "    X_train, X_test, y_train, y_test = sklms.train_test_split(\n",
    "        df.loc[:, df.columns != \"FFVC\"],\n",
    "        df[\"FFVC\"],\n",
    "        test_size=0.5,\n",
    "        stratify=df[\"FSATEM\"],\n",
    "    )\n",
    "# Scaling scalar features based on train set\n",
    "scaler = sklpre.StandardScaler()\n",
    "X_train_continous = scaler.fit_transform(\n",
    "    X_train.loc[:, np.logical_not(np.isin(X_train.columns, categorical))].values\n",
    ")\n",
    "X_test_continous = scaler.transform(\n",
    "    X_test.loc[:, np.logical_not(np.isin(X_test.columns, categorical))].values\n",
    ")\n",
    "# Putting all scalar and categorical features together\n",
    "X_train.loc[\n",
    "    :, np.logical_not(np.isin(X_train.columns, categorical))\n",
    "] = X_train_continous\n",
    "X_test.loc[:, np.logical_not(np.isin(X_test.columns, categorical))] = X_test_continous\n",
    "# All preprocessing done!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1.2\n",
    "Running OLS, calculating uncertainties and p-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_formatted_code = \"def get_summary_linear_model(model, X_train, y_train):\\n    \\\"\\\"\\\"\\n    Scikit-learn has no built in support for confidence intervals and p-values, so I \\n    made this to calculate it for me after fitting the model. Put into a function for reuse.\\n    \\\"\\\"\\\"\\n    # Combining intercept and coefficients in same array\\n    coefficients = np.append(model.intercept_, model.coef_)\\n\\n    # Predicting y\\n    y_hat = model.predict(X_train)\\n    # Calculating RSS to get variance for use when calculating stddev of coeffs\\n    residuals = y_train.values - y_hat\\n    rss = residuals.reshape(-1, 1).T @ residuals.reshape(-1, 1)\\n    var = rss[0, 0] / (len(X_train) - len(X_train.columns) - 1)\\n\\n    # Adding intercept to X_train, as sklearn usually does not need the column of 1's\\n    X_with_intercept = np.append(\\n        np.ones(X_train.shape[0]).reshape(-1, 1), X_train, axis=1\\n    )\\n    # Stddev of coefficients\\n    stddev = np.sqrt(\\n        (np.diag(var * np.linalg.pinv(X_with_intercept.T @ X_with_intercept)))\\n    )\\n    labels = [\\\"Intercept\\\"] + X_train.columns.tolist()\\n\\n    coef_over_std = coefficients / stddev\\n    p_values = [\\n        2 * (1 - stats.t.cdf(np.abs(i), (len(X_with_intercept) - 1)))\\n        for i in coef_over_std\\n    ]\\n\\n    # Putting results into table\\n    coeffs_table = pd.DataFrame(zip(labels, coefficients, stddev, p_values))\\n    # Giving nice names with TeX formatting\\n    coeffs_table.rename(\\n        columns={0: \\\"Feature\\\", 1: r\\\"$\\\\beta_i$\\\", 2: r\\\"$\\\\pm$\\\", 3: \\\"p-values\\\"},\\n        inplace=True,\\n    )\\n    return coeffs_table\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_summary_linear_model(model, X_train, y_train):\n",
    "    \"\"\"\n",
    "    Scikit-learn has no built in support for confidence intervals and p-values, so I \n",
    "    made this to calculate it for me after fitting the model. Put into a function for reuse.\n",
    "    \"\"\"\n",
    "    # Combining intercept and coefficients in same array\n",
    "    coefficients = np.append(model.intercept_, model.coef_)\n",
    "\n",
    "    # Predicting y\n",
    "    y_hat = model.predict(X_train)\n",
    "    # Calculating RSS to get variance for use when calculating stddev of coeffs\n",
    "    residuals = y_train.values - y_hat\n",
    "    rss = residuals.reshape(-1, 1).T @ residuals.reshape(-1, 1)\n",
    "    var = rss[0, 0] / (len(X_train) - len(X_train.columns) - 1)\n",
    "\n",
    "    # Adding intercept to X_train, as sklearn usually does not need the column of 1's\n",
    "    X_with_intercept = np.append(\n",
    "        np.ones(X_train.shape[0]).reshape(-1, 1), X_train, axis=1\n",
    "    )\n",
    "    # Stddev of coefficients\n",
    "    stddev = np.sqrt(\n",
    "        (np.diag(var * np.linalg.pinv(X_with_intercept.T @ X_with_intercept)))\n",
    "    )\n",
    "    labels = [\"Intercept\"] + X_train.columns.tolist()\n",
    "\n",
    "    coef_over_std = coefficients / stddev\n",
    "    p_values = [\n",
    "        2 * (1 - stats.t.cdf(np.abs(i), (len(X_with_intercept) - 1)))\n",
    "        for i in coef_over_std\n",
    "    ]\n",
    "\n",
    "    # Putting results into table\n",
    "    coeffs_table = pd.DataFrame(zip(labels, coefficients, stddev, p_values))\n",
    "    # Giving nice names with TeX formatting\n",
    "    coeffs_table.rename(\n",
    "        columns={0: \"Feature\", 1: r\"$\\beta_i$\", 2: r\"$\\pm$\", 3: \"p-values\"},\n",
    "        inplace=True,\n",
    "    )\n",
    "    return coeffs_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got an R^2 score of 0.60 for the test set.\n",
      "The most important feature (lowest p-value) is FLGROSS.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>$\\beta_i$</th>\n",
       "      <th>$\\pm$</th>\n",
       "      <th>p-values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Intercept</td>\n",
       "      <td>2.364216</td>\n",
       "      <td>0.027269</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ALTER</td>\n",
       "      <td>0.017773</td>\n",
       "      <td>0.017561</td>\n",
       "      <td>3.124780e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ADHEU</td>\n",
       "      <td>-0.063809</td>\n",
       "      <td>0.060722</td>\n",
       "      <td>2.943574e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HOCHOZON</td>\n",
       "      <td>-0.121314</td>\n",
       "      <td>0.040104</td>\n",
       "      <td>2.749271e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AMATOP</td>\n",
       "      <td>-0.011100</td>\n",
       "      <td>0.032427</td>\n",
       "      <td>7.324030e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AVATOP</td>\n",
       "      <td>-0.017774</td>\n",
       "      <td>0.033571</td>\n",
       "      <td>5.969704e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ADEKZ</td>\n",
       "      <td>-0.031533</td>\n",
       "      <td>0.035637</td>\n",
       "      <td>3.771026e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ARAUCH</td>\n",
       "      <td>-0.009976</td>\n",
       "      <td>0.032260</td>\n",
       "      <td>7.573923e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AGEBGEW</td>\n",
       "      <td>0.011899</td>\n",
       "      <td>0.014934</td>\n",
       "      <td>4.263524e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>FSNIGHT</td>\n",
       "      <td>0.030147</td>\n",
       "      <td>0.051487</td>\n",
       "      <td>5.587234e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>FLGROSS</td>\n",
       "      <td>0.177144</td>\n",
       "      <td>0.024128</td>\n",
       "      <td>3.045120e-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>FMILB</td>\n",
       "      <td>-0.015921</td>\n",
       "      <td>0.022741</td>\n",
       "      <td>4.845166e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>FNOH24</td>\n",
       "      <td>-0.062570</td>\n",
       "      <td>0.019469</td>\n",
       "      <td>1.484145e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>FTIER</td>\n",
       "      <td>-0.016777</td>\n",
       "      <td>0.017597</td>\n",
       "      <td>3.413134e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>FPOLL</td>\n",
       "      <td>-0.020439</td>\n",
       "      <td>0.029280</td>\n",
       "      <td>4.857975e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>FLTOTMED</td>\n",
       "      <td>-0.014647</td>\n",
       "      <td>0.014455</td>\n",
       "      <td>3.119325e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>FO3H24</td>\n",
       "      <td>0.058885</td>\n",
       "      <td>0.033844</td>\n",
       "      <td>8.312840e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>FSPT</td>\n",
       "      <td>0.004821</td>\n",
       "      <td>0.073463</td>\n",
       "      <td>9.477307e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>FTEH24</td>\n",
       "      <td>-0.045408</td>\n",
       "      <td>0.030823</td>\n",
       "      <td>1.419728e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>FSATEM</td>\n",
       "      <td>0.154991</td>\n",
       "      <td>0.077699</td>\n",
       "      <td>4.716680e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>FSAUGE</td>\n",
       "      <td>0.044864</td>\n",
       "      <td>0.046430</td>\n",
       "      <td>3.348539e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>FLGEW</td>\n",
       "      <td>0.082130</td>\n",
       "      <td>0.022560</td>\n",
       "      <td>3.314879e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>FSPFEI</td>\n",
       "      <td>0.132236</td>\n",
       "      <td>0.071567</td>\n",
       "      <td>6.583715e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>FSHLAUF</td>\n",
       "      <td>0.003888</td>\n",
       "      <td>0.063718</td>\n",
       "      <td>9.513876e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Male</td>\n",
       "      <td>0.096416</td>\n",
       "      <td>0.021466</td>\n",
       "      <td>1.086482e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Female</td>\n",
       "      <td>-0.096416</td>\n",
       "      <td>0.018985</td>\n",
       "      <td>7.500175e-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Feature  $\\beta_i$     $\\pm$      p-values\n",
       "0   Intercept   2.364216  0.027269  0.000000e+00\n",
       "1       ALTER   0.017773  0.017561  3.124780e-01\n",
       "2       ADHEU  -0.063809  0.060722  2.943574e-01\n",
       "3    HOCHOZON  -0.121314  0.040104  2.749271e-03\n",
       "4      AMATOP  -0.011100  0.032427  7.324030e-01\n",
       "5      AVATOP  -0.017774  0.033571  5.969704e-01\n",
       "6       ADEKZ  -0.031533  0.035637  3.771026e-01\n",
       "7      ARAUCH  -0.009976  0.032260  7.573923e-01\n",
       "8     AGEBGEW   0.011899  0.014934  4.263524e-01\n",
       "9     FSNIGHT   0.030147  0.051487  5.587234e-01\n",
       "10    FLGROSS   0.177144  0.024128  3.045120e-12\n",
       "11      FMILB  -0.015921  0.022741  4.845166e-01\n",
       "12     FNOH24  -0.062570  0.019469  1.484145e-03\n",
       "13      FTIER  -0.016777  0.017597  3.413134e-01\n",
       "14      FPOLL  -0.020439  0.029280  4.857975e-01\n",
       "15   FLTOTMED  -0.014647  0.014455  3.119325e-01\n",
       "16     FO3H24   0.058885  0.033844  8.312840e-02\n",
       "17       FSPT   0.004821  0.073463  9.477307e-01\n",
       "18     FTEH24  -0.045408  0.030823  1.419728e-01\n",
       "19     FSATEM   0.154991  0.077699  4.716680e-02\n",
       "20     FSAUGE   0.044864  0.046430  3.348539e-01\n",
       "21      FLGEW   0.082130  0.022560  3.314879e-04\n",
       "22     FSPFEI   0.132236  0.071567  6.583715e-02\n",
       "23    FSHLAUF   0.003888  0.063718  9.513876e-01\n",
       "24       Male   0.096416  0.021466  1.086482e-05\n",
       "25     Female  -0.096416  0.018985  7.500175e-07"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_formatted_code = \"# OLS on train data\\nols_reg = skllm.LinearRegression().fit(X_train, y_train)\\n# R2 test score\\nr2 = ols_reg.score(X_test, y_test)\\n# Using the function in the above cell to get table\\ncoeffs_table = get_summary_linear_model(ols_reg, X_train, y_train)\\n# Using Numpy indexing stuff to get feature with lowest p-value\\nmost_important = coeffs_table[\\\"Feature\\\"].values[1:][\\n    np.argmin(coeffs_table[\\\"p-values\\\"].values[1:])\\n]\\n# Printing results\\nprint(f\\\"Got an R^2 score of {r2:.2f} for the test set.\\\")\\nprint(f\\\"The most important feature (lowest p-value) is {most_important}.\\\")\\ncoeffs_table\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# OLS on train data\n",
    "ols_reg = skllm.LinearRegression().fit(X_train, y_train)\n",
    "# R2 test score\n",
    "r2 = ols_reg.score(X_test, y_test)\n",
    "# Using the function in the above cell to get table\n",
    "coeffs_table = get_summary_linear_model(ols_reg, X_train, y_train)\n",
    "# Using Numpy indexing stuff to get feature with lowest p-value\n",
    "most_important = coeffs_table[\"Feature\"].values[1:][\n",
    "    np.argmin(coeffs_table[\"p-values\"].values[1:])\n",
    "]\n",
    "# Printing results\n",
    "print(f\"Got an R^2 score of {r2:.2f} for the test set.\")\n",
    "print(f\"The most important feature (lowest p-value) is {most_important}.\")\n",
    "coeffs_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important (lower p-value) feature seems to be FLGROSS. Some other important features seem to be gender. Male and female seem to completely cancel each other, implying that men are of higher risk?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1.3\n",
    "Scikit-learn for some reason doesn't have built in forward and backward selection, so I will create my own functions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 5;\n",
       "                var nbb_formatted_code = \"def backward_elimination(regressor, X_train, y_train, max_p_limit):\\n    \\\"\\\"\\\"\\n    Takes a regressor, training set and a max p-value, runs backward\\n    elimination and returns the regresson fitted on the reduced\\n    features, the reduced feature matrix, a table of betas, \\n    standard deviations and p-values and the removed features\\n    \\\"\\\"\\\"\\n    # Fitting regressor on full model\\n    regressor.fit(X_train, y_train)\\n    # Getting table of p-values to find what to eliminate\\n    result_table = get_summary_linear_model(regressor, X_train, y_train)\\n    p_values = result_table[\\\"p-values\\\"].values\\n    p_val_max, p_val_max_pos = p_values[1:].max(), p_values[1:].argmax() + 1\\n    # Getting name of feature with highest p-val to make list of removed features\\n    feature_max_p_val = result_table[\\\"Feature\\\"][p_val_max_pos]\\n    removed_features = [feature_max_p_val]\\n    # Dropping feature with highest p-val\\n    X_reduce = X_train.drop(columns=feature_max_p_val, inplace=False)\\n    # Running backwards elimination until all p-values are below limit\\n    while p_val_max > max_p_limit:\\n        # Fitting on reduced model\\n        regressor.fit(X_reduce, y_train)\\n        result_table = get_summary_linear_model(regressor, X_reduce, y_train)\\n        p_values = result_table[\\\"p-values\\\"].values\\n        p_val_max, p_val_max_pos = p_values[1:].max(), p_values[1:].argmax() + 1\\n        feature_max_p_val = result_table[\\\"Feature\\\"][p_val_max_pos]\\n        # If one or more features have to high p-value, remove\\n        if p_val_max > max_p_limit:\\n            # Append name to list that keeps track of removed features\\n            removed_features.append(feature_max_p_val)\\n            # Dropping feature\\n            X_reduce.drop(columns=feature_max_p_val, inplace=True)\\n            # Sorting features\\n            X_reduce.sort_index(axis=1, inplace=True)\\n            # Fitting reduced model\\n            regressor.fit(X_reduce, y_train)\\n\\n    return regressor, X_reduce, result_table, removed_features\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def backward_elimination(regressor, X_train, y_train, max_p_limit):\n",
    "    \"\"\"\n",
    "    Takes a regressor, training set and a max p-value, runs backward\n",
    "    elimination and returns the regresson fitted on the reduced\n",
    "    features, the reduced feature matrix, a table of betas, \n",
    "    standard deviations and p-values and the removed features\n",
    "    \"\"\"\n",
    "    # Fitting regressor on full model\n",
    "    regressor.fit(X_train, y_train)\n",
    "    # Getting table of p-values to find what to eliminate\n",
    "    result_table = get_summary_linear_model(regressor, X_train, y_train)\n",
    "    p_values = result_table[\"p-values\"].values\n",
    "    p_val_max, p_val_max_pos = p_values[1:].max(), p_values[1:].argmax() + 1\n",
    "    # Getting name of feature with highest p-val to make list of removed features\n",
    "    feature_max_p_val = result_table[\"Feature\"][p_val_max_pos]\n",
    "    removed_features = [feature_max_p_val]\n",
    "    # Dropping feature with highest p-val\n",
    "    X_reduce = X_train.drop(columns=feature_max_p_val, inplace=False)\n",
    "    # Running backwards elimination until all p-values are below limit\n",
    "    while p_val_max > max_p_limit:\n",
    "        # Fitting on reduced model\n",
    "        regressor.fit(X_reduce, y_train)\n",
    "        result_table = get_summary_linear_model(regressor, X_reduce, y_train)\n",
    "        p_values = result_table[\"p-values\"].values\n",
    "        p_val_max, p_val_max_pos = p_values[1:].max(), p_values[1:].argmax() + 1\n",
    "        feature_max_p_val = result_table[\"Feature\"][p_val_max_pos]\n",
    "        # If one or more features have to high p-value, remove\n",
    "        if p_val_max > max_p_limit:\n",
    "            # Append name to list that keeps track of removed features\n",
    "            removed_features.append(feature_max_p_val)\n",
    "            # Dropping feature\n",
    "            X_reduce.drop(columns=feature_max_p_val, inplace=True)\n",
    "            # Sorting features\n",
    "            X_reduce.sort_index(axis=1, inplace=True)\n",
    "            # Fitting reduced model\n",
    "            regressor.fit(X_reduce, y_train)\n",
    "\n",
    "    return regressor, X_reduce, result_table, removed_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 score of full model: 0.60 Backward Model: 0.61\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>$\\beta_i$</th>\n",
       "      <th>$\\pm$</th>\n",
       "      <th>p-values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Intercept</td>\n",
       "      <td>2.337186</td>\n",
       "      <td>0.015915</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FLGEW</td>\n",
       "      <td>0.087173</td>\n",
       "      <td>0.021256</td>\n",
       "      <td>5.582698e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FLGROSS</td>\n",
       "      <td>0.190917</td>\n",
       "      <td>0.021817</td>\n",
       "      <td>2.220446e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FNOH24</td>\n",
       "      <td>-0.045487</td>\n",
       "      <td>0.016242</td>\n",
       "      <td>5.503514e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FSPFEI</td>\n",
       "      <td>0.160057</td>\n",
       "      <td>0.056114</td>\n",
       "      <td>4.707088e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Female</td>\n",
       "      <td>-0.084221</td>\n",
       "      <td>0.016172</td>\n",
       "      <td>4.026660e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>HOCHOZON</td>\n",
       "      <td>-0.085258</td>\n",
       "      <td>0.032370</td>\n",
       "      <td>8.975990e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Male</td>\n",
       "      <td>0.084221</td>\n",
       "      <td>0.016481</td>\n",
       "      <td>6.450400e-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Feature  $\\beta_i$     $\\pm$      p-values\n",
       "0  Intercept   2.337186  0.015915  0.000000e+00\n",
       "1      FLGEW   0.087173  0.021256  5.582698e-05\n",
       "2    FLGROSS   0.190917  0.021817  2.220446e-16\n",
       "3     FNOH24  -0.045487  0.016242  5.503514e-03\n",
       "4     FSPFEI   0.160057  0.056114  4.707088e-03\n",
       "5     Female  -0.084221  0.016172  4.026660e-07\n",
       "6   HOCHOZON  -0.085258  0.032370  8.975990e-03\n",
       "7       Male   0.084221  0.016481  6.450400e-07"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 6;\n",
       "                var nbb_formatted_code = \"# Model to send in to function. It is an OLS regressor.\\nbase_regressor = skllm.LinearRegression()\\n# Running backwards elimination\\n(\\n    regressor_reduced,\\n    X_reduce_train,\\n    result_table_reduced,\\n    removed_features,\\n) = backward_elimination(base_regressor, X_train, y_train, max_p_limit=1e-2)\\n# Using list of reduced features to also create test feature matrix with same features\\nX_reduce_test = X_test.drop(columns=removed_features).sort_index(axis=1)\\n# Printing results\\nprint(\\n    f\\\"R^2 score of full model: {ols_reg.score(X_test, y_test):.2f} Backward Model: {regressor_reduced.score(X_reduce_test, y_test):.2f}\\\"\\n)\\nresult_table_reduced\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Model to send in to function. It is an OLS regressor.\n",
    "base_regressor = skllm.LinearRegression()\n",
    "# Running backwards elimination\n",
    "(\n",
    "    regressor_reduced,\n",
    "    X_reduce_train,\n",
    "    result_table_reduced,\n",
    "    removed_features,\n",
    ") = backward_elimination(base_regressor, X_train, y_train, max_p_limit=1e-2)\n",
    "# Using list of reduced features to also create test feature matrix with same features\n",
    "X_reduce_test = X_test.drop(columns=removed_features).sort_index(axis=1)\n",
    "# Printing results\n",
    "print(\n",
    "    f\"R^2 score of full model: {ols_reg.score(X_test, y_test):.2f} Backward Model: {regressor_reduced.score(X_reduce_test, y_test):.2f}\"\n",
    ")\n",
    "result_table_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 7;\n",
       "                var nbb_formatted_code = \"def forward_selection(regressor, X_train, y_train, max_p_limit):\\n    \\\"\\\"\\\"\\n    Takes a regressor, training set and a max p-value, runs forward\\n    selection and returns the regresson fitted on the reduced\\n    features, the reduced feature matrix, a table of betas, \\n    standard deviations and p-values and the removed features\\n    \\\"\\\"\\\"\\n    X_null = pd.DataFrame({\\\"null\\\": np.zeros_like(y_train)})\\n    regressor.fit(X_null, y_train)\\n    # The p-value for the 0-column is invalid, but also not used, so I ignore the warnings\\n    with np.errstate(invalid=\\\"ignore\\\"):\\n        # Getting results for null-model\\n        result_table = get_summary_linear_model(regressor, X_null, y_train)\\n    # p-value for intercept\\n    p_val_max = result_table[\\\"p-values\\\"][0]\\n    # Dataframe used for incresing\\n    X_increased = pd.DataFrame()\\n    # List of features\\n    features = X_train.columns.values\\n    # while max p-val is below threshold, repeat\\n    while p_val_max < max_p_limit:\\n        # Set best p to infinity so that all values are less\\n        best_p = np.inf\\n        # Looping over features\\n        for feature in features:\\n            # Creating new column with feature in loop\\n            new_col = pd.DataFrame({feature: X_train[feature].values})\\n            # If null model we need to append to the dataframe differently than usual\\n            if len(X_increased.values) == 0:\\n                # Adding new feature to null model\\n                X_candidate = X_increased.append(new_col)\\n            else:\\n                # Adding new feature to model\\n                new_col_names = np.append(\\n                    X_increased.columns.values, new_col.columns.values\\n                )\\n                X_candidate = pd.DataFrame(\\n                    np.append(X_increased.values, new_col.values, axis=1),\\n                    columns=new_col_names,\\n                )\\n            # Fitting increased model to find p-value\\n            regressor.fit(X_candidate, y_train)\\n            result_table = get_summary_linear_model(regressor, X_candidate, y_train)\\n            p_i = result_table[\\\"p-values\\\"].values[-1]\\n            # This if-statement is used to find the minimum p-value of the potential features to add\\n            if p_i < best_p:\\n                best_p = p_i\\n                best_new_feature = feature\\n        # Now that we have the best feature to add, we add it properly\\n        new_col = pd.DataFrame({best_new_feature: X_train[best_new_feature].values})\\n        if len(X_increased.values) == 0:\\n            X_candidate = X_increased.append(new_col)\\n        else:\\n            new_col_names = np.append(\\n                X_increased.columns.values, new_col.columns.values\\n            )\\n            X_candidate = pd.DataFrame(\\n                np.append(X_increased.values, new_col.values, axis=1),\\n                columns=new_col_names,\\n            )\\n        # Get results for new model\\n        result_table = get_summary_linear_model(regressor, X_candidate, y_train)\\n        p_val_max = result_table[\\\"p-values\\\"].values.max()\\n\\n        # Sorting features\\n        X_increased = X_candidate.sort_index(axis=1)\\n        # Removing added feature from list of potential featues so that we can't add it again next iteration\\n        features = features[features != best_new_feature]\\n\\n    # List of omitted features\\n    omitted_features = features\\n    # Fitting increased model\\n    regressor.fit(X_increased, y_train)\\n    # Table of results for best model\\n    result_table_best = get_summary_linear_model(regressor, X_increased, y_train)\\n    return regressor, X_increased, result_table_best, omitted_features\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def forward_selection(regressor, X_train, y_train, max_p_limit):\n",
    "    \"\"\"\n",
    "    Takes a regressor, training set and a max p-value, runs forward\n",
    "    selection and returns the regresson fitted on the reduced\n",
    "    features, the reduced feature matrix, a table of betas, \n",
    "    standard deviations and p-values and the removed features\n",
    "    \"\"\"\n",
    "    X_null = pd.DataFrame({\"null\": np.zeros_like(y_train)})\n",
    "    regressor.fit(X_null, y_train)\n",
    "    # The p-value for the 0-column is invalid, but also not used, so I ignore the warnings\n",
    "    with np.errstate(invalid=\"ignore\"):\n",
    "        # Getting results for null-model\n",
    "        result_table = get_summary_linear_model(regressor, X_null, y_train)\n",
    "    # p-value for intercept\n",
    "    p_val_max = result_table[\"p-values\"][0]\n",
    "    # Dataframe used for incresing\n",
    "    X_increased = pd.DataFrame()\n",
    "    # List of features\n",
    "    features = X_train.columns.values\n",
    "    # while max p-val is below threshold, repeat\n",
    "    while p_val_max < max_p_limit:\n",
    "        # Set best p to infinity so that all values are less\n",
    "        best_p = np.inf\n",
    "        # Looping over features\n",
    "        for feature in features:\n",
    "            # Creating new column with feature in loop\n",
    "            new_col = pd.DataFrame({feature: X_train[feature].values})\n",
    "            # If null model we need to append to the dataframe differently than usual\n",
    "            if len(X_increased.values) == 0:\n",
    "                # Adding new feature to null model\n",
    "                X_candidate = X_increased.append(new_col)\n",
    "            else:\n",
    "                # Adding new feature to model\n",
    "                new_col_names = np.append(\n",
    "                    X_increased.columns.values, new_col.columns.values\n",
    "                )\n",
    "                X_candidate = pd.DataFrame(\n",
    "                    np.append(X_increased.values, new_col.values, axis=1),\n",
    "                    columns=new_col_names,\n",
    "                )\n",
    "            # Fitting increased model to find p-value\n",
    "            regressor.fit(X_candidate, y_train)\n",
    "            result_table = get_summary_linear_model(regressor, X_candidate, y_train)\n",
    "            p_i = result_table[\"p-values\"].values[-1]\n",
    "            # This if-statement is used to find the minimum p-value of the potential features to add\n",
    "            if p_i < best_p:\n",
    "                best_p = p_i\n",
    "                best_new_feature = feature\n",
    "        # Now that we have the best feature to add, we add it properly\n",
    "        new_col = pd.DataFrame({best_new_feature: X_train[best_new_feature].values})\n",
    "        if len(X_increased.values) == 0:\n",
    "            X_candidate = X_increased.append(new_col)\n",
    "        else:\n",
    "            new_col_names = np.append(\n",
    "                X_increased.columns.values, new_col.columns.values\n",
    "            )\n",
    "            X_candidate = pd.DataFrame(\n",
    "                np.append(X_increased.values, new_col.values, axis=1),\n",
    "                columns=new_col_names,\n",
    "            )\n",
    "        # Get results for new model\n",
    "        result_table = get_summary_linear_model(regressor, X_candidate, y_train)\n",
    "        p_val_max = result_table[\"p-values\"].values.max()\n",
    "\n",
    "        # Sorting features\n",
    "        X_increased = X_candidate.sort_index(axis=1)\n",
    "        # Removing added feature from list of potential featues so that we can't add it again next iteration\n",
    "        features = features[features != best_new_feature]\n",
    "\n",
    "    # List of omitted features\n",
    "    omitted_features = features\n",
    "    # Fitting increased model\n",
    "    regressor.fit(X_increased, y_train)\n",
    "    # Table of results for best model\n",
    "    result_table_best = get_summary_linear_model(regressor, X_increased, y_train)\n",
    "    return regressor, X_increased, result_table_best, omitted_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 score of forward model: 0.62\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>$\\beta_i$</th>\n",
       "      <th>$\\pm$</th>\n",
       "      <th>p-values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Intercept</td>\n",
       "      <td>2.288028</td>\n",
       "      <td>0.009545</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FLGEW</td>\n",
       "      <td>0.081910</td>\n",
       "      <td>0.021383</td>\n",
       "      <td>1.621706e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FLGROSS</td>\n",
       "      <td>0.197607</td>\n",
       "      <td>0.021644</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FSATEM</td>\n",
       "      <td>0.163249</td>\n",
       "      <td>0.063049</td>\n",
       "      <td>1.019037e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Female</td>\n",
       "      <td>-0.092164</td>\n",
       "      <td>0.014655</td>\n",
       "      <td>1.440385e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Male</td>\n",
       "      <td>0.092164</td>\n",
       "      <td>0.015432</td>\n",
       "      <td>8.098835e-09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Feature  $\\beta_i$     $\\pm$      p-values\n",
       "0  Intercept   2.288028  0.009545  0.000000e+00\n",
       "1      FLGEW   0.081910  0.021383  1.621706e-04\n",
       "2    FLGROSS   0.197607  0.021644  0.000000e+00\n",
       "3     FSATEM   0.163249  0.063049  1.019037e-02\n",
       "4     Female  -0.092164  0.014655  1.440385e-09\n",
       "5       Male   0.092164  0.015432  8.098835e-09"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 8;\n",
       "                var nbb_formatted_code = \"# Model to send in to function. It is an OLS regressor.\\nbase_regressor = skllm.LinearRegression()\\n# Running forward selection\\n(\\n    regressor_increased,\\n    X_increased_train,\\n    result_table_increased,\\n    omitted_features_increased,\\n) = forward_selection(base_regressor, X_train, y_train, max_p_limit=1e-2)\\n# Using list of reduced features to also create test feature matrix with same features\\nX_increased_test = X_test.drop(columns=omitted_features_increased).sort_index(axis=1)\\n# Printing results\\nprint(\\n    f\\\"R^2 score of forward model: {regressor_increased.score(X_increased_test, y_test):.2f}\\\"\\n)\\nresult_table_increased\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Model to send in to function. It is an OLS regressor.\n",
    "base_regressor = skllm.LinearRegression()\n",
    "# Running forward selection\n",
    "(\n",
    "    regressor_increased,\n",
    "    X_increased_train,\n",
    "    result_table_increased,\n",
    "    omitted_features_increased,\n",
    ") = forward_selection(base_regressor, X_train, y_train, max_p_limit=1e-2)\n",
    "# Using list of reduced features to also create test feature matrix with same features\n",
    "X_increased_test = X_test.drop(columns=omitted_features_increased).sort_index(axis=1)\n",
    "# Printing results\n",
    "print(\n",
    "    f\"R^2 score of forward model: {regressor_increased.score(X_increased_test, y_test):.2f}\"\n",
    ")\n",
    "result_table_increased"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the reduced models with $p_\\text{max}=0.01$, both forward and backward selection give the exact same model, and therefore the same features. They also get a slightly better $R^2$-score, possibly because they have less features, and are therefore less likely to overfit on the training data. I chose to look at $R^2$ instead of MSE as I feel it is a more intuitive value. However, higher $R^2$ also implies lower MSE, so the models are better.\n",
    "\n",
    "Next I will test with a less strict $p_\\text{max}=0.1$ and see how the models perform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 score of full model: 0.60 Backward Model: 0.59\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>$\\beta_i$</th>\n",
       "      <th>$\\pm$</th>\n",
       "      <th>p-values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Intercept</td>\n",
       "      <td>2.356130</td>\n",
       "      <td>0.017614</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FLGEW</td>\n",
       "      <td>0.085413</td>\n",
       "      <td>0.020977</td>\n",
       "      <td>6.284887e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FLGROSS</td>\n",
       "      <td>0.185501</td>\n",
       "      <td>0.021658</td>\n",
       "      <td>1.110223e-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FNOH24</td>\n",
       "      <td>-0.059083</td>\n",
       "      <td>0.018648</td>\n",
       "      <td>1.726465e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FO3H24</td>\n",
       "      <td>0.066141</td>\n",
       "      <td>0.032154</td>\n",
       "      <td>4.073414e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>FSATEM</td>\n",
       "      <td>0.115505</td>\n",
       "      <td>0.069290</td>\n",
       "      <td>9.678552e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>FSPFEI</td>\n",
       "      <td>0.139534</td>\n",
       "      <td>0.062715</td>\n",
       "      <td>2.699165e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>FTEH24</td>\n",
       "      <td>-0.052844</td>\n",
       "      <td>0.029422</td>\n",
       "      <td>7.370182e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>FTIER</td>\n",
       "      <td>-0.034312</td>\n",
       "      <td>0.013747</td>\n",
       "      <td>1.321092e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Female</td>\n",
       "      <td>-0.092246</td>\n",
       "      <td>0.016138</td>\n",
       "      <td>3.133708e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>HOCHOZON</td>\n",
       "      <td>-0.123329</td>\n",
       "      <td>0.037770</td>\n",
       "      <td>1.248606e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Male</td>\n",
       "      <td>0.092246</td>\n",
       "      <td>0.017260</td>\n",
       "      <td>2.060206e-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Feature  $\\beta_i$     $\\pm$      p-values\n",
       "0   Intercept   2.356130  0.017614  0.000000e+00\n",
       "1       FLGEW   0.085413  0.020977  6.284887e-05\n",
       "2     FLGROSS   0.185501  0.021658  1.110223e-15\n",
       "3      FNOH24  -0.059083  0.018648  1.726465e-03\n",
       "4      FO3H24   0.066141  0.032154  4.073414e-02\n",
       "5      FSATEM   0.115505  0.069290  9.678552e-02\n",
       "6      FSPFEI   0.139534  0.062715  2.699165e-02\n",
       "7      FTEH24  -0.052844  0.029422  7.370182e-02\n",
       "8       FTIER  -0.034312  0.013747  1.321092e-02\n",
       "9      Female  -0.092246  0.016138  3.133708e-08\n",
       "10   HOCHOZON  -0.123329  0.037770  1.248606e-03\n",
       "11       Male   0.092246  0.017260  2.060206e-07"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 9;\n",
       "                var nbb_formatted_code = \"# Model to send in to function. It is an OLS regressor.\\nbase_regressor = skllm.LinearRegression()\\n# Running backwards elimination\\n(\\n    regressor_reduced_less_strict,\\n    X_reduce_train_less_strict,\\n    result_table_reduced_less_strict,\\n    removed_features_less_strict,\\n) = backward_elimination(base_regressor, X_train, y_train, max_p_limit=1e-1)\\n# Using list of reduced features to also create test feature matrix with same features\\nX_reduce_test_less_strict = X_test.drop(\\n    columns=removed_features_less_strict\\n).sort_index(axis=1)\\n# Printing results\\nprint(\\n    f\\\"R^2 score of full model: {ols_reg.score(X_test, y_test):.2f}\\\"\\n    + f\\\" Backward Model: {regressor_reduced_less_strict.score(X_reduce_test_less_strict, y_test):.2f}\\\"\\n)\\nresult_table_reduced_less_strict\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Model to send in to function. It is an OLS regressor.\n",
    "base_regressor = skllm.LinearRegression()\n",
    "# Running backwards elimination\n",
    "(\n",
    "    regressor_reduced_less_strict,\n",
    "    X_reduce_train_less_strict,\n",
    "    result_table_reduced_less_strict,\n",
    "    removed_features_less_strict,\n",
    ") = backward_elimination(base_regressor, X_train, y_train, max_p_limit=1e-1)\n",
    "# Using list of reduced features to also create test feature matrix with same features\n",
    "X_reduce_test_less_strict = X_test.drop(\n",
    "    columns=removed_features_less_strict\n",
    ").sort_index(axis=1)\n",
    "# Printing results\n",
    "print(\n",
    "    f\"R^2 score of full model: {ols_reg.score(X_test, y_test):.2f}\"\n",
    "    + f\" Backward Model: {regressor_reduced_less_strict.score(X_reduce_test_less_strict, y_test):.2f}\"\n",
    ")\n",
    "result_table_reduced_less_strict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 score of forward model: 0.62\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>$\\beta_i$</th>\n",
       "      <th>$\\pm$</th>\n",
       "      <th>p-values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Intercept</td>\n",
       "      <td>2.288028</td>\n",
       "      <td>0.009545</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FLGEW</td>\n",
       "      <td>0.081910</td>\n",
       "      <td>0.021383</td>\n",
       "      <td>1.621706e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FLGROSS</td>\n",
       "      <td>0.197607</td>\n",
       "      <td>0.021644</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FSATEM</td>\n",
       "      <td>0.163249</td>\n",
       "      <td>0.063049</td>\n",
       "      <td>1.019037e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Female</td>\n",
       "      <td>-0.092164</td>\n",
       "      <td>0.014655</td>\n",
       "      <td>1.440385e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Male</td>\n",
       "      <td>0.092164</td>\n",
       "      <td>0.015432</td>\n",
       "      <td>8.098835e-09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Feature  $\\beta_i$     $\\pm$      p-values\n",
       "0  Intercept   2.288028  0.009545  0.000000e+00\n",
       "1      FLGEW   0.081910  0.021383  1.621706e-04\n",
       "2    FLGROSS   0.197607  0.021644  0.000000e+00\n",
       "3     FSATEM   0.163249  0.063049  1.019037e-02\n",
       "4     Female  -0.092164  0.014655  1.440385e-09\n",
       "5       Male   0.092164  0.015432  8.098835e-09"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 10;\n",
       "                var nbb_formatted_code = \"# Model to send in to function. It is an OLS regressor.\\nbase_regressor = skllm.LinearRegression()\\n# Running forward selection\\n(\\n    regressor_increased_less_strict,\\n    X_increased_train_less_strict,\\n    result_table_increased_less_strict,\\n    omitted_features_increased_less_strict,\\n) = forward_selection(base_regressor, X_train, y_train, max_p_limit=1e-1)\\n# Using list of reduced features to also create test feature matrix with same features\\nX_increased_test_less_strict = X_test.drop(\\n    columns=omitted_features_increased_less_strict\\n).sort_index(axis=1)\\n# Printing results\\nprint(\\n    f\\\"R^2 score of forward model: {regressor_increased.score(X_increased_test, y_test):.2f}\\\"\\n)\\nresult_table_increased_less_strict\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Model to send in to function. It is an OLS regressor.\n",
    "base_regressor = skllm.LinearRegression()\n",
    "# Running forward selection\n",
    "(\n",
    "    regressor_increased_less_strict,\n",
    "    X_increased_train_less_strict,\n",
    "    result_table_increased_less_strict,\n",
    "    omitted_features_increased_less_strict,\n",
    ") = forward_selection(base_regressor, X_train, y_train, max_p_limit=1e-1)\n",
    "# Using list of reduced features to also create test feature matrix with same features\n",
    "X_increased_test_less_strict = X_test.drop(\n",
    "    columns=omitted_features_increased_less_strict\n",
    ").sort_index(axis=1)\n",
    "# Printing results\n",
    "print(\n",
    "    f\"R^2 score of forward model: {regressor_increased.score(X_increased_test, y_test):.2f}\"\n",
    ")\n",
    "result_table_increased_less_strict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the models are not the same anymore. This is to be expected, as the p-values estimated are not the same for each feature independent of the other features. The backward elimination model seems to give a better $R^2$-score this time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1.4\n",
    "CV is easily implemented in Scikit-Learn. Bootstap on the other hand... I need to create my own class (Maybe there is a better way of doing this than what I'm doing...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameter is 3.98e-03, giving a test R^2 score of 0.62\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 44;\n",
       "                var nbb_formatted_code = \"# 5-fold cross validation, n-jobs=-1 is for parallelisation (use multiple cpu cores)\\nlasso_cv = skllm.LassoCV(n_jobs=-1, cv=5).fit(X_train, y_train)\\n# List of hyperparameters\\nlambdas_lasso_cv = lasso_cv.alphas_\\n# List of validation mean squared errors. Need to average them over axis 1 to get average across all 5 folds\\nmses_lasso_cv = lasso_cv.mse_path_.mean(axis=1)\\n\\nprint(\\n    f\\\"Best hyperparameter is {lasso_cv.alpha_:.2e}, giving a test R^2 score of {lasso_cv.score(X_test, y_test):.2f}\\\"\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 5-fold cross validation, n-jobs=-1 is for parallelisation (use multiple cpu cores)\n",
    "lasso_cv = skllm.LassoCV(n_jobs=-1, cv=5).fit(X_train, y_train)\n",
    "# List of hyperparameters\n",
    "lambdas_lasso_cv = lasso_cv.alphas_\n",
    "# List of validation mean squared errors. Need to average them over axis 1 to get average across all 5 folds\n",
    "mses_lasso_cv = lasso_cv.mse_path_.mean(axis=1)\n",
    "print(\n",
    "    f\"Best hyperparameter is {lasso_cv.alpha_:.2e}, giving a test R^2 score of {lasso_cv.score(X_test, y_test):.2f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems to be very similar to forward and backward selection. Now I need to make a new class for bootstrap manually implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameter is 3.71e-03, giving a test R^2 score of 0.62\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 45;\n",
       "                var nbb_formatted_code = \"class Bootstrap:\\n    def __init__(self, y):\\n        \\\"\\\"\\\"\\n        I was sort of confused on how to this, so I just made a class and tried stuff. \\n        Now it works, so I won't change it anymore.\\n        This class takes y_train and saves its length.\\n        \\\"\\\"\\\"\\n        self.len_y = len(y)\\n\\n    @property\\n    def get_bootstrap(self):\\n        \\\"\\\"\\\"\\n        This method returns one train-validation bootstrap split of the training data (indices of the data).\\n        The @property is just so that i can call get_bootstrap without the () after (like in get_bootstrap()).\\n        This splits train and validation into 2/3, 1/3 of the length of the data. Not sure if that is the correct\\n        method.\\n        \\\"\\\"\\\"\\n        # All indices (0-lenght of y)\\n        indices = np.arange(self.len_y)\\n        # Picking random indices with replacement.\\n        indices_train = np.random.choice(\\n            indices, replace=True, size=int(self.len_y * 0.67)\\n        )\\n        indices_validate = np.random.choice(\\n            indices, replace=True, size=int(self.len_y * 0.37)\\n        )\\n        # Returns a list of lists\\n        return [indices_train.tolist(), indices_validate.tolist()]\\n\\n\\n# Creating instance of Bootstrap class\\nbootstrap = Bootstrap(y_train)\\nsplits = []\\n# This loops creates 100 different bootstrap samples\\nfor i in range(100):\\n    splits.append(bootstrap.get_bootstrap)\\n# Running LassoCV with bootstrap instead of CV.\\nlasso_bootstrap = skllm.LassoCV(n_jobs=-1, cv=splits).fit(X_train, y_train)\\n\\n# Lambdas tried by the solver\\nlambdas_lasso_bootstrap = lasso_bootstrap.alphas_\\n# List of validation mean squared errors. Need to average them over axis 1 to get average across all 100 bootstraps\\nmses_lasso_bootstrap = lasso_bootstrap.mse_path_.mean(axis=1)\\n\\nprint(\\n    f\\\"Best hyperparameter is {lasso_bootstrap.alpha_:.2e}, giving a test R^2 score of {lasso_bootstrap.score(X_test, y_test):.2f}\\\"\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Bootstrap:\n",
    "    def __init__(self, y):\n",
    "        \"\"\"\n",
    "        I was sort of confused on how to this, so I just made a class and tried stuff. \n",
    "        Now it works, so I won't change it anymore.\n",
    "        This class takes y_train and saves its length.\n",
    "        \"\"\"\n",
    "        self.len_y = len(y)\n",
    "\n",
    "    @property\n",
    "    def get_bootstrap(self):\n",
    "        \"\"\"\n",
    "        This method returns one train-validation bootstrap split of the training data (indices of the data).\n",
    "        The @property is just so that i can call get_bootstrap without the () after (like in get_bootstrap()).\n",
    "        This splits train and validation into 2/3, 1/3 of the length of the data. Not sure if that is the correct\n",
    "        method.\n",
    "        \"\"\"\n",
    "        # All indices (0-lenght of y)\n",
    "        indices = np.arange(self.len_y)\n",
    "        # Picking random indices with replacement.\n",
    "        indices_train = np.random.choice(\n",
    "            indices, replace=True, size=int(self.len_y * 0.67)\n",
    "        )\n",
    "        indices_validate = np.random.choice(\n",
    "            indices, replace=True, size=int(self.len_y * 0.37)\n",
    "        )\n",
    "        # Returns a list of lists\n",
    "        return [indices_train.tolist(), indices_validate.tolist()]\n",
    "\n",
    "\n",
    "# Creating instance of Bootstrap class\n",
    "bootstrap = Bootstrap(y_train)\n",
    "splits = []\n",
    "# This loops creates 100 different bootstrap samples\n",
    "for i in range(100):\n",
    "    splits.append(bootstrap.get_bootstrap)\n",
    "# Running LassoCV with bootstrap instead of CV.\n",
    "lasso_bootstrap = skllm.LassoCV(n_jobs=-1, cv=splits).fit(X_train, y_train)\n",
    "\n",
    "# Lambdas tried by the solver\n",
    "lambdas_lasso_bootstrap = lasso_bootstrap.alphas_\n",
    "# List of validation mean squared errors. Need to average them over axis 1 to get average across all 100 bootstraps\n",
    "mses_lasso_bootstrap = lasso_bootstrap.mse_path_.mean(axis=1)\n",
    "\n",
    "print(\n",
    "    f\"Best hyperparameter is {lasso_bootstrap.alpha_:.2e}, giving a test R^2 score of {lasso_bootstrap.score(X_test, y_test):.2f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x482.4 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 56;\n",
       "                var nbb_formatted_code = \"fonts = {\\n    \\\"font.family\\\": \\\"serif\\\",\\n    \\\"axes.labelsize\\\": 8,\\n    \\\"font.size\\\": 8,\\n    \\\"legend.fontsize\\\": 8,\\n    \\\"xtick.labelsize\\\": 8,\\n    \\\"ytick.labelsize\\\": 8,\\n}\\n\\nplt.rcParams.update(fonts)\\n\\n# Log plot\\nplt.figure(figsize=(10, 6.7))\\nplt.semilogx(lambdas_lasso_bootstrap, mses_lasso_bootstrap, label=\\\"Bootsrap\\\")\\nplt.semilogx(lambdas_lasso_cv, mses_lasso_cv, label=\\\"5-fold cross validation\\\")\\nplt.xlabel(r\\\"Hyperparameter $\\\\lambda$\\\")\\nplt.ylabel(r\\\"|MSE|\\\")\\nplt.title(\\\"Bootstrap lasso results\\\")\\nplt.legend()\\nplt.grid()\\nplt.tight_layout()\\nplt.show()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fonts = {\n",
    "    \"font.family\": \"serif\",\n",
    "    \"axes.labelsize\": 8,\n",
    "    \"font.size\": 8,\n",
    "    \"legend.fontsize\": 8,\n",
    "    \"xtick.labelsize\": 8,\n",
    "    \"ytick.labelsize\": 8,\n",
    "}\n",
    "\n",
    "plt.rcParams.update(fonts)\n",
    "\n",
    "# Log plot\n",
    "plt.figure(figsize=(10, 6.7))\n",
    "plt.semilogx(lambdas_lasso_bootstrap, mses_lasso_bootstrap, label=\"Bootsrap\")\n",
    "plt.semilogx(lambdas_lasso_cv, mses_lasso_cv, label=\"5-fold cross validation\")\n",
    "plt.xlabel(r\"Hyperparameter $\\lambda$\")\n",
    "plt.ylabel(r\"|MSE|\")\n",
    "plt.title(\"Bootstrap lasso results\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
