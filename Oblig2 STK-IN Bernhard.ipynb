{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The nb_black extension is already loaded. To reload it, use:\n",
      "  %reload_ext nb_black\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 18;\n",
       "                var nbb_formatted_code = \"import pandas as pd  # For dataframe\\nimport numpy as np  # For matrix operations\\nimport sklearn.preprocessing as sklpre  # For preprocessing (scaling)\\nimport sklearn.linear_model as skllm  # For OLS\\nimport sklearn.model_selection as sklms  # For train_test_split\\nfrom scipy import stats  # To calc p-value\\n\\n# For automatic formatting of code, sparing you from my usually horrible looking code\\n%load_ext nb_black\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd  # For dataframe\n",
    "import numpy as np  # For matrix operations\n",
    "import sklearn.preprocessing as sklpre  # For preprocessing (scaling)\n",
    "import sklearn.linear_model as skllm  # For OLS\n",
    "import sklearn.model_selection as sklms  # For train_test_split\n",
    "from scipy import stats  # To calc p-value\n",
    "\n",
    "# For automatic formatting of code, sparing you from my usually horrible looking code\n",
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1.1\n",
    "I have chosen to one-hot encode the SEX-category, as neither male nor female should be considered adifferent numbers. \n",
    "The rest of the categorical values are just true/false, so those aren't encoded. \n",
    "Then I scale all the scalar features, not touching the categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 20;\n",
       "                var nbb_formatted_code = \"# Reading the data into dataframe\\ndf = pd.read_csv(\\\"data_task_1.txt\\\", header=0, sep=\\\" \\\")\\n# Onehot-encoding gender\\nonehot_gender = pd.get_dummies(df[\\\"SEX\\\"]).set_axis(\\n    [\\\"Male\\\", \\\"Female\\\"], axis=1, inplace=False\\n)\\n# Replacing old gender column\\ndf = df.join(onehot_gender)\\ndf.drop(\\\"SEX\\\", axis=1, inplace=True)\\n# List of boolean categories\\ncategorical = [\\n    \\\"ADHEU\\\",\\n    \\\"HOCHOZON\\\",\\n    \\\"AMATOP\\\",\\n    \\\"AVATOP\\\",\\n    \\\"ADEKZ\\\",\\n    \\\"ARAUCH\\\",\\n    \\\"FSNIGHT\\\",\\n    \\\"FSPT\\\",\\n    \\\"FSATEM\\\",\\n    \\\"FSAUGE\\\",\\n    \\\"FSPFEI\\\",\\n    \\\"FSHLAUF\\\",\\n    \\\"Male\\\",\\n    \\\"Female\\\",\\n]\\n\\n# A loop that splits the data and tries again until there is no split where only one modality is in one split\\nfirst = True\\nwhile (\\n    first\\n    or np.any(\\n        np.logical_or(X_train.sum(axis=0) == 0, X_train.sum(axis=0) == X_train.shape[0])\\n    )\\n    or np.any(\\n        np.logical_or(X_test.sum(axis=0) == 0, X_test.sum(axis=0) == X_test.shape[0])\\n    )\\n):\\n    first = False\\n    # Splitting over and over until splits are good, stratifying the most biased feature.\\n    X_train, X_test, y_train, y_test = sklms.train_test_split(\\n        df.loc[:, df.columns != \\\"FFVC\\\"],\\n        df[\\\"FFVC\\\"],\\n        test_size=0.5,\\n        stratify=df[\\\"FSATEM\\\"],\\n    )\\n# Scaling scalar features based on train set\\nscaler = sklpre.StandardScaler()\\nX_train_continous = scaler.fit_transform(\\n    X_train.loc[:, np.logical_not(np.isin(X_train.columns, categorical))].values\\n)\\nX_test_continous = scaler.transform(\\n    X_test.loc[:, np.logical_not(np.isin(X_test.columns, categorical))].values\\n)\\nX_train.loc[\\n    :, np.logical_not(np.isin(X_train.columns, categorical))\\n] = X_train_continous\\nX_test.loc[:, np.logical_not(np.isin(X_test.columns, categorical))] = X_test_continous\\n# All preprocessing done!\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reading the data into dataframe\n",
    "df = pd.read_csv(\"data_task_1.txt\", header=0, sep=\" \")\n",
    "# Onehot-encoding gender\n",
    "onehot_gender = pd.get_dummies(df[\"SEX\"]).set_axis(\n",
    "    [\"Male\", \"Female\"], axis=1, inplace=False\n",
    ")\n",
    "# Replacing old gender column\n",
    "df = df.join(onehot_gender)\n",
    "df.drop(\"SEX\", axis=1, inplace=True)\n",
    "# List of boolean categories\n",
    "categorical = [\n",
    "    \"ADHEU\",\n",
    "    \"HOCHOZON\",\n",
    "    \"AMATOP\",\n",
    "    \"AVATOP\",\n",
    "    \"ADEKZ\",\n",
    "    \"ARAUCH\",\n",
    "    \"FSNIGHT\",\n",
    "    \"FSPT\",\n",
    "    \"FSATEM\",\n",
    "    \"FSAUGE\",\n",
    "    \"FSPFEI\",\n",
    "    \"FSHLAUF\",\n",
    "    \"Male\",\n",
    "    \"Female\",\n",
    "]\n",
    "\n",
    "# A loop that splits the data and tries again until there is no split where only one modality is in one split\n",
    "first = True\n",
    "while (\n",
    "    first\n",
    "    or np.any(\n",
    "        np.logical_or(X_train.sum(axis=0) == 0, X_train.sum(axis=0) == X_train.shape[0])\n",
    "    )\n",
    "    or np.any(\n",
    "        np.logical_or(X_test.sum(axis=0) == 0, X_test.sum(axis=0) == X_test.shape[0])\n",
    "    )\n",
    "):\n",
    "    first = False\n",
    "    # Splitting over and over until splits are good, stratifying the most biased feature.\n",
    "    X_train, X_test, y_train, y_test = sklms.train_test_split(\n",
    "        df.loc[:, df.columns != \"FFVC\"],\n",
    "        df[\"FFVC\"],\n",
    "        test_size=0.5,\n",
    "        stratify=df[\"FSATEM\"],\n",
    "    )\n",
    "# Scaling scalar features based on train set\n",
    "scaler = sklpre.StandardScaler()\n",
    "X_train_continous = scaler.fit_transform(\n",
    "    X_train.loc[:, np.logical_not(np.isin(X_train.columns, categorical))].values\n",
    ")\n",
    "X_test_continous = scaler.transform(\n",
    "    X_test.loc[:, np.logical_not(np.isin(X_test.columns, categorical))].values\n",
    ")\n",
    "X_train.loc[\n",
    "    :, np.logical_not(np.isin(X_train.columns, categorical))\n",
    "] = X_train_continous\n",
    "X_test.loc[:, np.logical_not(np.isin(X_test.columns, categorical))] = X_test_continous\n",
    "# All preprocessing done!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1.2\n",
    "Running OLS, calculating uncertainties and p-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 14;\n",
       "                var nbb_formatted_code = \"def get_summary_linear_model(model, X_train, y_train):\\n    \\\"\\\"\\\"\\n    Scikit-learn has no built in support for confidence intervals and p-values, so I \\n    made this to calculate it for me after fitting the model. Put into a function for reuse.\\n    \\\"\\\"\\\"\\n    # Combining intercept and coefficients in same array\\n    coefficients = np.append(model.intercept_, model.coef_)\\n\\n    # Predicting y\\n    y_hat = model.predict(X_train)\\n    # Calculating RSS to get variance for use when calculating stddev of coeffs\\n    residuals = y_train.values - y_hat\\n    rss = residuals.reshape(-1, 1).T @ residuals.reshape(-1, 1)\\n    var = rss[0, 0] / (len(X_train) - len(X_train.columns) - 1)\\n\\n    # Adding intercept to X_train, as sklearn usually does not need the column of 1's\\n    X_with_intercept = np.append(\\n        np.ones(X_train.shape[0]).reshape(-1, 1), X_train, axis=1\\n    )\\n    # Stddev of coefficients\\n    stddev = np.sqrt(\\n        (np.diag(var * np.linalg.pinv(X_with_intercept.T @ X_with_intercept)))\\n    )\\n    labels = [\\\"Intercept\\\"] + X_train.columns.tolist()\\n\\n    coef_over_std = coefficients / stddev\\n    p_values = [\\n        2 * (1 - stats.t.cdf(np.abs(i), (len(X_with_intercept) - 1)))\\n        for i in coef_over_std\\n    ]\\n\\n    # Putting results into table\\n    coeffs_table = pd.DataFrame(zip(labels, coefficients, stddev, p_values))\\n    # Giving nice names with TeX formatting\\n    coeffs_table.rename(\\n        columns={0: \\\"Feature\\\", 1: r\\\"$\\\\beta_i$\\\", 2: r\\\"$\\\\pm$\\\", 3: \\\"p-values\\\"},\\n        inplace=True,\\n    )\\n    return coeffs_table\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_summary_linear_model(model, X_train, y_train):\n",
    "    \"\"\"\n",
    "    Scikit-learn has no built in support for confidence intervals and p-values, so I \n",
    "    made this to calculate it for me after fitting the model. Put into a function for reuse.\n",
    "    \"\"\"\n",
    "    # Combining intercept and coefficients in same array\n",
    "    coefficients = np.append(model.intercept_, model.coef_)\n",
    "\n",
    "    # Predicting y\n",
    "    y_hat = model.predict(X_train)\n",
    "    # Calculating RSS to get variance for use when calculating stddev of coeffs\n",
    "    residuals = y_train.values - y_hat\n",
    "    rss = residuals.reshape(-1, 1).T @ residuals.reshape(-1, 1)\n",
    "    var = rss[0, 0] / (len(X_train) - len(X_train.columns) - 1)\n",
    "\n",
    "    # Adding intercept to X_train, as sklearn usually does not need the column of 1's\n",
    "    X_with_intercept = np.append(\n",
    "        np.ones(X_train.shape[0]).reshape(-1, 1), X_train, axis=1\n",
    "    )\n",
    "    # Stddev of coefficients\n",
    "    stddev = np.sqrt(\n",
    "        (np.diag(var * np.linalg.pinv(X_with_intercept.T @ X_with_intercept)))\n",
    "    )\n",
    "    labels = [\"Intercept\"] + X_train.columns.tolist()\n",
    "\n",
    "    coef_over_std = coefficients / stddev\n",
    "    p_values = [\n",
    "        2 * (1 - stats.t.cdf(np.abs(i), (len(X_with_intercept) - 1)))\n",
    "        for i in coef_over_std\n",
    "    ]\n",
    "\n",
    "    # Putting results into table\n",
    "    coeffs_table = pd.DataFrame(zip(labels, coefficients, stddev, p_values))\n",
    "    # Giving nice names with TeX formatting\n",
    "    coeffs_table.rename(\n",
    "        columns={0: \"Feature\", 1: r\"$\\beta_i$\", 2: r\"$\\pm$\", 3: \"p-values\"},\n",
    "        inplace=True,\n",
    "    )\n",
    "    return coeffs_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got an R^2 score of 0.63 for the test set.\n",
      "The most important feature (lowest p-value) is FLGROSS.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>$\\beta_i$</th>\n",
       "      <th>$\\pm$</th>\n",
       "      <th>p-values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Intercept</td>\n",
       "      <td>2.274022</td>\n",
       "      <td>0.027674</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ALTER</td>\n",
       "      <td>0.023731</td>\n",
       "      <td>0.016134</td>\n",
       "      <td>1.426104e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ADHEU</td>\n",
       "      <td>0.028401</td>\n",
       "      <td>0.059290</td>\n",
       "      <td>6.323468e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HOCHOZON</td>\n",
       "      <td>-0.020947</td>\n",
       "      <td>0.036790</td>\n",
       "      <td>5.696185e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AMATOP</td>\n",
       "      <td>-0.003260</td>\n",
       "      <td>0.032447</td>\n",
       "      <td>9.200490e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AVATOP</td>\n",
       "      <td>-0.028397</td>\n",
       "      <td>0.036298</td>\n",
       "      <td>4.347793e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ADEKZ</td>\n",
       "      <td>0.023396</td>\n",
       "      <td>0.038578</td>\n",
       "      <td>5.447658e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ARAUCH</td>\n",
       "      <td>0.040267</td>\n",
       "      <td>0.031856</td>\n",
       "      <td>2.074001e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AGEBGEW</td>\n",
       "      <td>0.024400</td>\n",
       "      <td>0.015209</td>\n",
       "      <td>1.099244e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>FSNIGHT</td>\n",
       "      <td>0.040921</td>\n",
       "      <td>0.050535</td>\n",
       "      <td>4.188617e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>FLGROSS</td>\n",
       "      <td>0.123604</td>\n",
       "      <td>0.021999</td>\n",
       "      <td>5.175526e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>FMILB</td>\n",
       "      <td>-0.016083</td>\n",
       "      <td>0.019923</td>\n",
       "      <td>4.202897e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>FNOH24</td>\n",
       "      <td>-0.043288</td>\n",
       "      <td>0.017605</td>\n",
       "      <td>1.462708e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>FTIER</td>\n",
       "      <td>-0.002393</td>\n",
       "      <td>0.017955</td>\n",
       "      <td>8.940952e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>FPOLL</td>\n",
       "      <td>-0.018508</td>\n",
       "      <td>0.029284</td>\n",
       "      <td>5.279634e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>FLTOTMED</td>\n",
       "      <td>-0.016014</td>\n",
       "      <td>0.014410</td>\n",
       "      <td>2.675135e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>FO3H24</td>\n",
       "      <td>-0.007303</td>\n",
       "      <td>0.031924</td>\n",
       "      <td>8.192443e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>FSPT</td>\n",
       "      <td>0.026774</td>\n",
       "      <td>0.073608</td>\n",
       "      <td>7.163625e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>FTEH24</td>\n",
       "      <td>0.009516</td>\n",
       "      <td>0.029807</td>\n",
       "      <td>7.498096e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>FSATEM</td>\n",
       "      <td>0.158407</td>\n",
       "      <td>0.081106</td>\n",
       "      <td>5.193847e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>FSAUGE</td>\n",
       "      <td>-0.055056</td>\n",
       "      <td>0.048150</td>\n",
       "      <td>2.539717e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>FLGEW</td>\n",
       "      <td>0.112053</td>\n",
       "      <td>0.020979</td>\n",
       "      <td>2.095533e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>FSPFEI</td>\n",
       "      <td>0.091577</td>\n",
       "      <td>0.076442</td>\n",
       "      <td>2.320707e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>FSHLAUF</td>\n",
       "      <td>-0.073128</td>\n",
       "      <td>0.067716</td>\n",
       "      <td>2.812346e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Male</td>\n",
       "      <td>0.090234</td>\n",
       "      <td>0.020613</td>\n",
       "      <td>1.773090e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Female</td>\n",
       "      <td>-0.090234</td>\n",
       "      <td>0.019923</td>\n",
       "      <td>9.220368e-06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Feature  $\\beta_i$     $\\pm$      p-values\n",
       "0   Intercept   2.274022  0.027674  0.000000e+00\n",
       "1       ALTER   0.023731  0.016134  1.426104e-01\n",
       "2       ADHEU   0.028401  0.059290  6.323468e-01\n",
       "3    HOCHOZON  -0.020947  0.036790  5.696185e-01\n",
       "4      AMATOP  -0.003260  0.032447  9.200490e-01\n",
       "5      AVATOP  -0.028397  0.036298  4.347793e-01\n",
       "6       ADEKZ   0.023396  0.038578  5.447658e-01\n",
       "7      ARAUCH   0.040267  0.031856  2.074001e-01\n",
       "8     AGEBGEW   0.024400  0.015209  1.099244e-01\n",
       "9     FSNIGHT   0.040921  0.050535  4.188617e-01\n",
       "10    FLGROSS   0.123604  0.021999  5.175526e-08\n",
       "11      FMILB  -0.016083  0.019923  4.202897e-01\n",
       "12     FNOH24  -0.043288  0.017605  1.462708e-02\n",
       "13      FTIER  -0.002393  0.017955  8.940952e-01\n",
       "14      FPOLL  -0.018508  0.029284  5.279634e-01\n",
       "15   FLTOTMED  -0.016014  0.014410  2.675135e-01\n",
       "16     FO3H24  -0.007303  0.031924  8.192443e-01\n",
       "17       FSPT   0.026774  0.073608  7.163625e-01\n",
       "18     FTEH24   0.009516  0.029807  7.498096e-01\n",
       "19     FSATEM   0.158407  0.081106  5.193847e-02\n",
       "20     FSAUGE  -0.055056  0.048150  2.539717e-01\n",
       "21      FLGEW   0.112053  0.020979  2.095533e-07\n",
       "22     FSPFEI   0.091577  0.076442  2.320707e-01\n",
       "23    FSHLAUF  -0.073128  0.067716  2.812346e-01\n",
       "24       Male   0.090234  0.020613  1.773090e-05\n",
       "25     Female  -0.090234  0.019923  9.220368e-06"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 15;\n",
       "                var nbb_formatted_code = \"# OLS on train data\\nols_reg = skllm.LinearRegression().fit(X_train, y_train)\\n# R2 score\\nr2 = ols_reg.score(X_test, y_test)\\ncoeffs_table = get_summary_linear_model(ols_reg, X_train, y_train)\\nmost_important = coeffs_table[\\\"Feature\\\"].values[1:][\\n    np.argmin(coeffs_table[\\\"p-values\\\"].values[1:])\\n]\\n# Printing results\\nprint(f\\\"Got an R^2 score of {r2:.2f} for the test set.\\\")\\nprint(f\\\"The most important feature (lowest p-value) is {most_important}.\\\")\\ncoeffs_table\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# OLS on train data\n",
    "ols_reg = skllm.LinearRegression().fit(X_train, y_train)\n",
    "# R2 score\n",
    "r2 = ols_reg.score(X_test, y_test)\n",
    "coeffs_table = get_summary_linear_model(ols_reg, X_train, y_train)\n",
    "most_important = coeffs_table[\"Feature\"].values[1:][\n",
    "    np.argmin(coeffs_table[\"p-values\"].values[1:])\n",
    "]\n",
    "# Printing results\n",
    "print(f\"Got an R^2 score of {r2:.2f} for the test set.\")\n",
    "print(f\"The most important feature (lowest p-value) is {most_important}.\")\n",
    "coeffs_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important (lower p-value) feature seems to be FLGROSS. Some other important features seem to be gender. Male and female seem to completely cancel each other... Overfit maybe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2.3\n",
    "Scikit-learn for some reason doesn't have built in forward and backward selection, so I will create my own functions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 22;\n",
       "                var nbb_formatted_code = \"def backward_elimination(regressor, X_train, y_train, max_p=1e-1):\\n    pass\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def backward_elimination(regressor, X_train, y_train, max_p=1e-1):\n",
    "    regressor.fit(X_train)\n",
    "    result_table = "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
