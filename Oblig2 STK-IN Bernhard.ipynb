{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 1;\n",
       "                var nbb_formatted_code = \"import pandas as pd  # For dataframe\\nimport numpy as np  # For matrix operations\\nimport sklearn.preprocessing as sklpre  # For preprocessing (scaling)\\nimport sklearn.linear_model as skllm  # For OLS\\nimport sklearn.model_selection as sklms  # For train_test_split\\nfrom scipy import stats  # To calc p-value\\n\\n# For automatic formatting of code, sparing you from my usually horrible looking code\\n%load_ext nb_black\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd  # For dataframe\n",
    "import numpy as np  # For matrix operations\n",
    "import sklearn.preprocessing as sklpre  # For preprocessing (scaling)\n",
    "import sklearn.linear_model as skllm  # For OLS\n",
    "import sklearn.model_selection as sklms  # For train_test_split\n",
    "from scipy import stats  # To calc p-value\n",
    "\n",
    "# For automatic formatting of code, sparing you from my usually horrible looking code\n",
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1.1\n",
    "I have chosen to one-hot encode the SEX-category, as neither male nor female should be considered adifferent numbers. \n",
    "The rest of the categorical values are just true/false, so those aren't encoded. \n",
    "Then I scale all the scalar features, not touching the categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_formatted_code = \"# Reading the data into dataframe\\ndf = pd.read_csv(\\\"data_task_1.txt\\\", header=0, sep=\\\" \\\")\\n# Onehot-encoding gender\\nonehot_gender = pd.get_dummies(df[\\\"SEX\\\"]).set_axis(\\n    [\\\"Male\\\", \\\"Female\\\"], axis=1, inplace=False\\n)\\n# Replacing old gender column\\ndf = df.join(onehot_gender)\\ndf.drop(\\\"SEX\\\", axis=1, inplace=True)\\n# List of boolean categories\\ncategorical = [\\n    \\\"ADHEU\\\",\\n    \\\"HOCHOZON\\\",\\n    \\\"AMATOP\\\",\\n    \\\"AVATOP\\\",\\n    \\\"ADEKZ\\\",\\n    \\\"ARAUCH\\\",\\n    \\\"FSNIGHT\\\",\\n    \\\"FSPT\\\",\\n    \\\"FSATEM\\\",\\n    \\\"FSAUGE\\\",\\n    \\\"FSPFEI\\\",\\n    \\\"FSHLAUF\\\",\\n    \\\"Male\\\",\\n    \\\"Female\\\",\\n]\\n\\n# A loop that splits the data and tries again until there is no split where only one modality is in one split\\nfirst = True\\nwhile (\\n    first\\n    or np.any(\\n        np.logical_or(X_train.sum(axis=0) == 0, X_train.sum(axis=0) == X_train.shape[0])\\n    )\\n    or np.any(\\n        np.logical_or(X_test.sum(axis=0) == 0, X_test.sum(axis=0) == X_test.shape[0])\\n    )\\n):\\n    first = False\\n    # Splitting over and over until splits are good, stratifying the most biased feature.\\n    X_train, X_test, y_train, y_test = sklms.train_test_split(\\n        df.loc[:, df.columns != \\\"FFVC\\\"],\\n        df[\\\"FFVC\\\"],\\n        test_size=0.5,\\n        stratify=df[\\\"FSATEM\\\"],\\n    )\\n# Scaling scalar features based on train set\\nscaler = sklpre.StandardScaler()\\nX_train_continous = scaler.fit_transform(\\n    X_train.loc[:, np.logical_not(np.isin(X_train.columns, categorical))].values\\n)\\nX_test_continous = scaler.transform(\\n    X_test.loc[:, np.logical_not(np.isin(X_test.columns, categorical))].values\\n)\\nX_train.loc[\\n    :, np.logical_not(np.isin(X_train.columns, categorical))\\n] = X_train_continous\\nX_test.loc[:, np.logical_not(np.isin(X_test.columns, categorical))] = X_test_continous\\n# All preprocessing done!\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reading the data into dataframe\n",
    "df = pd.read_csv(\"data_task_1.txt\", header=0, sep=\" \")\n",
    "# Onehot-encoding gender\n",
    "onehot_gender = pd.get_dummies(df[\"SEX\"]).set_axis(\n",
    "    [\"Male\", \"Female\"], axis=1, inplace=False\n",
    ")\n",
    "# Replacing old gender column\n",
    "df = df.join(onehot_gender)\n",
    "df.drop(\"SEX\", axis=1, inplace=True)\n",
    "# List of boolean categories\n",
    "categorical = [\n",
    "    \"ADHEU\",\n",
    "    \"HOCHOZON\",\n",
    "    \"AMATOP\",\n",
    "    \"AVATOP\",\n",
    "    \"ADEKZ\",\n",
    "    \"ARAUCH\",\n",
    "    \"FSNIGHT\",\n",
    "    \"FSPT\",\n",
    "    \"FSATEM\",\n",
    "    \"FSAUGE\",\n",
    "    \"FSPFEI\",\n",
    "    \"FSHLAUF\",\n",
    "    \"Male\",\n",
    "    \"Female\",\n",
    "]\n",
    "\n",
    "# A loop that splits the data and tries again until there is no split where only one modality is in one split\n",
    "first = True\n",
    "while (\n",
    "    first\n",
    "    or np.any(\n",
    "        np.logical_or(X_train.sum(axis=0) == 0, X_train.sum(axis=0) == X_train.shape[0])\n",
    "    )\n",
    "    or np.any(\n",
    "        np.logical_or(X_test.sum(axis=0) == 0, X_test.sum(axis=0) == X_test.shape[0])\n",
    "    )\n",
    "):\n",
    "    first = False\n",
    "    # Splitting over and over until splits are good, stratifying the most biased feature.\n",
    "    X_train, X_test, y_train, y_test = sklms.train_test_split(\n",
    "        df.loc[:, df.columns != \"FFVC\"],\n",
    "        df[\"FFVC\"],\n",
    "        test_size=0.5,\n",
    "        stratify=df[\"FSATEM\"],\n",
    "    )\n",
    "# Scaling scalar features based on train set\n",
    "scaler = sklpre.StandardScaler()\n",
    "X_train_continous = scaler.fit_transform(\n",
    "    X_train.loc[:, np.logical_not(np.isin(X_train.columns, categorical))].values\n",
    ")\n",
    "X_test_continous = scaler.transform(\n",
    "    X_test.loc[:, np.logical_not(np.isin(X_test.columns, categorical))].values\n",
    ")\n",
    "X_train.loc[\n",
    "    :, np.logical_not(np.isin(X_train.columns, categorical))\n",
    "] = X_train_continous\n",
    "X_test.loc[:, np.logical_not(np.isin(X_test.columns, categorical))] = X_test_continous\n",
    "# All preprocessing done!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1.2\n",
    "Running OLS, calculating uncertainties and p-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_formatted_code = \"def get_summary_linear_model(model, X_train, y_train):\\n    \\\"\\\"\\\"\\n    Scikit-learn has no built in support for confidence intervals and p-values, so I \\n    made this to calculate it for me after fitting the model. Put into a function for reuse.\\n    \\\"\\\"\\\"\\n    # Combining intercept and coefficients in same array\\n    coefficients = np.append(model.intercept_, model.coef_)\\n\\n    # Predicting y\\n    y_hat = model.predict(X_train)\\n    # Calculating RSS to get variance for use when calculating stddev of coeffs\\n    residuals = y_train.values - y_hat\\n    rss = residuals.reshape(-1, 1).T @ residuals.reshape(-1, 1)\\n    var = rss[0, 0] / (len(X_train) - len(X_train.columns) - 1)\\n\\n    # Adding intercept to X_train, as sklearn usually does not need the column of 1's\\n    X_with_intercept = np.append(\\n        np.ones(X_train.shape[0]).reshape(-1, 1), X_train, axis=1\\n    )\\n    # Stddev of coefficients\\n    stddev = np.sqrt(\\n        (np.diag(var * np.linalg.pinv(X_with_intercept.T @ X_with_intercept)))\\n    )\\n    labels = [\\\"Intercept\\\"] + X_train.columns.tolist()\\n\\n    coef_over_std = coefficients / stddev\\n    p_values = [\\n        2 * (1 - stats.t.cdf(np.abs(i), (len(X_with_intercept) - 1)))\\n        for i in coef_over_std\\n    ]\\n\\n    # Putting results into table\\n    coeffs_table = pd.DataFrame(zip(labels, coefficients, stddev, p_values))\\n    # Giving nice names with TeX formatting\\n    coeffs_table.rename(\\n        columns={0: \\\"Feature\\\", 1: r\\\"$\\\\beta_i$\\\", 2: r\\\"$\\\\pm$\\\", 3: \\\"p-values\\\"},\\n        inplace=True,\\n    )\\n    return coeffs_table\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_summary_linear_model(model, X_train, y_train):\n",
    "    \"\"\"\n",
    "    Scikit-learn has no built in support for confidence intervals and p-values, so I \n",
    "    made this to calculate it for me after fitting the model. Put into a function for reuse.\n",
    "    \"\"\"\n",
    "    # Combining intercept and coefficients in same array\n",
    "    coefficients = np.append(model.intercept_, model.coef_)\n",
    "\n",
    "    # Predicting y\n",
    "    y_hat = model.predict(X_train)\n",
    "    # Calculating RSS to get variance for use when calculating stddev of coeffs\n",
    "    residuals = y_train.values - y_hat\n",
    "    rss = residuals.reshape(-1, 1).T @ residuals.reshape(-1, 1)\n",
    "    var = rss[0, 0] / (len(X_train) - len(X_train.columns) - 1)\n",
    "\n",
    "    # Adding intercept to X_train, as sklearn usually does not need the column of 1's\n",
    "    X_with_intercept = np.append(\n",
    "        np.ones(X_train.shape[0]).reshape(-1, 1), X_train, axis=1\n",
    "    )\n",
    "    # Stddev of coefficients\n",
    "    stddev = np.sqrt(\n",
    "        (np.diag(var * np.linalg.pinv(X_with_intercept.T @ X_with_intercept)))\n",
    "    )\n",
    "    labels = [\"Intercept\"] + X_train.columns.tolist()\n",
    "\n",
    "    coef_over_std = coefficients / stddev\n",
    "    p_values = [\n",
    "        2 * (1 - stats.t.cdf(np.abs(i), (len(X_with_intercept) - 1)))\n",
    "        for i in coef_over_std\n",
    "    ]\n",
    "\n",
    "    # Putting results into table\n",
    "    coeffs_table = pd.DataFrame(zip(labels, coefficients, stddev, p_values))\n",
    "    # Giving nice names with TeX formatting\n",
    "    coeffs_table.rename(\n",
    "        columns={0: \"Feature\", 1: r\"$\\beta_i$\", 2: r\"$\\pm$\", 3: \"p-values\"},\n",
    "        inplace=True,\n",
    "    )\n",
    "    return coeffs_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got an R^2 score of 0.65 for the test set.\n",
      "The most important feature (lowest p-value) is FLGROSS.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>$\\beta_i$</th>\n",
       "      <th>$\\pm$</th>\n",
       "      <th>p-values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Intercept</td>\n",
       "      <td>2.317285</td>\n",
       "      <td>0.029736</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ALTER</td>\n",
       "      <td>0.008289</td>\n",
       "      <td>0.017125</td>\n",
       "      <td>6.288000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ADHEU</td>\n",
       "      <td>-0.004462</td>\n",
       "      <td>0.069220</td>\n",
       "      <td>9.486558e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HOCHOZON</td>\n",
       "      <td>-0.099654</td>\n",
       "      <td>0.042450</td>\n",
       "      <td>1.968645e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AMATOP</td>\n",
       "      <td>0.012938</td>\n",
       "      <td>0.034508</td>\n",
       "      <td>7.080294e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AVATOP</td>\n",
       "      <td>-0.045576</td>\n",
       "      <td>0.036278</td>\n",
       "      <td>2.101950e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ADEKZ</td>\n",
       "      <td>-0.013319</td>\n",
       "      <td>0.036899</td>\n",
       "      <td>7.184402e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ARAUCH</td>\n",
       "      <td>-0.016967</td>\n",
       "      <td>0.033215</td>\n",
       "      <td>6.099289e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AGEBGEW</td>\n",
       "      <td>0.018089</td>\n",
       "      <td>0.015769</td>\n",
       "      <td>2.524305e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>FSNIGHT</td>\n",
       "      <td>0.029517</td>\n",
       "      <td>0.053945</td>\n",
       "      <td>5.847592e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>FLGROSS</td>\n",
       "      <td>0.160807</td>\n",
       "      <td>0.022558</td>\n",
       "      <td>1.111844e-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>FMILB</td>\n",
       "      <td>-0.045283</td>\n",
       "      <td>0.018954</td>\n",
       "      <td>1.763886e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>FNOH24</td>\n",
       "      <td>-0.045715</td>\n",
       "      <td>0.019822</td>\n",
       "      <td>2.192640e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>FTIER</td>\n",
       "      <td>-0.020370</td>\n",
       "      <td>0.018788</td>\n",
       "      <td>2.793235e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>FPOLL</td>\n",
       "      <td>-0.011656</td>\n",
       "      <td>0.028447</td>\n",
       "      <td>6.823552e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>FLTOTMED</td>\n",
       "      <td>-0.016545</td>\n",
       "      <td>0.015892</td>\n",
       "      <td>2.988600e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>FO3H24</td>\n",
       "      <td>0.057992</td>\n",
       "      <td>0.032699</td>\n",
       "      <td>7.738207e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>FSPT</td>\n",
       "      <td>0.058432</td>\n",
       "      <td>0.072330</td>\n",
       "      <td>4.199576e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>FTEH24</td>\n",
       "      <td>-0.035450</td>\n",
       "      <td>0.029702</td>\n",
       "      <td>2.338075e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>FSATEM</td>\n",
       "      <td>0.178058</td>\n",
       "      <td>0.085160</td>\n",
       "      <td>3.756239e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>FSAUGE</td>\n",
       "      <td>-0.013788</td>\n",
       "      <td>0.052991</td>\n",
       "      <td>7.949313e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>FLGEW</td>\n",
       "      <td>0.090220</td>\n",
       "      <td>0.021825</td>\n",
       "      <td>4.890725e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>FSPFEI</td>\n",
       "      <td>0.048001</td>\n",
       "      <td>0.090367</td>\n",
       "      <td>5.957694e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>FSHLAUF</td>\n",
       "      <td>-0.025496</td>\n",
       "      <td>0.071632</td>\n",
       "      <td>7.221996e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Male</td>\n",
       "      <td>0.103017</td>\n",
       "      <td>0.020996</td>\n",
       "      <td>1.684908e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Female</td>\n",
       "      <td>-0.103017</td>\n",
       "      <td>0.021249</td>\n",
       "      <td>2.206050e-06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Feature  $\\beta_i$     $\\pm$      p-values\n",
       "0   Intercept   2.317285  0.029736  0.000000e+00\n",
       "1       ALTER   0.008289  0.017125  6.288000e-01\n",
       "2       ADHEU  -0.004462  0.069220  9.486558e-01\n",
       "3    HOCHOZON  -0.099654  0.042450  1.968645e-02\n",
       "4      AMATOP   0.012938  0.034508  7.080294e-01\n",
       "5      AVATOP  -0.045576  0.036278  2.101950e-01\n",
       "6       ADEKZ  -0.013319  0.036899  7.184402e-01\n",
       "7      ARAUCH  -0.016967  0.033215  6.099289e-01\n",
       "8     AGEBGEW   0.018089  0.015769  2.524305e-01\n",
       "9     FSNIGHT   0.029517  0.053945  5.847592e-01\n",
       "10    FLGROSS   0.160807  0.022558  1.111844e-11\n",
       "11      FMILB  -0.045283  0.018954  1.763886e-02\n",
       "12     FNOH24  -0.045715  0.019822  2.192640e-02\n",
       "13      FTIER  -0.020370  0.018788  2.793235e-01\n",
       "14      FPOLL  -0.011656  0.028447  6.823552e-01\n",
       "15   FLTOTMED  -0.016545  0.015892  2.988600e-01\n",
       "16     FO3H24   0.057992  0.032699  7.738207e-02\n",
       "17       FSPT   0.058432  0.072330  4.199576e-01\n",
       "18     FTEH24  -0.035450  0.029702  2.338075e-01\n",
       "19     FSATEM   0.178058  0.085160  3.756239e-02\n",
       "20     FSAUGE  -0.013788  0.052991  7.949313e-01\n",
       "21      FLGEW   0.090220  0.021825  4.890725e-05\n",
       "22     FSPFEI   0.048001  0.090367  5.957694e-01\n",
       "23    FSHLAUF  -0.025496  0.071632  7.221996e-01\n",
       "24       Male   0.103017  0.020996  1.684908e-06\n",
       "25     Female  -0.103017  0.021249  2.206050e-06"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_formatted_code = \"# OLS on train data\\nols_reg = skllm.LinearRegression().fit(X_train, y_train)\\n# R2 score\\nr2 = ols_reg.score(X_test, y_test)\\ncoeffs_table = get_summary_linear_model(ols_reg, X_train, y_train)\\nmost_important = coeffs_table[\\\"Feature\\\"].values[1:][\\n    np.argmin(coeffs_table[\\\"p-values\\\"].values[1:])\\n]\\n# Printing results\\nprint(f\\\"Got an R^2 score of {r2:.2f} for the test set.\\\")\\nprint(f\\\"The most important feature (lowest p-value) is {most_important}.\\\")\\ncoeffs_table\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# OLS on train data\n",
    "ols_reg = skllm.LinearRegression().fit(X_train, y_train)\n",
    "# R2 score\n",
    "r2 = ols_reg.score(X_test, y_test)\n",
    "coeffs_table = get_summary_linear_model(ols_reg, X_train, y_train)\n",
    "most_important = coeffs_table[\"Feature\"].values[1:][\n",
    "    np.argmin(coeffs_table[\"p-values\"].values[1:])\n",
    "]\n",
    "# Printing results\n",
    "print(f\"Got an R^2 score of {r2:.2f} for the test set.\")\n",
    "print(f\"The most important feature (lowest p-value) is {most_important}.\")\n",
    "coeffs_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important (lower p-value) feature seems to be FLGROSS. Some other important features seem to be gender. Male and female seem to completely cancel each other... Overfit maybe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1.3\n",
    "Scikit-learn for some reason doesn't have built in forward and backward selection, so I will create my own functions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 141;\n",
       "                var nbb_formatted_code = \"def backward_elimination(regressor, X_train, y_train, max_p_limit):\\n    regressor.fit(X_train, y_train)\\n    result_table = get_summary_linear_model(regressor, X_train, y_train)\\n    p_values = result_table[\\\"p-values\\\"].values\\n    p_val_max, p_val_max_pos = p_values[1:].max(), p_values[1:].argmax() + 1\\n    feature_max_p_val = result_table[\\\"Feature\\\"][p_val_max_pos]\\n    removed_features = [feature_max_p_val]\\n    X_reduce = X_train.drop(columns=feature_max_p_val, inplace=False)\\n    while p_val_max > max_p_limit:\\n        regressor.fit(X_reduce, y_train)\\n        result_table = get_summary_linear_model(regressor, X_reduce, y_train)\\n        p_values = result_table[\\\"p-values\\\"].values\\n        p_val_max, p_val_max_pos = p_values[1:].max(), p_values[1:].argmax() + 1\\n        feature_max_p_val = result_table[\\\"Feature\\\"][p_val_max_pos]\\n        if p_val_max > max_p_limit:\\n            removed_features.append(feature_max_p_val)\\n            X_reduce.drop(columns=feature_max_p_val, inplace=True)\\n            X_reduce.sort_index(axis=1, inplace=True)\\n            regressor.fit(X_reduce, y_train)\\n\\n    return regressor, X_reduce, result_table, removed_features\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def backward_elimination(regressor, X_train, y_train, max_p_limit):\n",
    "    regressor.fit(X_train, y_train)\n",
    "    result_table = get_summary_linear_model(regressor, X_train, y_train)\n",
    "    p_values = result_table[\"p-values\"].values\n",
    "    p_val_max, p_val_max_pos = p_values[1:].max(), p_values[1:].argmax() + 1\n",
    "    feature_max_p_val = result_table[\"Feature\"][p_val_max_pos]\n",
    "    removed_features = [feature_max_p_val]\n",
    "    X_reduce = X_train.drop(columns=feature_max_p_val, inplace=False)\n",
    "    while p_val_max > max_p_limit:\n",
    "        regressor.fit(X_reduce, y_train)\n",
    "        result_table = get_summary_linear_model(regressor, X_reduce, y_train)\n",
    "        p_values = result_table[\"p-values\"].values\n",
    "        p_val_max, p_val_max_pos = p_values[1:].max(), p_values[1:].argmax() + 1\n",
    "        feature_max_p_val = result_table[\"Feature\"][p_val_max_pos]\n",
    "        if p_val_max > max_p_limit:\n",
    "            removed_features.append(feature_max_p_val)\n",
    "            X_reduce.drop(columns=feature_max_p_val, inplace=True)\n",
    "            X_reduce.sort_index(axis=1, inplace=True)\n",
    "            regressor.fit(X_reduce, y_train)\n",
    "\n",
    "    return regressor, X_reduce, result_table, removed_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6500127981002248 0.6555450391780477\n",
      "        FLGEW   FLGROSS     FMILB  FSATEM  Female  Male\n",
      "414  0.492973 -1.327469 -0.412082       0       1     0\n",
      "123 -0.780541  0.459580 -0.412082       0       1     0\n",
      "488 -0.780541 -0.805588 -0.412082       1       0     1\n",
      "294  1.058980  0.443765 -0.412082       0       0     1\n",
      "26   0.068469 -0.520925 -0.412082       0       1     0\n",
      "..        ...       ...       ...     ...     ...   ...\n",
      "398 -0.780541 -1.153509 -0.412082       0       1     0\n",
      "443 -0.356036  0.507023 -0.412082       0       0     1\n",
      "28  -0.497538 -0.679071 -0.412082       0       1     0\n",
      "402 -1.063544 -0.457667 -0.412082       0       0     1\n",
      "237 -0.922043 -0.884661 -0.412082       0       0     1\n",
      "\n",
      "[248 rows x 6 columns]\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 142;\n",
       "                var nbb_formatted_code = \"base_regressor = skllm.LinearRegression()\\n(\\n    regressor_reduced,\\n    X_reduce_train,\\n    result_table_reduced,\\n    removed_features,\\n) = backward_elimination(base_regressor, X_train, y_train, max_p_limit=1e-2)\\nX_reduce_test = X_test.drop(columns=removed_features).sort_index(axis=1)\\nprint(\\n    ols_reg.score(X_test, y_test), regressor_reduced.score(X_reduce_test, y_test),\\n)\\nprint(X_reduce_train,)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_regressor = skllm.LinearRegression()\n",
    "(\n",
    "    regressor_reduced,\n",
    "    X_reduce_train,\n",
    "    result_table_reduced,\n",
    "    removed_features,\n",
    ") = backward_elimination(base_regressor, X_train, y_train, max_p_limit=1e-2)\n",
    "X_reduce_test = X_test.drop(columns=removed_features).sort_index(axis=1)\n",
    "print(\n",
    "    ols_reg.score(X_test, y_test), regressor_reduced.score(X_reduce_test, y_test),\n",
    ")\n",
    "print(X_reduce_train,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 134;\n",
       "                var nbb_formatted_code = \"def forward_selection(regressor, X_train, y_train, max_p_limit):\\n    X_null = pd.DataFrame({\\\"null\\\": np.zeros_like(y_train)})\\n    regressor.fit(X_null, y_train)\\n    result_table = get_summary_linear_model(regressor, X_null, y_train)\\n    p_val_max = result_table[\\\"p-values\\\"][0]\\n    X_increased = pd.DataFrame()\\n    features = X_train.columns.values\\n    while p_val_max < max_p_limit:\\n        best_p = np.inf\\n        for feature in features:\\n            new_col = pd.DataFrame({feature: X_train[feature].values})\\n            if len(X_increased.values) == 0:\\n                X_candidate = X_increased.append(new_col)\\n            else:\\n                new_col_names = np.append(\\n                    X_increased.columns.values, new_col.columns.values\\n                )\\n                X_candidate = pd.DataFrame(\\n                    np.append(X_increased.values, new_col.values, axis=1),\\n                    columns=new_col_names,\\n                )\\n            regressor.fit(X_candidate, y_train)\\n            result_table = get_summary_linear_model(regressor, X_candidate, y_train)\\n            p_i = result_table[\\\"p-values\\\"].values[-1]\\n            if p_i < best_p:\\n                best_p = p_i\\n                best_new_feature = feature\\n        new_col = pd.DataFrame({best_new_feature: X_train[best_new_feature].values})\\n        if len(X_increased.values) == 0:\\n            X_candidate = X_increased.append(new_col)\\n        else:\\n            new_col_names = np.append(\\n                X_increased.columns.values, new_col.columns.values\\n            )\\n            X_candidate = pd.DataFrame(\\n                np.append(X_increased.values, new_col.values, axis=1),\\n                columns=new_col_names,\\n            )\\n        result_table = get_summary_linear_model(regressor, X_candidate, y_train)\\n        p_val_max = result_table[\\\"p-values\\\"].values.max()\\n        if p_val_max < max_p_limit or True:\\n            X_increased = X_candidate.sort_index(axis=1)\\n            features = features[features != best_new_feature]\\n\\n    omitted_features = features\\n    regressor.fit(X_increased, y_train)\\n    return regressor, X_increased, result_table, omitted_features\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def forward_selection(regressor, X_train, y_train, max_p_limit):\n",
    "    X_null = pd.DataFrame({\"null\": np.zeros_like(y_train)})\n",
    "    regressor.fit(X_null, y_train)\n",
    "    result_table = get_summary_linear_model(regressor, X_null, y_train)\n",
    "    p_val_max = result_table[\"p-values\"][0]\n",
    "    X_increased = pd.DataFrame()\n",
    "    features = X_train.columns.values\n",
    "    while p_val_max < max_p_limit:\n",
    "        best_p = np.inf\n",
    "        for feature in features:\n",
    "            new_col = pd.DataFrame({feature: X_train[feature].values})\n",
    "            if len(X_increased.values) == 0:\n",
    "                X_candidate = X_increased.append(new_col)\n",
    "            else:\n",
    "                new_col_names = np.append(\n",
    "                    X_increased.columns.values, new_col.columns.values\n",
    "                )\n",
    "                X_candidate = pd.DataFrame(\n",
    "                    np.append(X_increased.values, new_col.values, axis=1),\n",
    "                    columns=new_col_names,\n",
    "                )\n",
    "            regressor.fit(X_candidate, y_train)\n",
    "            result_table = get_summary_linear_model(regressor, X_candidate, y_train)\n",
    "            p_i = result_table[\"p-values\"].values[-1]\n",
    "            if p_i < best_p:\n",
    "                best_p = p_i\n",
    "                best_new_feature = feature\n",
    "        new_col = pd.DataFrame({best_new_feature: X_train[best_new_feature].values})\n",
    "        if len(X_increased.values) == 0:\n",
    "            X_candidate = X_increased.append(new_col)\n",
    "        else:\n",
    "            new_col_names = np.append(\n",
    "                X_increased.columns.values, new_col.columns.values\n",
    "            )\n",
    "            X_candidate = pd.DataFrame(\n",
    "                np.append(X_increased.values, new_col.values, axis=1),\n",
    "                columns=new_col_names,\n",
    "            )\n",
    "        result_table = get_summary_linear_model(regressor, X_candidate, y_train)\n",
    "        p_val_max = result_table[\"p-values\"].values.max()\n",
    "        if p_val_max < max_p_limit or True:\n",
    "            X_increased = X_candidate.sort_index(axis=1)\n",
    "            features = features[features != best_new_feature]\n",
    "\n",
    "    omitted_features = features\n",
    "    regressor.fit(X_increased, y_train)\n",
    "    return regressor, X_increased, result_table, omitted_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bernhard/.local/share/virtualenvs/STK-IN4300-oblig2-Oip3DdD8/lib/python3.7/site-packages/ipykernel_launcher.py:26: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6633110164289205\n",
      "        FLGEW   FLGROSS  FSATEM  Female  Male\n",
      "0    0.492973 -1.327469     0.0     1.0   0.0\n",
      "1   -0.780541  0.459580     0.0     1.0   0.0\n",
      "2   -0.780541 -0.805588     1.0     0.0   1.0\n",
      "3    1.058980  0.443765     0.0     0.0   1.0\n",
      "4    0.068469 -0.520925     0.0     1.0   0.0\n",
      "..        ...       ...     ...     ...   ...\n",
      "243 -0.780541 -1.153509     0.0     1.0   0.0\n",
      "244 -0.356036  0.507023     0.0     0.0   1.0\n",
      "245 -0.497538 -0.679071     0.0     1.0   0.0\n",
      "246 -1.063544 -0.457667     0.0     0.0   1.0\n",
      "247 -0.922043 -0.884661     0.0     0.0   1.0\n",
      "\n",
      "[248 rows x 5 columns]         FLGEW   FLGROSS  FSATEM  Female  Male\n",
      "419  1.058980  0.317248       0       1     0\n",
      "366  3.181504  1.487528       0       0     1\n",
      "329 -1.063544 -1.627946       0       0     1\n",
      "404 -1.346548 -1.548874       0       0     1\n",
      "454 -0.497538 -0.520925       0       1     0\n",
      "..        ...       ...     ...     ...   ...\n",
      "42  -0.073033 -0.758144       0       1     0\n",
      "433  1.058980  0.775872       0       0     1\n",
      "53   1.341983  1.772191       0       1     0\n",
      "35   1.907990  0.680984       0       0     1\n",
      "326 -1.346548 -0.615813       0       0     1\n",
      "\n",
      "[248 rows x 5 columns]\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 135;\n",
       "                var nbb_formatted_code = \"base_regressor = skllm.LinearRegression()\\n(\\n    regressor_increased,\\n    X_increased_train,\\n    result_table_increased,\\n    omitted_features_increased,\\n) = forward_selection(base_regressor, X_train, y_train, max_p_limit=1e-2)\\nX_increased_test = X_test.drop(columns=omitted_features_increased).sort_index(axis=1)\\nprint(regressor_increased.score(X_increased_test, y_test))\\nprint(X_increased_train, X_increased_test)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_regressor = skllm.LinearRegression()\n",
    "(\n",
    "    regressor_increased,\n",
    "    X_increased_train,\n",
    "    result_table_increased,\n",
    "    omitted_features_increased,\n",
    ") = forward_selection(base_regressor, X_train, y_train, max_p_limit=1e-2)\n",
    "X_increased_test = X_test.drop(columns=omitted_features_increased).sort_index(axis=1)\n",
    "print(regressor_increased.score(X_increased_test, y_test))\n",
    "print(X_increased_train, X_increased_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
