{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 1;\n",
       "                var nbb_formatted_code = \"import pandas as pd  # For dataframe\\nimport numpy as np  # For matrix operations\\nimport sklearn.preprocessing as sklpre  # For preprocessing (scaling)\\nimport sklearn.linear_model as skllm  # For OLS\\nimport sklearn.model_selection as sklms  # For train_test_split\\nfrom scipy import stats  # To calc p-value\\nimport matplotlib.pyplot as plt  # For plotting\\nimport pygam  # For generalized additive models\\n\\n# For automatic formatting of code, sparing you from my usually horrible looking code\\n%load_ext nb_black\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd  # For dataframe\n",
    "import numpy as np  # For matrix operations\n",
    "import sklearn.preprocessing as sklpre  # For preprocessing (scaling)\n",
    "import sklearn.linear_model as skllm  # For OLS\n",
    "import sklearn.model_selection as sklms  # For train_test_split\n",
    "from scipy import stats  # To calc p-value\n",
    "import matplotlib.pyplot as plt  # For plotting\n",
    "import pygam  # For generalized additive models\n",
    "\n",
    "# For automatic formatting of code, sparing you from my usually horrible looking code\n",
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1.1\n",
    "I have chosen to one-hot encode the SEX-category, as neither male nor female should be considered adifferent numbers. \n",
    "The rest of the categorical values are just true/false, so those aren't encoded. \n",
    "Then I scale all the scalar features, not touching the categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_formatted_code = \"# Reading the data into dataframe\\ndf = pd.read_csv(\\\"data_task_1.txt\\\", header=0, sep=\\\" \\\")\\n# Onehot-encoding gender\\nonehot_gender = pd.get_dummies(df[\\\"SEX\\\"]).set_axis(\\n    [\\\"Male\\\", \\\"Female\\\"], axis=1, inplace=False\\n)\\n# Replacing old gender column\\ndf = df.join(onehot_gender)\\ndf.drop(\\\"SEX\\\", axis=1, inplace=True)\\n# List of boolean categories\\ncategorical = [\\n    \\\"ADHEU\\\",\\n    \\\"HOCHOZON\\\",\\n    \\\"AMATOP\\\",\\n    \\\"AVATOP\\\",\\n    \\\"ADEKZ\\\",\\n    \\\"ARAUCH\\\",\\n    \\\"FSNIGHT\\\",\\n    \\\"FSPT\\\",\\n    \\\"FSATEM\\\",\\n    \\\"FSAUGE\\\",\\n    \\\"FSPFEI\\\",\\n    \\\"FSHLAUF\\\",\\n    \\\"Male\\\",\\n    \\\"Female\\\",\\n]\\n\\n# A loop that splits the data and tries again until there is no split where only one modality is in one split\\nfirst = True\\nwhile (\\n    first\\n    or np.any(\\n        np.logical_or(X_train.sum(axis=0) == 0, X_train.sum(axis=0) == X_train.shape[0])\\n    )\\n    or np.any(\\n        np.logical_or(X_test.sum(axis=0) == 0, X_test.sum(axis=0) == X_test.shape[0])\\n    )\\n):\\n    first = False\\n    # Splitting over and over until splits are good, stratifying the most biased feature.\\n    X_train, X_test, y_train, y_test = sklms.train_test_split(\\n        df.loc[:, df.columns != \\\"FFVC\\\"],\\n        df[\\\"FFVC\\\"],\\n        test_size=0.5,\\n        stratify=df[\\\"FSATEM\\\"],\\n    )\\n# Scaling scalar features based on train set\\nscaler = sklpre.StandardScaler()\\nX_train_continous = scaler.fit_transform(\\n    X_train.loc[:, np.logical_not(np.isin(X_train.columns, categorical))].values\\n)\\nX_test_continous = scaler.transform(\\n    X_test.loc[:, np.logical_not(np.isin(X_test.columns, categorical))].values\\n)\\n# Putting all scalar and categorical features together\\nX_train.loc[\\n    :, np.logical_not(np.isin(X_train.columns, categorical))\\n] = X_train_continous\\nX_test.loc[:, np.logical_not(np.isin(X_test.columns, categorical))] = X_test_continous\\n# All preprocessing done!\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reading the data into dataframe\n",
    "df = pd.read_csv(\"data_task_1.txt\", header=0, sep=\" \")\n",
    "# Onehot-encoding gender\n",
    "onehot_gender = pd.get_dummies(df[\"SEX\"]).set_axis(\n",
    "    [\"Male\", \"Female\"], axis=1, inplace=False\n",
    ")\n",
    "# Replacing old gender column\n",
    "df = df.join(onehot_gender)\n",
    "df.drop(\"SEX\", axis=1, inplace=True)\n",
    "# List of boolean categories\n",
    "categorical = [\n",
    "    \"ADHEU\",\n",
    "    \"HOCHOZON\",\n",
    "    \"AMATOP\",\n",
    "    \"AVATOP\",\n",
    "    \"ADEKZ\",\n",
    "    \"ARAUCH\",\n",
    "    \"FSNIGHT\",\n",
    "    \"FSPT\",\n",
    "    \"FSATEM\",\n",
    "    \"FSAUGE\",\n",
    "    \"FSPFEI\",\n",
    "    \"FSHLAUF\",\n",
    "    \"Male\",\n",
    "    \"Female\",\n",
    "]\n",
    "\n",
    "# A loop that splits the data and tries again until there is no split where only one modality is in one split\n",
    "first = True\n",
    "while (\n",
    "    first\n",
    "    or np.any(\n",
    "        np.logical_or(X_train.sum(axis=0) == 0, X_train.sum(axis=0) == X_train.shape[0])\n",
    "    )\n",
    "    or np.any(\n",
    "        np.logical_or(X_test.sum(axis=0) == 0, X_test.sum(axis=0) == X_test.shape[0])\n",
    "    )\n",
    "):\n",
    "    first = False\n",
    "    # Splitting over and over until splits are good, stratifying the most biased feature.\n",
    "    X_train, X_test, y_train, y_test = sklms.train_test_split(\n",
    "        df.loc[:, df.columns != \"FFVC\"],\n",
    "        df[\"FFVC\"],\n",
    "        test_size=0.5,\n",
    "        stratify=df[\"FSATEM\"],\n",
    "    )\n",
    "# Scaling scalar features based on train set\n",
    "scaler = sklpre.StandardScaler()\n",
    "X_train_continous = scaler.fit_transform(\n",
    "    X_train.loc[:, np.logical_not(np.isin(X_train.columns, categorical))].values\n",
    ")\n",
    "X_test_continous = scaler.transform(\n",
    "    X_test.loc[:, np.logical_not(np.isin(X_test.columns, categorical))].values\n",
    ")\n",
    "# Putting all scalar and categorical features together\n",
    "X_train.loc[\n",
    "    :, np.logical_not(np.isin(X_train.columns, categorical))\n",
    "] = X_train_continous\n",
    "X_test.loc[:, np.logical_not(np.isin(X_test.columns, categorical))] = X_test_continous\n",
    "# All preprocessing done!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1.2\n",
    "Running OLS, calculating uncertainties and p-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_formatted_code = \"gam = pygam.GAM(pygam.s)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gam = pygam.GAM(pygam.s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "terms must be a TermList, but found terms = <function s at 0x7fa014a26950>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-ee94b9c82332>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpygam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGAM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpygam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/share/virtualenvs/STK-IN4300-oblig2-dUJPFG6m/lib/python3.7/site-packages/pygam/pygam.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, weights)\u001b[0m\n\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0;31m# validate parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 892\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m         \u001b[0;31m# validate data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/STK-IN4300-oblig2-dUJPFG6m/lib/python3.7/site-packages/pygam/pygam.py\u001b[0m in \u001b[0;36m_validate_params\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterms\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m'auto'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTermList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTerm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             raise ValueError('terms must be a TermList, but found '\\\n\u001b[0;32m--> 227\u001b[0;31m                              'terms = {}'.format(self.terms))\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0;31m# max_iter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: terms must be a TermList, but found terms = <function s at 0x7fa014a26950>"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_formatted_code = \"gam = pygam.GAM(pygam.s).fit(X_train, y_train)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gam = pygam.GAM(pygam.s).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important (lower p-value) feature seems to be FLGROSS. Some other important features seem to be gender. Male and female seem to completely cancel each other, implying that men are of higher risk?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1.3\n",
    "Scikit-learn for some reason doesn't have built in forward and backward selection, so I will create my own functions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_elimination(regressor, X_train, y_train, max_p_limit):\n",
    "    \"\"\"\n",
    "    Takes a regressor, training set and a max p-value, runs backward\n",
    "    elimination and returns the regresson fitted on the reduced\n",
    "    features, the reduced feature matrix, a table of betas, \n",
    "    standard deviations and p-values and the removed features\n",
    "    \"\"\"\n",
    "    # Fitting regressor on full model\n",
    "    regressor.fit(X_train, y_train)\n",
    "    # Getting table of p-values to find what to eliminate\n",
    "    result_table = get_summary_linear_model(regressor, X_train, y_train)\n",
    "    p_values = result_table[\"p-values\"].values\n",
    "    p_val_max, p_val_max_pos = p_values[1:].max(), p_values[1:].argmax() + 1\n",
    "    # Getting name of feature with highest p-val to make list of removed features\n",
    "    feature_max_p_val = result_table[\"Feature\"][p_val_max_pos]\n",
    "    removed_features = [feature_max_p_val]\n",
    "    # Dropping feature with highest p-val\n",
    "    X_reduce = X_train.drop(columns=feature_max_p_val, inplace=False)\n",
    "    # Running backwards elimination until all p-values are below limit\n",
    "    while p_val_max > max_p_limit:\n",
    "        # Fitting on reduced model\n",
    "        regressor.fit(X_reduce, y_train)\n",
    "        result_table = get_summary_linear_model(regressor, X_reduce, y_train)\n",
    "        p_values = result_table[\"p-values\"].values\n",
    "        p_val_max, p_val_max_pos = p_values[1:].max(), p_values[1:].argmax() + 1\n",
    "        feature_max_p_val = result_table[\"Feature\"][p_val_max_pos]\n",
    "        # If one or more features have to high p-value, remove\n",
    "        if p_val_max > max_p_limit:\n",
    "            # Append name to list that keeps track of removed features\n",
    "            removed_features.append(feature_max_p_val)\n",
    "            # Dropping feature\n",
    "            X_reduce.drop(columns=feature_max_p_val, inplace=True)\n",
    "            # Sorting features\n",
    "            X_reduce.sort_index(axis=1, inplace=True)\n",
    "            # Fitting reduced model\n",
    "            regressor.fit(X_reduce, y_train)\n",
    "\n",
    "    return regressor, X_reduce, result_table, removed_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model to send in to function. It is an OLS regressor.\n",
    "base_regressor = skllm.LinearRegression()\n",
    "# Running backwards elimination\n",
    "(\n",
    "    regressor_reduced,\n",
    "    X_reduce_train,\n",
    "    result_table_reduced,\n",
    "    removed_features,\n",
    ") = backward_elimination(base_regressor, X_train, y_train, max_p_limit=1e-2)\n",
    "# Using list of reduced features to also create test feature matrix with same features\n",
    "X_reduce_test = X_test.drop(columns=removed_features).sort_index(axis=1)\n",
    "# Printing results\n",
    "print(\n",
    "    f\"R^2 score of full model: {ols_reg.score(X_test, y_test):.2f} Backward Model: {regressor_reduced.score(X_reduce_test, y_test):.2f}\"\n",
    ")\n",
    "result_table_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_selection(regressor, X_train, y_train, max_p_limit):\n",
    "    \"\"\"\n",
    "    Takes a regressor, training set and a max p-value, runs forward\n",
    "    selection and returns the regresson fitted on the reduced\n",
    "    features, the reduced feature matrix, a table of betas, \n",
    "    standard deviations and p-values and the removed features\n",
    "    \"\"\"\n",
    "    X_null = pd.DataFrame({\"null\": np.zeros_like(y_train)})\n",
    "    regressor.fit(X_null, y_train)\n",
    "    # The p-value for the 0-column is invalid, but also not used, so I ignore the warnings\n",
    "    with np.errstate(invalid=\"ignore\"):\n",
    "        # Getting results for null-model\n",
    "        result_table = get_summary_linear_model(regressor, X_null, y_train)\n",
    "    # p-value for intercept\n",
    "    p_val_max = result_table[\"p-values\"][0]\n",
    "    # Dataframe used for incresing\n",
    "    X_increased = pd.DataFrame()\n",
    "    # List of features\n",
    "    features = X_train.columns.values\n",
    "    # while max p-val is below threshold, repeat\n",
    "    while p_val_max < max_p_limit:\n",
    "        # Set best p to infinity so that all values are less\n",
    "        best_p = np.inf\n",
    "        # Looping over features\n",
    "        for feature in features:\n",
    "            # Creating new column with feature in loop\n",
    "            new_col = pd.DataFrame({feature: X_train[feature].values})\n",
    "            # If null model we need to append to the dataframe differently than usual\n",
    "            if len(X_increased.values) == 0:\n",
    "                # Adding new feature to null model\n",
    "                X_candidate = X_increased.append(new_col)\n",
    "            else:\n",
    "                # Adding new feature to model\n",
    "                new_col_names = np.append(\n",
    "                    X_increased.columns.values, new_col.columns.values\n",
    "                )\n",
    "                X_candidate = pd.DataFrame(\n",
    "                    np.append(X_increased.values, new_col.values, axis=1),\n",
    "                    columns=new_col_names,\n",
    "                )\n",
    "            # Fitting increased model to find p-value\n",
    "            regressor.fit(X_candidate, y_train)\n",
    "            result_table = get_summary_linear_model(regressor, X_candidate, y_train)\n",
    "            p_i = result_table[\"p-values\"].values[-1]\n",
    "            # This if-statement is used to find the minimum p-value of the potential features to add\n",
    "            if p_i < best_p:\n",
    "                best_p = p_i\n",
    "                best_new_feature = feature\n",
    "        # Now that we have the best feature to add, we add it properly\n",
    "        new_col = pd.DataFrame({best_new_feature: X_train[best_new_feature].values})\n",
    "        if len(X_increased.values) == 0:\n",
    "            X_candidate = X_increased.append(new_col)\n",
    "        else:\n",
    "            new_col_names = np.append(\n",
    "                X_increased.columns.values, new_col.columns.values\n",
    "            )\n",
    "            X_candidate = pd.DataFrame(\n",
    "                np.append(X_increased.values, new_col.values, axis=1),\n",
    "                columns=new_col_names,\n",
    "            )\n",
    "        # Get results for new model\n",
    "        result_table = get_summary_linear_model(regressor, X_candidate, y_train)\n",
    "        p_val_max = result_table[\"p-values\"].values.max()\n",
    "\n",
    "        # Sorting features\n",
    "        X_increased = X_candidate.sort_index(axis=1)\n",
    "        # Removing added feature from list of potential featues so that we can't add it again next iteration\n",
    "        features = features[features != best_new_feature]\n",
    "\n",
    "    # List of omitted features\n",
    "    omitted_features = features\n",
    "    # Fitting increased model\n",
    "    regressor.fit(X_increased, y_train)\n",
    "    # Table of results for best model\n",
    "    result_table_best = get_summary_linear_model(regressor, X_increased, y_train)\n",
    "    return regressor, X_increased, result_table_best, omitted_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model to send in to function. It is an OLS regressor.\n",
    "base_regressor = skllm.LinearRegression()\n",
    "# Running forward selection\n",
    "(\n",
    "    regressor_increased,\n",
    "    X_increased_train,\n",
    "    result_table_increased,\n",
    "    omitted_features_increased,\n",
    ") = forward_selection(base_regressor, X_train, y_train, max_p_limit=1e-2)\n",
    "# Using list of reduced features to also create test feature matrix with same features\n",
    "X_increased_test = X_test.drop(columns=omitted_features_increased).sort_index(axis=1)\n",
    "# Printing results\n",
    "print(\n",
    "    f\"R^2 score of forward model: {regressor_increased.score(X_increased_test, y_test):.2f}\"\n",
    ")\n",
    "result_table_increased"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the reduced models with $p_\\text{max}=0.01$, both forward and backward selection give the exact same model, and therefore the same features. They also get a slightly better $R^2$-score, possibly because they have less features, and are therefore less likely to overfit on the training data. I chose to look at $R^2$ instead of MSE as I feel it is a more intuitive value. However, higher $R^2$ also implies lower MSE, so the models are better.\n",
    "\n",
    "Next I will test with a less strict $p_\\text{max}=0.1$ and see how the models perform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model to send in to function. It is an OLS regressor.\n",
    "base_regressor = skllm.LinearRegression()\n",
    "# Running backwards elimination\n",
    "(\n",
    "    regressor_reduced_less_strict,\n",
    "    X_reduce_train_less_strict,\n",
    "    result_table_reduced_less_strict,\n",
    "    removed_features_less_strict,\n",
    ") = backward_elimination(base_regressor, X_train, y_train, max_p_limit=1e-1)\n",
    "# Using list of reduced features to also create test feature matrix with same features\n",
    "X_reduce_test_less_strict = X_test.drop(\n",
    "    columns=removed_features_less_strict\n",
    ").sort_index(axis=1)\n",
    "# Printing results\n",
    "print(\n",
    "    f\"R^2 score of full model: {ols_reg.score(X_test, y_test):.2f}\"\n",
    "    + f\" Backward Model: {regressor_reduced_less_strict.score(X_reduce_test_less_strict, y_test):.2f}\"\n",
    ")\n",
    "result_table_reduced_less_strict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model to send in to function. It is an OLS regressor.\n",
    "base_regressor = skllm.LinearRegression()\n",
    "# Running forward selection\n",
    "(\n",
    "    regressor_increased_less_strict,\n",
    "    X_increased_train_less_strict,\n",
    "    result_table_increased_less_strict,\n",
    "    omitted_features_increased_less_strict,\n",
    ") = forward_selection(base_regressor, X_train, y_train, max_p_limit=1e-1)\n",
    "# Using list of reduced features to also create test feature matrix with same features\n",
    "X_increased_test_less_strict = X_test.drop(\n",
    "    columns=omitted_features_increased_less_strict\n",
    ").sort_index(axis=1)\n",
    "# Printing results\n",
    "print(\n",
    "    f\"R^2 score of forward model: {regressor_increased.score(X_increased_test, y_test):.2f}\"\n",
    ")\n",
    "result_table_increased_less_strict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the models are not the same anymore. This is to be expected, as the p-values estimated are not the same for each feature independent of the other features. The backward elimination model seems to give a better $R^2$-score this time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1.4\n",
    "CV is easily implemented in Scikit-Learn. Bootstap on the other hand... I need to create my own class (Maybe there is a better way of doing this than what I'm doing...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-fold cross validation, n-jobs=-1 is for parallelisation (use multiple cpu cores)\n",
    "lasso_cv = skllm.LassoCV(n_jobs=-1, cv=5).fit(X_train, y_train)\n",
    "# List of hyperparameters\n",
    "lambdas_lasso_cv = lasso_cv.alphas_\n",
    "# List of validation mean squared errors. Need to average them over axis 1 to get average across all 5 folds\n",
    "mses_lasso_cv = lasso_cv.mse_path_.mean(axis=1)\n",
    "print(\n",
    "    f\"Best hyperparameter is {lasso_cv.alpha_:.2e}, giving a test R^2 score of {lasso_cv.score(X_test, y_test):.2f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems to be very similar to forward and backward selection. Now I need to make a new class for bootstrap manually implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bootstrap:\n",
    "    def __init__(self, y):\n",
    "        \"\"\"\n",
    "        I was sort of confused on how to this, so I just made a class and tried stuff. \n",
    "        Now it works, so I won't change it anymore.\n",
    "        This class takes y_train and saves its length.\n",
    "        \"\"\"\n",
    "        self.len_y = len(y)\n",
    "\n",
    "    @property\n",
    "    def get_bootstrap(self):\n",
    "        \"\"\"\n",
    "        This method returns one train-validation bootstrap split of the training data (indices of the data).\n",
    "        The @property is just so that i can call get_bootstrap without the () after (like in get_bootstrap()).\n",
    "        This splits train and validation into 2/3, 1/3 of the length of the data. Not sure if that is the correct\n",
    "        method.\n",
    "        \"\"\"\n",
    "        # All indices (0-lenght of y)\n",
    "        indices = np.arange(self.len_y)\n",
    "        # Picking random indices with replacement.\n",
    "        indices_train = np.random.choice(\n",
    "            indices, replace=True, size=int(self.len_y * 0.67)\n",
    "        )\n",
    "        indices_validate = np.random.choice(\n",
    "            indices, replace=True, size=int(self.len_y * 0.37)\n",
    "        )\n",
    "        # Returns a list of lists\n",
    "        return [indices_train.tolist(), indices_validate.tolist()]\n",
    "\n",
    "\n",
    "# Creating instance of Bootstrap class\n",
    "bootstrap = Bootstrap(y_train)\n",
    "splits = []\n",
    "# This loops creates 100 different bootstrap samples\n",
    "for i in range(100):\n",
    "    splits.append(bootstrap.get_bootstrap)\n",
    "# Running LassoCV with bootstrap instead of CV.\n",
    "lasso_bootstrap = skllm.LassoCV(n_jobs=-1, cv=splits).fit(X_train, y_train)\n",
    "\n",
    "# Lambdas tried by the solver\n",
    "lambdas_lasso_bootstrap = lasso_bootstrap.alphas_\n",
    "# List of validation mean squared errors. Need to average them over axis 1 to get average across all 100 bootstraps\n",
    "mses_lasso_bootstrap = lasso_bootstrap.mse_path_.mean(axis=1)\n",
    "\n",
    "print(\n",
    "    f\"Best hyperparameter is {lasso_bootstrap.alpha_:.2e}, giving a test R^2 score of {lasso_bootstrap.score(X_test, y_test):.2f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fonts = {\n",
    "    \"font.family\": \"serif\",\n",
    "    \"axes.labelsize\": 12,\n",
    "    \"font.size\": 12,\n",
    "    \"legend.fontsize\": 12,\n",
    "    \"xtick.labelsize\": 12,\n",
    "    \"ytick.labelsize\": 12,\n",
    "}\n",
    "\n",
    "plt.rcParams.update(fonts)\n",
    "\n",
    "# Log plot\n",
    "plt.figure(figsize=(10, 6.7))\n",
    "plt.semilogx(lambdas_lasso_bootstrap, mses_lasso_bootstrap, label=\"Bootsrap\")\n",
    "plt.semilogx(lambdas_lasso_cv, mses_lasso_cv, label=\"5-fold cross validation\")\n",
    "plt.xlabel(r\"Hyperparameter $\\lambda$\")\n",
    "plt.ylabel(r\"|MSE|\")\n",
    "plt.title(\"Bootstrap lasso results\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bootstrap and cross validation seem to estimate the error quite similarily. CV is probably a bit more realistic, as the error is a bit higher, but that i not important. The important thing is where the minimum is, and it seems to be almost the same for both methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 score test: 0.48710655488811494\n",
      "LinearGAM                                                                                                 \n",
      "=============================================== ==========================================================\n",
      "Distribution:                        NormalDist Effective DoF:                                     54.3309\n",
      "Link Function:                     IdentityLink Log Likelihood:                                 -2143.4033\n",
      "Number of Samples:                          248 AIC:                                             4397.4684\n",
      "                                                AICc:                                            4429.9916\n",
      "                                                GCV:                                                0.0573\n",
      "                                                Scale:                                              0.0353\n",
      "                                                Pseudo R-Squared:                                   0.7423\n",
      "==========================================================================================================\n",
      "Feature Function                  Lambda               Rank         EDoF         P > x        Sig. Code   \n",
      "================================= ==================== ============ ============ ============ ============\n",
      "s(0)                              [0.6]                20                        1.11e-16     ***         \n",
      "s(1)                              [0.6]                20                        1.11e-16     ***         \n",
      "s(2)                              [0.6]                20                        1.11e-16     ***         \n",
      "s(3)                              [0.6]                20                        1.11e-16     ***         \n",
      "s(4)                              [0.6]                20                        1.11e-16     ***         \n",
      "s(5)                              [0.6]                20                        1.11e-16     ***         \n",
      "s(6)                              [0.6]                20                        1.11e-16     ***         \n",
      "s(7)                              [0.6]                20                        1.11e-16     ***         \n",
      "s(8)                              [0.6]                20                        1.11e-16     ***         \n",
      "s(9)                              [0.6]                20                        1.11e-16     ***         \n",
      "s(10)                             [0.6]                20                        1.11e-16     ***         \n",
      "s(11)                             [0.6]                20                        1.11e-16     ***         \n",
      "s(12)                             [0.6]                20                        1.11e-16     ***         \n",
      "s(13)                             [0.6]                20                        1.11e-16     ***         \n",
      "s(14)                             [0.6]                20                        1.11e-16     ***         \n",
      "s(15)                             [0.6]                20                        8.14e-10     ***         \n",
      "s(16)                             [0.6]                20                        1.11e-16     ***         \n",
      "s(17)                             [0.6]                20                        2.05e-06     ***         \n",
      "s(18)                             [0.6]                20                        1.11e-16     ***         \n",
      "s(19)                             [0.6]                20                        1.11e-16     ***         \n",
      "s(20)                             [0.6]                20                        1.11e-16     ***         \n",
      "s(21)                             [0.6]                20                        1.11e-16     ***         \n",
      "s(22)                             [0.6]                20                        1.11e-16     ***         \n",
      "s(23)                             [0.6]                20                        1.11e-16     ***         \n",
      "s(24)                             [0.6]                20                        1.11e-16     ***         \n",
      "intercept                                              1                         1.11e-16     ***         \n",
      "==========================================================================================================\n",
      "Significance codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
      "\n",
      "WARNING: Fitting splines and a linear function to a feature introduces a model identifiability problem\n",
      "         which can cause p-values to appear significant when they are not.\n",
      "\n",
      "WARNING: p-values calculated in this manner behave correctly for un-penalized models or models with\n",
      "         known smoothing parameters, but when smoothing parameters have been estimated, the p-values\n",
      "         are typically lower than they should be, meaning that the tests reject the null too readily.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bernhard/.local/share/virtualenvs/STK-IN4300-oblig2-dUJPFG6m/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: KNOWN BUG: p-values computed in this summary are likely much smaller than they should be. \n",
      " \n",
      "Please do not make inferences based on these values! \n",
      "\n",
      "Collaborate on a solution, and stay up to date at: \n",
      "github.com/dswah/pyGAM/issues/163 \n",
      "\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 23;\n",
       "                var nbb_formatted_code = \"splines_features = pygam.s(0)\\nfor i in range(1, X_train.shape[1]):\\n    splines_features += pygam.s(i)\\ngam = pygam.LinearGAM(splines_features)\\ngam.fit(X_train, y_train)\\nr2_test_gam = gam._estimate_r2(X_test, y_test)[\\\"explained_deviance\\\"]\\nprint(f\\\"R^2 score test: {r2_test_gam}\\\")\\ngam.summary()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "splines_features = pygam.s(0)\n",
    "for i in range(1, X_train.shape[1]):\n",
    "    splines_features += pygam.s(i)\n",
    "gam = pygam.LinearGAM(splines_features)\n",
    "gam.fit(X_train, y_train)\n",
    "r2_test_gam = gam._estimate_r2(X_test, y_test)[\"explained_deviance\"]\n",
    "print(f\"R^2 score test: {r2_test_gam}\")\n",
    "gam.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
