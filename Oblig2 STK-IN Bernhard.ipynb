{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 1;\n",
       "                var nbb_formatted_code = \"import pandas as pd  # For dataframe\\nimport numpy as np  # For matrix operations\\nimport sklearn.preprocessing as sklpre  # For preprocessing (scaling)\\nimport sklearn.linear_model as skllm  # For OLS\\nimport sklearn.model_selection as sklms  # For train_test_split\\nfrom scipy import stats  # To calc p-value\\nimport matplotlib.pyplot as plt  # For plotting\\nimport pygam  # For generalized additive models\\n\\n# For automatic formatting of code, sparing you from my usually horrible looking code\\n%load_ext nb_black\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd  # For dataframe\n",
    "import numpy as np  # For matrix operations\n",
    "import sklearn.preprocessing as sklpre  # For preprocessing (scaling)\n",
    "import sklearn.linear_model as skllm  # For OLS\n",
    "import sklearn.model_selection as sklms  # For train_test_split\n",
    "from scipy import stats  # To calc p-value\n",
    "import matplotlib.pyplot as plt  # For plotting\n",
    "import pygam  # For generalized additive models\n",
    "\n",
    "# For automatic formatting of code, sparing you from my usually horrible looking code\n",
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1.1\n",
    "I have chosen to one-hot encode the SEX-category, as neither male nor female should be considered adifferent numbers. \n",
    "The rest of the categorical values are just true/false, so those aren't encoded. \n",
    "Then I scale all the scalar features, not touching the categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_formatted_code = \"# Reading the data into dataframe\\ndf = pd.read_csv(\\\"data_task_1.txt\\\", header=0, sep=\\\" \\\")\\n# Onehot-encoding gender\\nonehot_gender = pd.get_dummies(df[\\\"SEX\\\"]).set_axis(\\n    [\\\"Male\\\", \\\"Female\\\"], axis=1, inplace=False\\n)\\n# Replacing old gender column\\ndf = df.join(onehot_gender)\\ndf.drop(\\\"SEX\\\", axis=1, inplace=True)\\n# List of boolean categories\\ncategorical = [\\n    \\\"ADHEU\\\",\\n    \\\"HOCHOZON\\\",\\n    \\\"AMATOP\\\",\\n    \\\"AVATOP\\\",\\n    \\\"ADEKZ\\\",\\n    \\\"ARAUCH\\\",\\n    \\\"FSNIGHT\\\",\\n    \\\"FSPT\\\",\\n    \\\"FSATEM\\\",\\n    \\\"FSAUGE\\\",\\n    \\\"FSPFEI\\\",\\n    \\\"FSHLAUF\\\",\\n    \\\"Male\\\",\\n    \\\"Female\\\",\\n]\\n\\n# A loop that splits the data and tries again until there is no split where only one modality is in one split\\nfirst = True\\nwhile (\\n    first\\n    or np.any(\\n        np.logical_or(X_train.sum(axis=0) == 0, X_train.sum(axis=0) == X_train.shape[0])\\n    )\\n    or np.any(\\n        np.logical_or(X_test.sum(axis=0) == 0, X_test.sum(axis=0) == X_test.shape[0])\\n    )\\n):\\n    first = False\\n    # Splitting over and over until splits are good, stratifying the most biased feature.\\n    X_train, X_test, y_train, y_test = sklms.train_test_split(\\n        df.loc[:, df.columns != \\\"FFVC\\\"],\\n        df[\\\"FFVC\\\"],\\n        test_size=0.5,\\n        stratify=df[\\\"FSATEM\\\"],\\n    )\\n# Scaling scalar features based on train set\\nscaler = sklpre.StandardScaler()\\nX_train_continous = scaler.fit_transform(\\n    X_train.loc[:, np.logical_not(np.isin(X_train.columns, categorical))].values\\n)\\nX_test_continous = scaler.transform(\\n    X_test.loc[:, np.logical_not(np.isin(X_test.columns, categorical))].values\\n)\\n# Putting all scalar and categorical features together\\nX_train.loc[\\n    :, np.logical_not(np.isin(X_train.columns, categorical))\\n] = X_train_continous\\nX_test.loc[:, np.logical_not(np.isin(X_test.columns, categorical))] = X_test_continous\\n# All preprocessing done!\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reading the data into dataframe\n",
    "df = pd.read_csv(\"data_task_1.txt\", header=0, sep=\" \")\n",
    "# Onehot-encoding gender\n",
    "onehot_gender = pd.get_dummies(df[\"SEX\"]).set_axis(\n",
    "    [\"Male\", \"Female\"], axis=1, inplace=False\n",
    ")\n",
    "# Replacing old gender column\n",
    "df = df.join(onehot_gender)\n",
    "df.drop(\"SEX\", axis=1, inplace=True)\n",
    "# List of boolean categories\n",
    "categorical = [\n",
    "    \"ADHEU\",\n",
    "    \"HOCHOZON\",\n",
    "    \"AMATOP\",\n",
    "    \"AVATOP\",\n",
    "    \"ADEKZ\",\n",
    "    \"ARAUCH\",\n",
    "    \"FSNIGHT\",\n",
    "    \"FSPT\",\n",
    "    \"FSATEM\",\n",
    "    \"FSAUGE\",\n",
    "    \"FSPFEI\",\n",
    "    \"FSHLAUF\",\n",
    "    \"Male\",\n",
    "    \"Female\",\n",
    "]\n",
    "\n",
    "# A loop that splits the data and tries again until there is no split where only one modality is in one split\n",
    "first = True\n",
    "while (\n",
    "    first\n",
    "    or np.any(\n",
    "        np.logical_or(X_train.sum(axis=0) == 0, X_train.sum(axis=0) == X_train.shape[0])\n",
    "    )\n",
    "    or np.any(\n",
    "        np.logical_or(X_test.sum(axis=0) == 0, X_test.sum(axis=0) == X_test.shape[0])\n",
    "    )\n",
    "):\n",
    "    first = False\n",
    "    # Splitting over and over until splits are good, stratifying the most biased feature.\n",
    "    X_train, X_test, y_train, y_test = sklms.train_test_split(\n",
    "        df.loc[:, df.columns != \"FFVC\"],\n",
    "        df[\"FFVC\"],\n",
    "        test_size=0.5,\n",
    "        stratify=df[\"FSATEM\"],\n",
    "    )\n",
    "# Scaling scalar features based on train set\n",
    "scaler = sklpre.StandardScaler()\n",
    "X_train_continous = scaler.fit_transform(\n",
    "    X_train.loc[:, np.logical_not(np.isin(X_train.columns, categorical))].values\n",
    ")\n",
    "X_test_continous = scaler.transform(\n",
    "    X_test.loc[:, np.logical_not(np.isin(X_test.columns, categorical))].values\n",
    ")\n",
    "# Putting all scalar and categorical features together\n",
    "X_train.loc[\n",
    "    :, np.logical_not(np.isin(X_train.columns, categorical))\n",
    "] = X_train_continous\n",
    "X_test.loc[:, np.logical_not(np.isin(X_test.columns, categorical))] = X_test_continous\n",
    "# All preprocessing done!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1.2\n",
    "Running OLS, calculating uncertainties and p-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_formatted_code = \"def get_summary_linear_model(model, X_train, y_train):\\n    \\\"\\\"\\\"\\n    Scikit-learn has no built in support for confidence intervals and p-values, so I \\n    made this to calculate it for me after fitting the model. Put into a function for reuse.\\n    \\\"\\\"\\\"\\n    # Combining intercept and coefficients in same array\\n    coefficients = np.append(model.intercept_, model.coef_)\\n\\n    # Predicting y\\n    y_hat = model.predict(X_train)\\n    # Calculating RSS to get variance for use when calculating stddev of coeffs\\n    residuals = y_train.values - y_hat\\n    rss = residuals.reshape(-1, 1).T @ residuals.reshape(-1, 1)\\n    var = rss[0, 0] / (len(X_train) - len(X_train.columns) - 1)\\n\\n    # Adding intercept to X_train, as sklearn usually does not need the column of 1's\\n    X_with_intercept = np.append(\\n        np.ones(X_train.shape[0]).reshape(-1, 1), X_train, axis=1\\n    )\\n    # Stddev of coefficients\\n    stddev = np.sqrt(\\n        (np.diag(var * np.linalg.pinv(X_with_intercept.T @ X_with_intercept)))\\n    )\\n    labels = [\\\"Intercept\\\"] + X_train.columns.tolist()\\n\\n    coef_over_std = coefficients / stddev\\n    p_values = [\\n        2 * (1 - stats.t.cdf(np.abs(i), (len(X_with_intercept) - 1)))\\n        for i in coef_over_std\\n    ]\\n\\n    # Putting results into table\\n    coeffs_table = pd.DataFrame(zip(labels, coefficients, stddev, p_values))\\n    # Giving nice names with TeX formatting\\n    coeffs_table.rename(\\n        columns={0: \\\"Feature\\\", 1: r\\\"$\\\\beta_i$\\\", 2: r\\\"$\\\\pm$\\\", 3: \\\"p-values\\\"},\\n        inplace=True,\\n    )\\n    return coeffs_table\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_summary_linear_model(model, X_train, y_train):\n",
    "    \"\"\"\n",
    "    Scikit-learn has no built in support for confidence intervals and p-values, so I \n",
    "    made this to calculate it for me after fitting the model. Put into a function for reuse.\n",
    "    \"\"\"\n",
    "    # Combining intercept and coefficients in same array\n",
    "    coefficients = np.append(model.intercept_, model.coef_)\n",
    "\n",
    "    # Predicting y\n",
    "    y_hat = model.predict(X_train)\n",
    "    # Calculating RSS to get variance for use when calculating stddev of coeffs\n",
    "    residuals = y_train.values - y_hat\n",
    "    rss = residuals.reshape(-1, 1).T @ residuals.reshape(-1, 1)\n",
    "    var = rss[0, 0] / (len(X_train) - len(X_train.columns) - 1)\n",
    "\n",
    "    # Adding intercept to X_train, as sklearn usually does not need the column of 1's\n",
    "    X_with_intercept = np.append(\n",
    "        np.ones(X_train.shape[0]).reshape(-1, 1), X_train, axis=1\n",
    "    )\n",
    "    # Stddev of coefficients\n",
    "    stddev = np.sqrt(\n",
    "        (np.diag(var * np.linalg.pinv(X_with_intercept.T @ X_with_intercept)))\n",
    "    )\n",
    "    labels = [\"Intercept\"] + X_train.columns.tolist()\n",
    "\n",
    "    coef_over_std = coefficients / stddev\n",
    "    p_values = [\n",
    "        2 * (1 - stats.t.cdf(np.abs(i), (len(X_with_intercept) - 1)))\n",
    "        for i in coef_over_std\n",
    "    ]\n",
    "\n",
    "    # Putting results into table\n",
    "    coeffs_table = pd.DataFrame(zip(labels, coefficients, stddev, p_values))\n",
    "    # Giving nice names with TeX formatting\n",
    "    coeffs_table.rename(\n",
    "        columns={0: \"Feature\", 1: r\"$\\beta_i$\", 2: r\"$\\pm$\", 3: \"p-values\"},\n",
    "        inplace=True,\n",
    "    )\n",
    "    return coeffs_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got an R^2 score of 0.56 for the test set.\n",
      "The most important feature (lowest p-value) is FLGROSS.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>$\\beta_i$</th>\n",
       "      <th>$\\pm$</th>\n",
       "      <th>p-values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Intercept</td>\n",
       "      <td>2.404864</td>\n",
       "      <td>0.025996</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ALTER</td>\n",
       "      <td>0.010902</td>\n",
       "      <td>0.015386</td>\n",
       "      <td>4.792887e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ADHEU</td>\n",
       "      <td>-0.034566</td>\n",
       "      <td>0.055522</td>\n",
       "      <td>5.341480e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HOCHOZON</td>\n",
       "      <td>-0.142444</td>\n",
       "      <td>0.037347</td>\n",
       "      <td>1.727508e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AMATOP</td>\n",
       "      <td>-0.008849</td>\n",
       "      <td>0.032278</td>\n",
       "      <td>7.841971e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AVATOP</td>\n",
       "      <td>-0.031797</td>\n",
       "      <td>0.033723</td>\n",
       "      <td>3.466491e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ADEKZ</td>\n",
       "      <td>0.006774</td>\n",
       "      <td>0.034745</td>\n",
       "      <td>8.455891e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ARAUCH</td>\n",
       "      <td>-0.015969</td>\n",
       "      <td>0.030271</td>\n",
       "      <td>5.982948e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AGEBGEW</td>\n",
       "      <td>0.008081</td>\n",
       "      <td>0.014265</td>\n",
       "      <td>5.715688e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>FSNIGHT</td>\n",
       "      <td>0.021410</td>\n",
       "      <td>0.048826</td>\n",
       "      <td>6.614067e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>FLGROSS</td>\n",
       "      <td>0.173077</td>\n",
       "      <td>0.020754</td>\n",
       "      <td>5.329071e-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>FMILB</td>\n",
       "      <td>-0.034068</td>\n",
       "      <td>0.021499</td>\n",
       "      <td>1.143309e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>FNOH24</td>\n",
       "      <td>-0.045520</td>\n",
       "      <td>0.016979</td>\n",
       "      <td>7.834625e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>FTIER</td>\n",
       "      <td>-0.003569</td>\n",
       "      <td>0.016459</td>\n",
       "      <td>8.285245e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>FPOLL</td>\n",
       "      <td>0.004415</td>\n",
       "      <td>0.028069</td>\n",
       "      <td>8.751322e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>FLTOTMED</td>\n",
       "      <td>0.007153</td>\n",
       "      <td>0.013607</td>\n",
       "      <td>5.995520e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>FO3H24</td>\n",
       "      <td>0.048460</td>\n",
       "      <td>0.029040</td>\n",
       "      <td>9.643416e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>FSPT</td>\n",
       "      <td>0.006960</td>\n",
       "      <td>0.070558</td>\n",
       "      <td>9.215027e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>FTEH24</td>\n",
       "      <td>-0.035771</td>\n",
       "      <td>0.026553</td>\n",
       "      <td>1.791664e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>FSATEM</td>\n",
       "      <td>0.170363</td>\n",
       "      <td>0.076516</td>\n",
       "      <td>2.688238e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>FSAUGE</td>\n",
       "      <td>0.057964</td>\n",
       "      <td>0.047324</td>\n",
       "      <td>2.218004e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>FLGEW</td>\n",
       "      <td>0.098457</td>\n",
       "      <td>0.019985</td>\n",
       "      <td>1.534548e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>FSPFEI</td>\n",
       "      <td>0.048634</td>\n",
       "      <td>0.076694</td>\n",
       "      <td>5.265825e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>FSHLAUF</td>\n",
       "      <td>-0.088719</td>\n",
       "      <td>0.056862</td>\n",
       "      <td>1.199803e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Male</td>\n",
       "      <td>0.089702</td>\n",
       "      <td>0.018946</td>\n",
       "      <td>3.701543e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Female</td>\n",
       "      <td>-0.089702</td>\n",
       "      <td>0.018960</td>\n",
       "      <td>3.759051e-06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Feature  $\\beta_i$     $\\pm$      p-values\n",
       "0   Intercept   2.404864  0.025996  0.000000e+00\n",
       "1       ALTER   0.010902  0.015386  4.792887e-01\n",
       "2       ADHEU  -0.034566  0.055522  5.341480e-01\n",
       "3    HOCHOZON  -0.142444  0.037347  1.727508e-04\n",
       "4      AMATOP  -0.008849  0.032278  7.841971e-01\n",
       "5      AVATOP  -0.031797  0.033723  3.466491e-01\n",
       "6       ADEKZ   0.006774  0.034745  8.455891e-01\n",
       "7      ARAUCH  -0.015969  0.030271  5.982948e-01\n",
       "8     AGEBGEW   0.008081  0.014265  5.715688e-01\n",
       "9     FSNIGHT   0.021410  0.048826  6.614067e-01\n",
       "10    FLGROSS   0.173077  0.020754  5.329071e-15\n",
       "11      FMILB  -0.034068  0.021499  1.143309e-01\n",
       "12     FNOH24  -0.045520  0.016979  7.834625e-03\n",
       "13      FTIER  -0.003569  0.016459  8.285245e-01\n",
       "14      FPOLL   0.004415  0.028069  8.751322e-01\n",
       "15   FLTOTMED   0.007153  0.013607  5.995520e-01\n",
       "16     FO3H24   0.048460  0.029040  9.643416e-02\n",
       "17       FSPT   0.006960  0.070558  9.215027e-01\n",
       "18     FTEH24  -0.035771  0.026553  1.791664e-01\n",
       "19     FSATEM   0.170363  0.076516  2.688238e-02\n",
       "20     FSAUGE   0.057964  0.047324  2.218004e-01\n",
       "21      FLGEW   0.098457  0.019985  1.534548e-06\n",
       "22     FSPFEI   0.048634  0.076694  5.265825e-01\n",
       "23    FSHLAUF  -0.088719  0.056862  1.199803e-01\n",
       "24       Male   0.089702  0.018946  3.701543e-06\n",
       "25     Female  -0.089702  0.018960  3.759051e-06"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_formatted_code = \"# OLS on train data\\nols_reg = skllm.LinearRegression().fit(X_train, y_train)\\n# R2 score\\nr2 = ols_reg.score(X_test, y_test)\\ncoeffs_table = get_summary_linear_model(ols_reg, X_train, y_train)\\nmost_important = coeffs_table[\\\"Feature\\\"].values[1:][\\n    np.argmin(coeffs_table[\\\"p-values\\\"].values[1:])\\n]\\n# Printing results\\nprint(f\\\"Got an R^2 score of {r2:.2f} for the test set.\\\")\\nprint(f\\\"The most important feature (lowest p-value) is {most_important}.\\\")\\ncoeffs_table\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# OLS on train data\n",
    "ols_reg = skllm.LinearRegression().fit(X_train, y_train)\n",
    "# R2 score\n",
    "r2 = ols_reg.score(X_test, y_test)\n",
    "coeffs_table = get_summary_linear_model(ols_reg, X_train, y_train)\n",
    "most_important = coeffs_table[\"Feature\"].values[1:][\n",
    "    np.argmin(coeffs_table[\"p-values\"].values[1:])\n",
    "]\n",
    "# Printing results\n",
    "print(f\"Got an R^2 score of {r2:.2f} for the test set.\")\n",
    "print(f\"The most important feature (lowest p-value) is {most_important}.\")\n",
    "coeffs_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important (lower p-value) feature seems to be FLGROSS. Some other important features seem to be gender. Male and female seem to completely cancel each other, implying that men are of higher risk?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1.3\n",
    "Scikit-learn for some reason doesn't have built in forward and backward selection, so I will create my own functions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 5;\n",
       "                var nbb_formatted_code = \"def backward_elimination(regressor, X_train, y_train, max_p_limit):\\n    \\\"\\\"\\\"\\n    Takes a regressor, training set and a max p-value, runs backward\\n    elimination and returns the regresson fitted on the reduced\\n    features, the reduced feature matrix, a table of betas, \\n    standard deviations and p-values and the removed features\\n    \\\"\\\"\\\"\\n    # Fitting regressor on full model\\n    regressor.fit(X_train, y_train)\\n    # Getting table of p-values to find what to eliminate\\n    result_table = get_summary_linear_model(regressor, X_train, y_train)\\n    p_values = result_table[\\\"p-values\\\"].values\\n    p_val_max, p_val_max_pos = p_values[1:].max(), p_values[1:].argmax() + 1\\n    # Getting name of feature with highest p-val to make list of removed features\\n    feature_max_p_val = result_table[\\\"Feature\\\"][p_val_max_pos]\\n    removed_features = [feature_max_p_val]\\n    # Dropping feature with highest p-val\\n    X_reduce = X_train.drop(columns=feature_max_p_val, inplace=False)\\n    # Running backwards elimination until all p-values are below limit\\n    while p_val_max > max_p_limit:\\n        # Fitting on reduced model\\n        regressor.fit(X_reduce, y_train)\\n        result_table = get_summary_linear_model(regressor, X_reduce, y_train)\\n        p_values = result_table[\\\"p-values\\\"].values\\n        p_val_max, p_val_max_pos = p_values[1:].max(), p_values[1:].argmax() + 1\\n        feature_max_p_val = result_table[\\\"Feature\\\"][p_val_max_pos]\\n        # If one or more features have to high p-value, remove\\n        if p_val_max > max_p_limit:\\n            # Append name to list that keeps track of removed features\\n            removed_features.append(feature_max_p_val)\\n            # Dropping feature\\n            X_reduce.drop(columns=feature_max_p_val, inplace=True)\\n            # Sorting features\\n            X_reduce.sort_index(axis=1, inplace=True)\\n            # Fitting reduced model\\n            regressor.fit(X_reduce, y_train)\\n\\n    return regressor, X_reduce, result_table, removed_features\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def backward_elimination(regressor, X_train, y_train, max_p_limit):\n",
    "    \"\"\"\n",
    "    Takes a regressor, training set and a max p-value, runs backward\n",
    "    elimination and returns the regresson fitted on the reduced\n",
    "    features, the reduced feature matrix, a table of betas, \n",
    "    standard deviations and p-values and the removed features\n",
    "    \"\"\"\n",
    "    # Fitting regressor on full model\n",
    "    regressor.fit(X_train, y_train)\n",
    "    # Getting table of p-values to find what to eliminate\n",
    "    result_table = get_summary_linear_model(regressor, X_train, y_train)\n",
    "    p_values = result_table[\"p-values\"].values\n",
    "    p_val_max, p_val_max_pos = p_values[1:].max(), p_values[1:].argmax() + 1\n",
    "    # Getting name of feature with highest p-val to make list of removed features\n",
    "    feature_max_p_val = result_table[\"Feature\"][p_val_max_pos]\n",
    "    removed_features = [feature_max_p_val]\n",
    "    # Dropping feature with highest p-val\n",
    "    X_reduce = X_train.drop(columns=feature_max_p_val, inplace=False)\n",
    "    # Running backwards elimination until all p-values are below limit\n",
    "    while p_val_max > max_p_limit:\n",
    "        # Fitting on reduced model\n",
    "        regressor.fit(X_reduce, y_train)\n",
    "        result_table = get_summary_linear_model(regressor, X_reduce, y_train)\n",
    "        p_values = result_table[\"p-values\"].values\n",
    "        p_val_max, p_val_max_pos = p_values[1:].max(), p_values[1:].argmax() + 1\n",
    "        feature_max_p_val = result_table[\"Feature\"][p_val_max_pos]\n",
    "        # If one or more features have to high p-value, remove\n",
    "        if p_val_max > max_p_limit:\n",
    "            # Append name to list that keeps track of removed features\n",
    "            removed_features.append(feature_max_p_val)\n",
    "            # Dropping feature\n",
    "            X_reduce.drop(columns=feature_max_p_val, inplace=True)\n",
    "            # Sorting features\n",
    "            X_reduce.sort_index(axis=1, inplace=True)\n",
    "            # Fitting reduced model\n",
    "            regressor.fit(X_reduce, y_train)\n",
    "\n",
    "    return regressor, X_reduce, result_table, removed_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 score of full model: 0.56 Backward Model: 0.58\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>$\\beta_i$</th>\n",
       "      <th>$\\pm$</th>\n",
       "      <th>p-values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Intercept</td>\n",
       "      <td>2.312926</td>\n",
       "      <td>0.008741</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FLGEW</td>\n",
       "      <td>0.107676</td>\n",
       "      <td>0.018653</td>\n",
       "      <td>2.331229e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FLGROSS</td>\n",
       "      <td>0.187149</td>\n",
       "      <td>0.018647</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Female</td>\n",
       "      <td>-0.091558</td>\n",
       "      <td>0.013661</td>\n",
       "      <td>1.382103e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Male</td>\n",
       "      <td>0.091558</td>\n",
       "      <td>0.013996</td>\n",
       "      <td>3.470819e-10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Feature  $\\beta_i$     $\\pm$      p-values\n",
       "0  Intercept   2.312926  0.008741  0.000000e+00\n",
       "1      FLGEW   0.107676  0.018653  2.331229e-08\n",
       "2    FLGROSS   0.187149  0.018647  0.000000e+00\n",
       "3     Female  -0.091558  0.013661  1.382103e-10\n",
       "4       Male   0.091558  0.013996  3.470819e-10"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 6;\n",
       "                var nbb_formatted_code = \"# Model to send in to function. It is an OLS regressor.\\nbase_regressor = skllm.LinearRegression()\\n# Running backwards elimination\\n(\\n    regressor_reduced,\\n    X_reduce_train,\\n    result_table_reduced,\\n    removed_features,\\n) = backward_elimination(base_regressor, X_train, y_train, max_p_limit=1e-2)\\n# Using list of reduced features to also create test feature matrix with same features\\nX_reduce_test = X_test.drop(columns=removed_features).sort_index(axis=1)\\n# Printing results\\nprint(\\n    f\\\"R^2 score of full model: {ols_reg.score(X_test, y_test):.2f} Backward Model: {regressor_reduced.score(X_reduce_test, y_test):.2f}\\\"\\n)\\nresult_table_reduced\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Model to send in to function. It is an OLS regressor.\n",
    "base_regressor = skllm.LinearRegression()\n",
    "# Running backwards elimination\n",
    "(\n",
    "    regressor_reduced,\n",
    "    X_reduce_train,\n",
    "    result_table_reduced,\n",
    "    removed_features,\n",
    ") = backward_elimination(base_regressor, X_train, y_train, max_p_limit=1e-2)\n",
    "# Using list of reduced features to also create test feature matrix with same features\n",
    "X_reduce_test = X_test.drop(columns=removed_features).sort_index(axis=1)\n",
    "# Printing results\n",
    "print(\n",
    "    f\"R^2 score of full model: {ols_reg.score(X_test, y_test):.2f} Backward Model: {regressor_reduced.score(X_reduce_test, y_test):.2f}\"\n",
    ")\n",
    "result_table_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 7;\n",
       "                var nbb_formatted_code = \"def forward_selection(regressor, X_train, y_train, max_p_limit):\\n    \\\"\\\"\\\"\\n    Takes a regressor, training set and a max p-value, runs forward\\n    selection and returns the regresson fitted on the reduced\\n    features, the reduced feature matrix, a table of betas, \\n    standard deviations and p-values and the removed features\\n    \\\"\\\"\\\"\\n    X_null = pd.DataFrame({\\\"null\\\": np.zeros_like(y_train)})\\n    regressor.fit(X_null, y_train)\\n    # The p-value for the 0-column is invalid, but also not used, so I ignore the warnings\\n    with np.errstate(invalid=\\\"ignore\\\"):\\n        # Getting results for null-model\\n        result_table = get_summary_linear_model(regressor, X_null, y_train)\\n    # p-value for intercept\\n    p_val_max = result_table[\\\"p-values\\\"][0]\\n    # Dataframe used for incresing\\n    X_increased = pd.DataFrame()\\n    # List of features\\n    features = X_train.columns.values\\n    # while max p-val is below threshold, repeat\\n    while p_val_max < max_p_limit:\\n        # Set best p to infinity so that all values are less\\n        best_p = np.inf\\n        # Looping over features\\n        for feature in features:\\n            # Creating new column with feature in loop\\n            new_col = pd.DataFrame({feature: X_train[feature].values})\\n            # If null model we need to append to the dataframe differently than usual\\n            if len(X_increased.values) == 0:\\n                # Adding new feature to null model\\n                X_candidate = X_increased.append(new_col)\\n            else:\\n                # Adding new feature to model\\n                new_col_names = np.append(\\n                    X_increased.columns.values, new_col.columns.values\\n                )\\n                X_candidate = pd.DataFrame(\\n                    np.append(X_increased.values, new_col.values, axis=1),\\n                    columns=new_col_names,\\n                )\\n            # Fitting increased model to find p-value\\n            regressor.fit(X_candidate, y_train)\\n            result_table = get_summary_linear_model(regressor, X_candidate, y_train)\\n            p_i = result_table[\\\"p-values\\\"].values[-1]\\n            # This if-statement is used to find the minimum p-value of the potential features to add\\n            if p_i < best_p:\\n                best_p = p_i\\n                best_new_feature = feature\\n        # Now that we have the best feature to add, we add it properly\\n        new_col = pd.DataFrame({best_new_feature: X_train[best_new_feature].values})\\n        if len(X_increased.values) == 0:\\n            X_candidate = X_increased.append(new_col)\\n        else:\\n            new_col_names = np.append(\\n                X_increased.columns.values, new_col.columns.values\\n            )\\n            X_candidate = pd.DataFrame(\\n                np.append(X_increased.values, new_col.values, axis=1),\\n                columns=new_col_names,\\n            )\\n        # Get results for new model\\n        result_table = get_summary_linear_model(regressor, X_candidate, y_train)\\n        p_val_max = result_table[\\\"p-values\\\"].values.max()\\n\\n        # Sorting features\\n        X_increased = X_candidate.sort_index(axis=1)\\n        # Removing added feature from list of potential featues so that we can't add it again next iteration\\n        features = features[features != best_new_feature]\\n\\n    # List of omitted features\\n    omitted_features = features\\n    # Fitting increased model\\n    regressor.fit(X_increased, y_train)\\n    # Table of results for best model\\n    result_table_best = get_summary_linear_model(regressor, X_increased, y_train)\\n    return regressor, X_increased, result_table_best, omitted_features\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def forward_selection(regressor, X_train, y_train, max_p_limit):\n",
    "    \"\"\"\n",
    "    Takes a regressor, training set and a max p-value, runs forward\n",
    "    selection and returns the regresson fitted on the reduced\n",
    "    features, the reduced feature matrix, a table of betas, \n",
    "    standard deviations and p-values and the removed features\n",
    "    \"\"\"\n",
    "    X_null = pd.DataFrame({\"null\": np.zeros_like(y_train)})\n",
    "    regressor.fit(X_null, y_train)\n",
    "    # The p-value for the 0-column is invalid, but also not used, so I ignore the warnings\n",
    "    with np.errstate(invalid=\"ignore\"):\n",
    "        # Getting results for null-model\n",
    "        result_table = get_summary_linear_model(regressor, X_null, y_train)\n",
    "    # p-value for intercept\n",
    "    p_val_max = result_table[\"p-values\"][0]\n",
    "    # Dataframe used for incresing\n",
    "    X_increased = pd.DataFrame()\n",
    "    # List of features\n",
    "    features = X_train.columns.values\n",
    "    # while max p-val is below threshold, repeat\n",
    "    while p_val_max < max_p_limit:\n",
    "        # Set best p to infinity so that all values are less\n",
    "        best_p = np.inf\n",
    "        # Looping over features\n",
    "        for feature in features:\n",
    "            # Creating new column with feature in loop\n",
    "            new_col = pd.DataFrame({feature: X_train[feature].values})\n",
    "            # If null model we need to append to the dataframe differently than usual\n",
    "            if len(X_increased.values) == 0:\n",
    "                # Adding new feature to null model\n",
    "                X_candidate = X_increased.append(new_col)\n",
    "            else:\n",
    "                # Adding new feature to model\n",
    "                new_col_names = np.append(\n",
    "                    X_increased.columns.values, new_col.columns.values\n",
    "                )\n",
    "                X_candidate = pd.DataFrame(\n",
    "                    np.append(X_increased.values, new_col.values, axis=1),\n",
    "                    columns=new_col_names,\n",
    "                )\n",
    "            # Fitting increased model to find p-value\n",
    "            regressor.fit(X_candidate, y_train)\n",
    "            result_table = get_summary_linear_model(regressor, X_candidate, y_train)\n",
    "            p_i = result_table[\"p-values\"].values[-1]\n",
    "            # This if-statement is used to find the minimum p-value of the potential features to add\n",
    "            if p_i < best_p:\n",
    "                best_p = p_i\n",
    "                best_new_feature = feature\n",
    "        # Now that we have the best feature to add, we add it properly\n",
    "        new_col = pd.DataFrame({best_new_feature: X_train[best_new_feature].values})\n",
    "        if len(X_increased.values) == 0:\n",
    "            X_candidate = X_increased.append(new_col)\n",
    "        else:\n",
    "            new_col_names = np.append(\n",
    "                X_increased.columns.values, new_col.columns.values\n",
    "            )\n",
    "            X_candidate = pd.DataFrame(\n",
    "                np.append(X_increased.values, new_col.values, axis=1),\n",
    "                columns=new_col_names,\n",
    "            )\n",
    "        # Get results for new model\n",
    "        result_table = get_summary_linear_model(regressor, X_candidate, y_train)\n",
    "        p_val_max = result_table[\"p-values\"].values.max()\n",
    "\n",
    "        # Sorting features\n",
    "        X_increased = X_candidate.sort_index(axis=1)\n",
    "        # Removing added feature from list of potential featues so that we can't add it again next iteration\n",
    "        features = features[features != best_new_feature]\n",
    "\n",
    "    # List of omitted features\n",
    "    omitted_features = features\n",
    "    # Fitting increased model\n",
    "    regressor.fit(X_increased, y_train)\n",
    "    # Table of results for best model\n",
    "    result_table_best = get_summary_linear_model(regressor, X_increased, y_train)\n",
    "    return regressor, X_increased, result_table_best, omitted_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 score of forward model: 0.58\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>$\\beta_i$</th>\n",
       "      <th>$\\pm$</th>\n",
       "      <th>p-values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Intercept</td>\n",
       "      <td>2.312926</td>\n",
       "      <td>0.008741</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FLGEW</td>\n",
       "      <td>0.107676</td>\n",
       "      <td>0.018653</td>\n",
       "      <td>2.331229e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FLGROSS</td>\n",
       "      <td>0.187149</td>\n",
       "      <td>0.018647</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Female</td>\n",
       "      <td>-0.091558</td>\n",
       "      <td>0.013661</td>\n",
       "      <td>1.382103e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Male</td>\n",
       "      <td>0.091558</td>\n",
       "      <td>0.013996</td>\n",
       "      <td>3.470819e-10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Feature  $\\beta_i$     $\\pm$      p-values\n",
       "0  Intercept   2.312926  0.008741  0.000000e+00\n",
       "1      FLGEW   0.107676  0.018653  2.331229e-08\n",
       "2    FLGROSS   0.187149  0.018647  0.000000e+00\n",
       "3     Female  -0.091558  0.013661  1.382103e-10\n",
       "4       Male   0.091558  0.013996  3.470819e-10"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 8;\n",
       "                var nbb_formatted_code = \"# Model to send in to function. It is an OLS regressor.\\nbase_regressor = skllm.LinearRegression()\\n# Running forward selection\\n(\\n    regressor_increased,\\n    X_increased_train,\\n    result_table_increased,\\n    omitted_features_increased,\\n) = forward_selection(base_regressor, X_train, y_train, max_p_limit=1e-2)\\n# Using list of reduced features to also create test feature matrix with same features\\nX_increased_test = X_test.drop(columns=omitted_features_increased).sort_index(axis=1)\\n# Printing results\\nprint(\\n    f\\\"R^2 score of forward model: {regressor_increased.score(X_increased_test, y_test):.2f}\\\"\\n)\\nresult_table_increased\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Model to send in to function. It is an OLS regressor.\n",
    "base_regressor = skllm.LinearRegression()\n",
    "# Running forward selection\n",
    "(\n",
    "    regressor_increased,\n",
    "    X_increased_train,\n",
    "    result_table_increased,\n",
    "    omitted_features_increased,\n",
    ") = forward_selection(base_regressor, X_train, y_train, max_p_limit=1e-2)\n",
    "# Using list of reduced features to also create test feature matrix with same features\n",
    "X_increased_test = X_test.drop(columns=omitted_features_increased).sort_index(axis=1)\n",
    "# Printing results\n",
    "print(\n",
    "    f\"R^2 score of forward model: {regressor_increased.score(X_increased_test, y_test):.2f}\"\n",
    ")\n",
    "result_table_increased"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the reduced models with $p_\\text{max}=0.01$, both forward and backward selection give the exact same model, and therefore the same features. They also get a slightly better $R^2$-score, possibly because they have less features, and are therefore less likely to overfit on the training data. I chose to look at $R^2$ instead of MSE as I feel it is a more intuitive value. However, higher $R^2$ also implies lower MSE, so the models are better.\n",
    "\n",
    "Next I will test with a less strict $p_\\text{max}=0.1$ and see how the models perform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 score of full model: 0.56 Backward Model: 0.58\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>$\\beta_i$</th>\n",
       "      <th>$\\pm$</th>\n",
       "      <th>p-values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Intercept</td>\n",
       "      <td>2.395716</td>\n",
       "      <td>0.017552</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FLGEW</td>\n",
       "      <td>0.094500</td>\n",
       "      <td>0.018392</td>\n",
       "      <td>5.641129e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FLGROSS</td>\n",
       "      <td>0.186385</td>\n",
       "      <td>0.018498</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FMILB</td>\n",
       "      <td>-0.028619</td>\n",
       "      <td>0.013494</td>\n",
       "      <td>3.492958e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FNOH24</td>\n",
       "      <td>-0.043226</td>\n",
       "      <td>0.016336</td>\n",
       "      <td>8.664813e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>FO3H24</td>\n",
       "      <td>0.055272</td>\n",
       "      <td>0.027557</td>\n",
       "      <td>4.597409e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>FSATEM</td>\n",
       "      <td>0.174443</td>\n",
       "      <td>0.060503</td>\n",
       "      <td>4.283578e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>FTEH24</td>\n",
       "      <td>-0.043142</td>\n",
       "      <td>0.025373</td>\n",
       "      <td>9.033456e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Female</td>\n",
       "      <td>-0.094777</td>\n",
       "      <td>0.015168</td>\n",
       "      <td>1.803255e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>HOCHOZON</td>\n",
       "      <td>-0.139679</td>\n",
       "      <td>0.035634</td>\n",
       "      <td>1.147912e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Male</td>\n",
       "      <td>0.094777</td>\n",
       "      <td>0.015842</td>\n",
       "      <td>7.673579e-09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Feature  $\\beta_i$     $\\pm$      p-values\n",
       "0   Intercept   2.395716  0.017552  0.000000e+00\n",
       "1       FLGEW   0.094500  0.018392  5.641129e-07\n",
       "2     FLGROSS   0.186385  0.018498  0.000000e+00\n",
       "3       FMILB  -0.028619  0.013494  3.492958e-02\n",
       "4      FNOH24  -0.043226  0.016336  8.664813e-03\n",
       "5      FO3H24   0.055272  0.027557  4.597409e-02\n",
       "6      FSATEM   0.174443  0.060503  4.283578e-03\n",
       "7      FTEH24  -0.043142  0.025373  9.033456e-02\n",
       "8      Female  -0.094777  0.015168  1.803255e-09\n",
       "9    HOCHOZON  -0.139679  0.035634  1.147912e-04\n",
       "10       Male   0.094777  0.015842  7.673579e-09"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 9;\n",
       "                var nbb_formatted_code = \"# Model to send in to function. It is an OLS regressor.\\nbase_regressor = skllm.LinearRegression()\\n# Running backwards elimination\\n(\\n    regressor_reduced_less_strict,\\n    X_reduce_train_less_strict,\\n    result_table_reduced_less_strict,\\n    removed_features_less_strict,\\n) = backward_elimination(base_regressor, X_train, y_train, max_p_limit=1e-1)\\n# Using list of reduced features to also create test feature matrix with same features\\nX_reduce_test_less_strict = X_test.drop(\\n    columns=removed_features_less_strict\\n).sort_index(axis=1)\\n# Printing results\\nprint(\\n    f\\\"R^2 score of full model: {ols_reg.score(X_test, y_test):.2f}\\\"\\n    + f\\\" Backward Model: {regressor_reduced_less_strict.score(X_reduce_test_less_strict, y_test):.2f}\\\"\\n)\\nresult_table_reduced_less_strict\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Model to send in to function. It is an OLS regressor.\n",
    "base_regressor = skllm.LinearRegression()\n",
    "# Running backwards elimination\n",
    "(\n",
    "    regressor_reduced_less_strict,\n",
    "    X_reduce_train_less_strict,\n",
    "    result_table_reduced_less_strict,\n",
    "    removed_features_less_strict,\n",
    ") = backward_elimination(base_regressor, X_train, y_train, max_p_limit=1e-1)\n",
    "# Using list of reduced features to also create test feature matrix with same features\n",
    "X_reduce_test_less_strict = X_test.drop(\n",
    "    columns=removed_features_less_strict\n",
    ").sort_index(axis=1)\n",
    "# Printing results\n",
    "print(\n",
    "    f\"R^2 score of full model: {ols_reg.score(X_test, y_test):.2f}\"\n",
    "    + f\" Backward Model: {regressor_reduced_less_strict.score(X_reduce_test_less_strict, y_test):.2f}\"\n",
    ")\n",
    "result_table_reduced_less_strict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 score of forward model: 0.58\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>$\\beta_i$</th>\n",
       "      <th>$\\pm$</th>\n",
       "      <th>p-values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Intercept</td>\n",
       "      <td>2.373489</td>\n",
       "      <td>0.015975</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FLGEW</td>\n",
       "      <td>0.100789</td>\n",
       "      <td>0.018403</td>\n",
       "      <td>1.064494e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FLGROSS</td>\n",
       "      <td>0.179629</td>\n",
       "      <td>0.018427</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FNOH24</td>\n",
       "      <td>-0.035192</td>\n",
       "      <td>0.014644</td>\n",
       "      <td>1.699628e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FSATEM</td>\n",
       "      <td>0.131264</td>\n",
       "      <td>0.058277</td>\n",
       "      <td>2.517346e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Female</td>\n",
       "      <td>-0.090005</td>\n",
       "      <td>0.015146</td>\n",
       "      <td>9.498857e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>HOCHOZON</td>\n",
       "      <td>-0.102710</td>\n",
       "      <td>0.030996</td>\n",
       "      <td>1.058828e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Male</td>\n",
       "      <td>0.090005</td>\n",
       "      <td>0.015024</td>\n",
       "      <td>7.332416e-09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Feature  $\\beta_i$     $\\pm$      p-values\n",
       "0  Intercept   2.373489  0.015975  0.000000e+00\n",
       "1      FLGEW   0.100789  0.018403  1.064494e-07\n",
       "2    FLGROSS   0.179629  0.018427  0.000000e+00\n",
       "3     FNOH24  -0.035192  0.014644  1.699628e-02\n",
       "4     FSATEM   0.131264  0.058277  2.517346e-02\n",
       "5     Female  -0.090005  0.015146  9.498857e-09\n",
       "6   HOCHOZON  -0.102710  0.030996  1.058828e-03\n",
       "7       Male   0.090005  0.015024  7.332416e-09"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 10;\n",
       "                var nbb_formatted_code = \"# Model to send in to function. It is an OLS regressor.\\nbase_regressor = skllm.LinearRegression()\\n# Running forward selection\\n(\\n    regressor_increased_less_strict,\\n    X_increased_train_less_strict,\\n    result_table_increased_less_strict,\\n    omitted_features_increased_less_strict,\\n) = forward_selection(base_regressor, X_train, y_train, max_p_limit=1e-1)\\n# Using list of reduced features to also create test feature matrix with same features\\nX_increased_test_less_strict = X_test.drop(\\n    columns=omitted_features_increased_less_strict\\n).sort_index(axis=1)\\n# Printing results\\nprint(\\n    f\\\"R^2 score of forward model: {regressor_increased.score(X_increased_test, y_test):.2f}\\\"\\n)\\nresult_table_increased_less_strict\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Model to send in to function. It is an OLS regressor.\n",
    "base_regressor = skllm.LinearRegression()\n",
    "# Running forward selection\n",
    "(\n",
    "    regressor_increased_less_strict,\n",
    "    X_increased_train_less_strict,\n",
    "    result_table_increased_less_strict,\n",
    "    omitted_features_increased_less_strict,\n",
    ") = forward_selection(base_regressor, X_train, y_train, max_p_limit=1e-1)\n",
    "# Using list of reduced features to also create test feature matrix with same features\n",
    "X_increased_test_less_strict = X_test.drop(\n",
    "    columns=omitted_features_increased_less_strict\n",
    ").sort_index(axis=1)\n",
    "# Printing results\n",
    "print(\n",
    "    f\"R^2 score of forward model: {regressor_increased.score(X_increased_test, y_test):.2f}\"\n",
    ")\n",
    "result_table_increased_less_strict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the models are not the same anymore. This is to be expected, as the p-values estimated are not the same for each feature independent of the other features. The backward elimination model seems to give a better $R^2$-score this time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1.4\n",
    "CV is easily implemented in Scikit-Learn. Bootstap on the other hand... I need to create my own class (Maybe there is a better way of doing this than what I'm doing...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameter is 5.30e-03, giving a test R^2 score of 0.59\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 11;\n",
       "                var nbb_formatted_code = \"# 5-fold cross validation, n-jobs=-1 is for parallelisation (use multiple cpu cores)\\nlasso_cv = skllm.LassoCV(n_jobs=-1, cv=5).fit(X_train, y_train)\\n# List of hyperparameters\\nlambdas_lasso_cv = lasso_cv.alphas_\\n# List of validation mean squared errors. Need to average them over axis 1 to get average across all 5 folds\\nmses_lasso_cv = lasso_cv.mse_path_.mean(axis=1)\\nprint(\\n    f\\\"Best hyperparameter is {lasso_cv.alpha_:.2e}, giving a test R^2 score of {lasso_cv.score(X_test, y_test):.2f}\\\"\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 5-fold cross validation, n-jobs=-1 is for parallelisation (use multiple cpu cores)\n",
    "lasso_cv = skllm.LassoCV(n_jobs=-1, cv=5).fit(X_train, y_train)\n",
    "# List of hyperparameters\n",
    "lambdas_lasso_cv = lasso_cv.alphas_\n",
    "# List of validation mean squared errors. Need to average them over axis 1 to get average across all 5 folds\n",
    "mses_lasso_cv = lasso_cv.mse_path_.mean(axis=1)\n",
    "print(\n",
    "    f\"Best hyperparameter is {lasso_cv.alpha_:.2e}, giving a test R^2 score of {lasso_cv.score(X_test, y_test):.2f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems to be very similar to forward and backward selection. Now I need to make a new class for bootstrap manually implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameter is 3.03e-03, giving a test R^2 score of 0.58\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 12;\n",
       "                var nbb_formatted_code = \"class Bootstrap:\\n    def __init__(self, y):\\n        \\\"\\\"\\\"\\n        I was sort of confused on how to this, so I just made a class and tried stuff. \\n        Now it works, so I won't change it anymore.\\n        This class takes y_train and saves its length.\\n        \\\"\\\"\\\"\\n        self.len_y = len(y)\\n\\n    @property\\n    def get_bootstrap(self):\\n        \\\"\\\"\\\"\\n        This method returns one train-validation bootstrap split of the training data (indices of the data).\\n        The @property is just so that i can call get_bootstrap without the () after (like in get_bootstrap()).\\n        This splits train and validation into 2/3, 1/3 of the length of the data. Not sure if that is the correct\\n        method.\\n        \\\"\\\"\\\"\\n        # All indices (0-lenght of y)\\n        indices = np.arange(self.len_y)\\n        # Picking random indices with replacement.\\n        indices_train = np.random.choice(\\n            indices, replace=True, size=int(self.len_y * 0.67)\\n        )\\n        indices_validate = np.random.choice(\\n            indices, replace=True, size=int(self.len_y * 0.37)\\n        )\\n        # Returns a list of lists\\n        return [indices_train.tolist(), indices_validate.tolist()]\\n\\n\\n# Creating instance of Bootstrap class\\nbootstrap = Bootstrap(y_train)\\nsplits = []\\n# This loops creates 100 different bootstrap samples\\nfor i in range(100):\\n    splits.append(bootstrap.get_bootstrap)\\n# Running LassoCV with bootstrap instead of CV.\\nlasso_bootstrap = skllm.LassoCV(n_jobs=-1, cv=splits).fit(X_train, y_train)\\n\\n# Lambdas tried by the solver\\nlambdas_lasso_bootstrap = lasso_bootstrap.alphas_\\n# List of validation mean squared errors. Need to average them over axis 1 to get average across all 100 bootstraps\\nmses_lasso_bootstrap = lasso_bootstrap.mse_path_.mean(axis=1)\\n\\nprint(\\n    f\\\"Best hyperparameter is {lasso_bootstrap.alpha_:.2e}, giving a test R^2 score of {lasso_bootstrap.score(X_test, y_test):.2f}\\\"\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Bootstrap:\n",
    "    def __init__(self, y):\n",
    "        \"\"\"\n",
    "        I was sort of confused on how to this, so I just made a class and tried stuff. \n",
    "        Now it works, so I won't change it anymore.\n",
    "        This class takes y_train and saves its length.\n",
    "        \"\"\"\n",
    "        self.len_y = len(y)\n",
    "\n",
    "    @property\n",
    "    def get_bootstrap(self):\n",
    "        \"\"\"\n",
    "        This method returns one train-validation bootstrap split of the training data (indices of the data).\n",
    "        The @property is just so that i can call get_bootstrap without the () after (like in get_bootstrap()).\n",
    "        This splits train and validation into 2/3, 1/3 of the length of the data. Not sure if that is the correct\n",
    "        method.\n",
    "        \"\"\"\n",
    "        # All indices (0-lenght of y)\n",
    "        indices = np.arange(self.len_y)\n",
    "        # Picking random indices with replacement.\n",
    "        indices_train = np.random.choice(\n",
    "            indices, replace=True, size=int(self.len_y * 0.67)\n",
    "        )\n",
    "        indices_validate = np.random.choice(\n",
    "            indices, replace=True, size=int(self.len_y * 0.37)\n",
    "        )\n",
    "        # Returns a list of lists\n",
    "        return [indices_train.tolist(), indices_validate.tolist()]\n",
    "\n",
    "\n",
    "# Creating instance of Bootstrap class\n",
    "bootstrap = Bootstrap(y_train)\n",
    "splits = []\n",
    "# This loops creates 100 different bootstrap samples\n",
    "for i in range(100):\n",
    "    splits.append(bootstrap.get_bootstrap)\n",
    "# Running LassoCV with bootstrap instead of CV.\n",
    "lasso_bootstrap = skllm.LassoCV(n_jobs=-1, cv=splits).fit(X_train, y_train)\n",
    "\n",
    "# Lambdas tried by the solver\n",
    "lambdas_lasso_bootstrap = lasso_bootstrap.alphas_\n",
    "# List of validation mean squared errors. Need to average them over axis 1 to get average across all 100 bootstraps\n",
    "mses_lasso_bootstrap = lasso_bootstrap.mse_path_.mean(axis=1)\n",
    "\n",
    "print(\n",
    "    f\"Best hyperparameter is {lasso_bootstrap.alpha_:.2e}, giving a test R^2 score of {lasso_bootstrap.score(X_test, y_test):.2f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x482.4 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 13;\n",
       "                var nbb_formatted_code = \"fonts = {\\n    \\\"font.family\\\": \\\"serif\\\",\\n    \\\"axes.labelsize\\\": 12,\\n    \\\"font.size\\\": 12,\\n    \\\"legend.fontsize\\\": 12,\\n    \\\"xtick.labelsize\\\": 12,\\n    \\\"ytick.labelsize\\\": 12,\\n}\\n\\nplt.rcParams.update(fonts)\\n\\n# Log plot\\nplt.figure(figsize=(10, 6.7))\\nplt.semilogx(lambdas_lasso_bootstrap, mses_lasso_bootstrap, label=\\\"Bootsrap\\\")\\nplt.semilogx(lambdas_lasso_cv, mses_lasso_cv, label=\\\"5-fold cross validation\\\")\\nplt.xlabel(r\\\"Hyperparameter $\\\\lambda$\\\")\\nplt.ylabel(r\\\"|MSE|\\\")\\nplt.title(\\\"Bootstrap lasso results\\\")\\nplt.legend()\\nplt.grid()\\nplt.tight_layout()\\nplt.show()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fonts = {\n",
    "    \"font.family\": \"serif\",\n",
    "    \"axes.labelsize\": 12,\n",
    "    \"font.size\": 12,\n",
    "    \"legend.fontsize\": 12,\n",
    "    \"xtick.labelsize\": 12,\n",
    "    \"ytick.labelsize\": 12,\n",
    "}\n",
    "\n",
    "plt.rcParams.update(fonts)\n",
    "\n",
    "# Log plot\n",
    "plt.figure(figsize=(10, 6.7))\n",
    "plt.semilogx(lambdas_lasso_bootstrap, mses_lasso_bootstrap, label=\"Bootsrap\")\n",
    "plt.semilogx(lambdas_lasso_cv, mses_lasso_cv, label=\"5-fold cross validation\")\n",
    "plt.xlabel(r\"Hyperparameter $\\lambda$\")\n",
    "plt.ylabel(r\"|MSE|\")\n",
    "plt.title(\"Bootstrap lasso results\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bootstrap and cross validation seem to estimate the error quite similarily. CV is probably a bit more realistic, as the error is a bit higher, but that i not important. The important thing is where the minimum is, and it seems to be almost the same for both methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (100 of 100) |######################| Elapsed Time: 0:00:27 Time:  0:00:27\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 score test: 0.5158772804550404\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 72;\n",
       "                var nbb_formatted_code = \"features_gam = pygam.s(0) + pygam.l(0)\\n\\nnonlinear_features = [\\\"FLGROSS\\\"]\\n\\nfor i, feature in enumerate(X_train.columns.values[1:]):\\n    if feature in categorical:\\n        features_gam += pygam.s(i)  # + pygam.l(i)\\n    else:\\n        features_gam += pygam.l(i)\\n\\n\\nlambdas = np.exp(np.random.rand(100, len(features_gam)) * 3 - 3)\\ngam = pygam.LinearGAM(features_gam).gridsearch(\\n    X_train.values, y_train.values, lam=lambdas\\n)\\n\\n# gam = pygam.LinearGAM(features_gam).fit(X_train, y_train)\\nr2_test_gam = gam._estimate_r2(X_test, y_test)[\\\"explained_deviance\\\"]\\nprint(f\\\"R^2 score test: {r2_test_gam}\\\")\\n# gam.summary()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "features_gam = pygam.s(0) + pygam.l(0)\n",
    "\n",
    "nonlinear_features = [\"FLGROSS\"]\n",
    "\n",
    "for i, feature in enumerate(X_train.columns.values[1:]):\n",
    "    if feature in categorical:\n",
    "        features_gam += pygam.s(i)  # + pygam.l(i)\n",
    "    else:\n",
    "        features_gam += pygam.l(i)\n",
    "\n",
    "\n",
    "lambdas = np.exp(np.random.rand(100, len(features_gam)) * 3 - 3)\n",
    "gam = pygam.LinearGAM(features_gam).gridsearch(\n",
    "    X_train.values, y_train.values, lam=lambdas\n",
    ")\n",
    "\n",
    "# gam = pygam.LinearGAM(features_gam).fit(X_train, y_train)\n",
    "r2_test_gam = gam._estimate_r2(X_test, y_test)[\"explained_deviance\"]\n",
    "print(f\"R^2 score test: {r2_test_gam}\")\n",
    "# gam.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GAM model does not seem to improve anything. Maybe not all features should have non-linearities?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
